#!/usr/bin/env python3
"""
ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ê°œì„  í›„ ë°ì´í„°ì˜ ê²€ìƒ‰ í’ˆì§ˆì„ ì¸¡ì •í•©ë‹ˆë‹¤.
"""

import os
import sys
import psycopg2
from psycopg2.extras import RealDictCursor
from dotenv import load_dotenv
import requests
from typing import List, Dict, Tuple
import json
from datetime import datetime

load_dotenv()

class SearchQualityTester:
    """ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        try:
            self.conn = psycopg2.connect(
                host=os.getenv('DB_HOST', 'localhost'),
                port=os.getenv('DB_PORT', '5432'),
                database=os.getenv('DB_NAME', 'ddoksori'),
                user=os.getenv('DB_USER', 'postgres'),
                password=os.getenv('DB_PASSWORD', 'postgres')
            )
            self.cur = self.conn.cursor(cursor_factory=RealDictCursor)
            print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
        except Exception as e:
            print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            sys.exit(1)
        
        self.embed_api_url = os.getenv('EMBED_API_URL', 'http://localhost:8001/embed')
    
    def check_data_status(self) -> Dict:
        """ë°ì´í„° ìƒíƒœ í™•ì¸"""
        print("\n" + "=" * 100)
        print("ë°ì´í„° ìƒíƒœ í™•ì¸")
        print("=" * 100)
        
        # ì´ ì²­í¬ ë° ì„ë² ë”© ìƒíƒœ
        self.cur.execute("""
            SELECT 
                COUNT(*) as total_chunks,
                COUNT(CASE WHEN embedding IS NOT NULL THEN 1 END) as embedded_chunks,
                COUNT(CASE WHEN drop = FALSE THEN 1 END) as active_chunks,
                COUNT(CASE WHEN drop = FALSE AND embedding IS NOT NULL THEN 1 END) as searchable_chunks
            FROM chunks
        """)
        
        stats = dict(self.cur.fetchone())
        
        print(f"\nğŸ“Š ì²­í¬ í†µê³„:")
        print(f"  - ì´ ì²­í¬: {stats['total_chunks']:,}ê°œ")
        print(f"  - í™œì„± ì²­í¬: {stats['active_chunks']:,}ê°œ")
        print(f"  - ì„ë² ë”© ì™„ë£Œ: {stats['embedded_chunks']:,}ê°œ")
        print(f"  - ê²€ìƒ‰ ê°€ëŠ¥: {stats['searchable_chunks']:,}ê°œ")
        
        if stats['searchable_chunks'] > 0:
            embed_rate = stats['embedded_chunks'] / stats['total_chunks'] * 100
            searchable_rate = stats['searchable_chunks'] / stats['active_chunks'] * 100
            print(f"\n  ì„ë² ë”© ì™„ë£Œìœ¨: {embed_rate:.1f}%")
            print(f"  ê²€ìƒ‰ ê°€ëŠ¥ìœ¨: {searchable_rate:.1f}%")
        
        return stats
    
    def get_query_embedding(self, query: str) -> List[float]:
        """ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±"""
        try:
            response = requests.post(
                self.embed_api_url,
                json={"texts": [query]},
                timeout=30
            )
            response.raise_for_status()
            embeddings = response.json()['embeddings']
            return embeddings[0]
        except requests.exceptions.RequestException as e:
            print(f"  âŒ ì„ë² ë”© API ì˜¤ë¥˜: {e}")
            return None
    
    def search_chunks(self, query: str, top_k: int = 10, 
                     doc_types: List[str] = None) -> List[Dict]:
        """ì²­í¬ ê²€ìƒ‰"""
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = self.get_query_embedding(query)
        if query_embedding is None:
            return []
        
        # ê²€ìƒ‰
        if doc_types:
            query_sql = """
                SELECT 
                    c.chunk_id,
                    c.doc_id,
                    c.chunk_type,
                    c.content,
                    c.content_length,
                    d.doc_type,
                    d.title,
                    d.source_org,
                    1 - (c.embedding <=> %s::vector) AS similarity
                FROM chunks c
                JOIN documents d ON c.doc_id = d.doc_id
                WHERE 
                    c.drop = FALSE
                    AND c.embedding IS NOT NULL
                    AND d.doc_type = ANY(%s)
                ORDER BY c.embedding <=> %s::vector
                LIMIT %s
            """
            params = [query_embedding, doc_types, query_embedding, top_k]
        else:
            query_sql = """
                SELECT 
                    c.chunk_id,
                    c.doc_id,
                    c.chunk_type,
                    c.content,
                    c.content_length,
                    d.doc_type,
                    d.title,
                    d.source_org,
                    1 - (c.embedding <=> %s::vector) AS similarity
                FROM chunks c
                JOIN documents d ON c.doc_id = d.doc_id
                WHERE 
                    c.drop = FALSE
                    AND c.embedding IS NOT NULL
                ORDER BY c.embedding <=> %s::vector
                LIMIT %s
            """
            params = [query_embedding, query_embedding, top_k]
        
        self.cur.execute(query_sql, params)
        return [dict(row) for row in self.cur.fetchall()]
    
    def evaluate_relevance(self, query: str, result: Dict) -> Dict:
        """ê²°ê³¼ ê´€ë ¨ì„± í‰ê°€ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)"""
        content = result['content'].lower()
        query_terms = query.lower().split()
        
        # í‚¤ì›Œë“œ ë§¤ì¹­
        matched_terms = [term for term in query_terms if term in content]
        keyword_score = len(matched_terms) / len(query_terms) if query_terms else 0
        
        return {
            'keyword_score': keyword_score,
            'matched_terms': matched_terms,
            'similarity': result['similarity']
        }
    
    def run_test_queries(self) -> List[Dict]:
        """í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰"""
        print("\n" + "=" * 100)
        print("ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸")
        print("=" * 100)
        
        test_cases = [
            {
                'id': 1,
                'query': 'ì˜¨ë¼ì¸ ì‡¼í•‘ëª°ì—ì„œ êµ¬ë§¤í•œ ì œí’ˆì´ ë¶ˆëŸ‰ì´ì—ìš”. í™˜ë¶ˆ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?',
                'doc_types': ['counsel_case', 'mediation_case'],
                'expected_keywords': ['ë¶ˆëŸ‰', 'í™˜ë¶ˆ', 'ì˜¨ë¼ì¸', 'ì‡¼í•‘']
            },
            {
                'id': 2,
                'query': 'ë°°ì†¡ë¹„ê°€ ê³¼ë‹¤í•˜ê²Œ ì²­êµ¬ë˜ì—ˆìŠµë‹ˆë‹¤',
                'doc_types': ['counsel_case', 'mediation_case'],
                'expected_keywords': ['ë°°ì†¡ë¹„', 'ì²­êµ¬']
            },
            {
                'id': 3,
                'query': 'ì „ììƒê±°ë˜ ê³„ì•½ í•´ì§€ ì‹œ ìœ„ì•½ê¸ˆì„ ë°›ì•˜ìŠµë‹ˆë‹¤',
                'doc_types': ['counsel_case', 'mediation_case'],
                'expected_keywords': ['ê³„ì•½', 'í•´ì§€', 'ìœ„ì•½ê¸ˆ']
            },
            {
                'id': 4,
                'query': 'ì‹í’ˆ í‘œì‹œê°€ ì˜ëª»ë˜ì–´ ìˆìŠµë‹ˆë‹¤',
                'doc_types': ['counsel_case'],
                'expected_keywords': ['ì‹í’ˆ', 'í‘œì‹œ']
            },
            {
                'id': 5,
                'query': 'í†µì‹ íŒë§¤ì—…ìì˜ ê±°ì§“ ê´‘ê³ ',
                'doc_types': ['counsel_case', 'mediation_case'],
                'expected_keywords': ['í†µì‹ íŒë§¤', 'ê´‘ê³ ', 'ê±°ì§“']
            }
        ]
        
        results = []
        
        for test_case in test_cases:
            print(f"\n[í…ŒìŠ¤íŠ¸ {test_case['id']}] {test_case['query']}")
            print("-" * 100)
            
            search_results = self.search_chunks(
                query=test_case['query'],
                top_k=5,
                doc_types=test_case.get('doc_types')
            )
            
            if not search_results:
                print("  âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                results.append({
                    'test_id': test_case['id'],
                    'query': test_case['query'],
                    'success': False,
                    'results_count': 0
                })
                continue
            
            print(f"  âœ… {len(search_results)}ê°œ ê²°ê³¼ ë°œê²¬")
            
            # ìƒìœ„ 3ê°œ ê²°ê³¼ í‰ê°€
            evaluations = []
            for idx, result in enumerate(search_results[:3], 1):
                eval_result = self.evaluate_relevance(test_case['query'], result)
                evaluations.append(eval_result)
                
                print(f"\n  [{idx}] ìœ ì‚¬ë„: {result['similarity']:.4f}, "
                      f"í‚¤ì›Œë“œ ë§¤ì¹­: {eval_result['keyword_score']:.2f}")
                print(f"      íƒ€ì…: {result['doc_type']}/{result['chunk_type']}, "
                      f"ê¸¸ì´: {result['content_length']}ì")
                print(f"      ì œëª©: {result['title'][:80]}")
                print(f"      ë‚´ìš©: {result['content'][:150].replace(chr(10), ' ')}...")
            
            # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
            avg_similarity = sum(e['similarity'] for e in evaluations) / len(evaluations)
            avg_keyword_score = sum(e['keyword_score'] for e in evaluations) / len(evaluations)
            
            results.append({
                'test_id': test_case['id'],
                'query': test_case['query'],
                'success': True,
                'results_count': len(search_results),
                'avg_similarity': avg_similarity,
                'avg_keyword_score': avg_keyword_score,
                'top_similarity': search_results[0]['similarity'],
                'evaluations': evaluations
            })
        
        return results
    
    def generate_report(self, test_results: List[Dict], stats: Dict) -> str:
        """í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\n" + "=" * 100)
        print("ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±")
        print("=" * 100)
        
        report = []
        report.append("=" * 100)
        report.append("ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸")
        report.append("=" * 100)
        report.append(f"ìƒì„± ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # 1. ë°ì´í„° ìƒíƒœ
        report.append("1. ë°ì´í„° ìƒíƒœ")
        report.append("-" * 100)
        report.append(f"ì´ ì²­í¬: {stats['total_chunks']:,}ê°œ")
        report.append(f"í™œì„± ì²­í¬: {stats['active_chunks']:,}ê°œ")
        report.append(f"ê²€ìƒ‰ ê°€ëŠ¥ ì²­í¬: {stats['searchable_chunks']:,}ê°œ")
        
        if stats['searchable_chunks'] > 0:
            searchable_rate = stats['searchable_chunks'] / stats['active_chunks'] * 100
            report.append(f"ê²€ìƒ‰ ê°€ëŠ¥ìœ¨: {searchable_rate:.1f}%")
        
        report.append("")
        
        # 2. í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½
        report.append("2. í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        report.append("-" * 100)
        
        successful_tests = [r for r in test_results if r['success']]
        
        if successful_tests:
            avg_similarity = sum(r['avg_similarity'] for r in successful_tests) / len(successful_tests)
            avg_keyword = sum(r['avg_keyword_score'] for r in successful_tests) / len(successful_tests)
            avg_top_sim = sum(r['top_similarity'] for r in successful_tests) / len(successful_tests)
            
            report.append(f"ì„±ê³µí•œ í…ŒìŠ¤íŠ¸: {len(successful_tests)}/{len(test_results)}ê°œ")
            report.append(f"í‰ê·  ìœ ì‚¬ë„ (ìƒìœ„ 3ê°œ): {avg_similarity:.4f}")
            report.append(f"í‰ê·  í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜: {avg_keyword:.2f}")
            report.append(f"ìµœê³  ìœ ì‚¬ë„ í‰ê· : {avg_top_sim:.4f}")
        else:
            report.append("âŒ ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ ì—†ìŒ")
        
        report.append("")
        
        # 3. í…ŒìŠ¤íŠ¸ë³„ ìƒì„¸ ê²°ê³¼
        report.append("3. í…ŒìŠ¤íŠ¸ë³„ ìƒì„¸ ê²°ê³¼")
        report.append("-" * 100)
        
        for result in test_results:
            report.append(f"\n[í…ŒìŠ¤íŠ¸ {result['test_id']}] {result['query']}")
            
            if result['success']:
                report.append(f"  âœ… ì„±ê³µ: {result['results_count']}ê°œ ê²°ê³¼")
                report.append(f"  í‰ê·  ìœ ì‚¬ë„: {result['avg_similarity']:.4f}")
                report.append(f"  í‚¤ì›Œë“œ ë§¤ì¹­: {result['avg_keyword_score']:.2f}")
                report.append(f"  ìµœê³  ìœ ì‚¬ë„: {result['top_similarity']:.4f}")
            else:
                report.append(f"  âŒ ì‹¤íŒ¨: ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        
        report.append("")
        
        # 4. í’ˆì§ˆ í‰ê°€
        report.append("4. ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€")
        report.append("-" * 100)
        
        if successful_tests:
            if avg_similarity >= 0.7:
                report.append("âœ… ìš°ìˆ˜: í‰ê·  ìœ ì‚¬ë„ 0.7 ì´ìƒ")
            elif avg_similarity >= 0.5:
                report.append("âš ï¸  ì–‘í˜¸: í‰ê·  ìœ ì‚¬ë„ 0.5 ì´ìƒ")
            else:
                report.append("âŒ ê°œì„  í•„ìš”: í‰ê·  ìœ ì‚¬ë„ 0.5 ë¯¸ë§Œ")
            
            if avg_keyword >= 0.3:
                report.append("âœ… í‚¤ì›Œë“œ ë§¤ì¹­ ì–‘í˜¸")
            else:
                report.append("âš ï¸  í‚¤ì›Œë“œ ë§¤ì¹­ ê°œì„  í•„ìš”")
        
        report.append("")
        
        # 5. ê°œì„  ì‚¬í•­
        report.append("5. ê¶Œì¥ ì‚¬í•­")
        report.append("-" * 100)
        
        if stats['searchable_chunks'] < stats['active_chunks']:
            remaining = stats['active_chunks'] - stats['searchable_chunks']
            report.append(f"âš ï¸  {remaining:,}ê°œ ì²­í¬ì˜ ì„ë² ë”© ìƒì„± í•„ìš”")
        
        if successful_tests and avg_similarity < 0.7:
            report.append("âš ï¸  ì„ë² ë”© ëª¨ë¸ ê°œì„  ë˜ëŠ” ì²­í¬ í¬ê¸° ì¬ì¡°ì • ê²€í†  í•„ìš”")
        
        report.append("")
        report.append("=" * 100)
        
        return "\n".join(report)
    
    def save_report(self, report: str, output_file: str = None):
        """ë¦¬í¬íŠ¸ ì €ì¥"""
        if output_file is None:
            output_file = "backend/data/transformed/search_quality_report.txt"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"\nğŸ’¾ ë¦¬í¬íŠ¸ ì €ì¥: {output_file}")
    
    def run(self):
        """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("=" * 100)
        print("ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 100)
        
        try:
            # 1. ë°ì´í„° ìƒíƒœ í™•ì¸
            stats = self.check_data_status()
            
            if stats['searchable_chunks'] == 0:
                print("\nâŒ ê²€ìƒ‰ ê°€ëŠ¥í•œ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                print("   ì„ë² ë”©ì„ ë¨¼ì € ìƒì„±í•˜ì„¸ìš”: python backend/scripts/embedding/embed_data_remote.py")
                return 1
            
            # 2. í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰
            test_results = self.run_test_queries()
            
            # 3. ë¦¬í¬íŠ¸ ìƒì„±
            report = self.generate_report(test_results, stats)
            
            # 4. ì¶œë ¥ ë° ì €ì¥
            print("\n" + report)
            self.save_report(report)
            
            print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
            return 0
            
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return 1
        finally:
            if self.cur:
                self.cur.close()
            if self.conn:
                self.conn.close()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    tester = SearchQualityTester()
    return tester.run()

if __name__ == '__main__':
    sys.exit(main())
