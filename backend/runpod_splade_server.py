# backend/runpod_splade_server.py
"""
RunPodì—ì„œ ì‹¤í–‰í•  SPLADE API ì„œë²„
ë¡œì»¬ì—ì„œ SSH í„°ë„ì„ í†µí•´ ì ‘ê·¼í•˜ì—¬ SPLADE ê²€ìƒ‰ ìˆ˜í–‰
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
import numpy as np
from typing import List, Dict, Optional
import os
from dotenv import load_dotenv

# transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
try:
    import transformers
    print(f"âœ… transformers {transformers.__version__} ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    print(f"âŒ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    print("\ní•´ê²° ë°©ë²•:")
    print("  pip install --upgrade transformers>=4.41.0")
    raise

# PreTrainedModel import í…ŒìŠ¤íŠ¸
try:
    from transformers import PreTrainedModel
    print(f"âœ… PreTrainedModel import ì„±ê³µ")
except (ImportError, ModuleNotFoundError) as e:
    print(f"âŒ PreTrainedModel import ì‹¤íŒ¨: {e}")
    print("\nğŸ’¡ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì†ìƒë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print("\ní•´ê²° ë°©ë²• (RunPodì—ì„œ ì‹¤í–‰):")
    print("  1. transformers ì™„ì „ ì¬ì„¤ì¹˜:")
    print("     pip uninstall transformers -y")
    print("     pip install transformers>=4.41.0")
    print("")
    print("  2. ë˜ëŠ” ìºì‹œ í´ë¦¬ì–´ í›„ ì¬ì„¤ì¹˜:")
    print("     pip cache purge")
    print("     pip install --force-reinstall transformers>=4.41.0")
    print("")
    print("  3. ì˜ì¡´ì„±ê³¼ í•¨ê»˜ ì¬ì„¤ì¹˜:")
    print("     pip install --upgrade transformers>=4.41.0 torch>=2.6")
    raise

# sentence-transformers ë²„ì „ í™•ì¸ ë° SparseEncoder import
try:
    import sentence_transformers
    version = sentence_transformers.__version__
    major_version = int(version.split('.')[0])
    
    if major_version < 5:
        raise ImportError(
            f"sentence-transformers ë²„ì „ì´ 5.0.0 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤. "
            f"í˜„ì¬ ë²„ì „: {version}. "
            f"ì—…ê·¸ë ˆì´ë“œ: pip install --upgrade sentence-transformers>=5.0.0"
        )
    
    from sentence_transformers import SparseEncoder
    print(f"âœ… sentence-transformers {version} - SparseEncoder ì‚¬ìš© ê°€ëŠ¥")
except ImportError as e:
    print(f"âŒ SparseEncoder import ì‹¤íŒ¨: {e}")
    print("\ní•´ê²° ë°©ë²•:")
    print("  1. transformers ì—…ê·¸ë ˆì´ë“œ: pip install --upgrade transformers>=4.41.0")
    print("  2. sentence-transformers ì—…ê·¸ë ˆì´ë“œ: pip install --upgrade sentence-transformers>=5.0.0")
    raise

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# hf_transferê°€ ì—†ìœ¼ë©´ ë¹„í™œì„±í™” (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜ ë°©ì§€)
try:
    import hf_transfer
except ImportError:
    os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '0'
    print("âš ï¸  hf_transferê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ê¸°ë³¸ ë‹¤ìš´ë¡œë“œ ë°©ì‹ ì‚¬ìš©")
    print("   ë¹ ë¥¸ ë‹¤ìš´ë¡œë“œë¥¼ ì›í•˜ë©´: pip install hf_transfer")

# HuggingFace í† í° í™•ì¸
HF_TOKEN = os.getenv('HF_TOKEN') or os.getenv('HUGGINGFACE_TOKEN')

# 1. ëª¨ë¸ ë¡œë“œ (GPU ì‚¬ìš©)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"ğŸ”§ Loading SPLADE model on {device}...")

# torch ë²„ì „ í™•ì¸
torch_version = torch.__version__
print(f"  PyTorch version: {torch_version}")

# torch 2.6 ë¯¸ë§Œì¸ ê²½ìš° ê²½ê³ 
try:
    major, minor = map(int, torch_version.split('.')[:2])
    if major < 2 or (major == 2 and minor < 6):
        print(f"  âš ï¸  PyTorch ë²„ì „ì´ 2.6 ë¯¸ë§Œì…ë‹ˆë‹¤ (í˜„ì¬: {torch_version})")
        print(f"  ğŸ’¡ safetensors í˜•ì‹ìœ¼ë¡œ ëª¨ë¸ì„ ë¡œë“œí•˜ë ¤ê³  ì‹œë„í•©ë‹ˆë‹¤...")
        # safetensors ì‚¬ìš© ê°•ì œ
        import os
        os.environ['SAFETENSORS_FAST_GPU'] = '1'
        os.environ['TRANSFORMERS_SAFE_LOADING'] = '1'
except:
    pass

try:
    model = SparseEncoder(
        "naver/splade-v3",
        token=HF_TOKEN if HF_TOKEN else None,
        trust_remote_code=True
    )
    print(f"âœ… SPLADE model loaded successfully on {device}!")
except Exception as e:
    error_str = str(e)
    if "torch.load" in error_str or "CVE-2025-32434" in error_str or "torch>=2.6" in error_str:
        print(f"âŒ Error loading SPLADE model: {error_str}")
        print(f"\nğŸ’¡ í•´ê²° ë°©ë²•:")
        print(f"   RunPodì—ì„œ torchë¥¼ ì—…ê·¸ë ˆì´ë“œí•˜ì„¸ìš”:")
        print(f"   pip install --upgrade torch>=2.6")
        print(f"   ë˜ëŠ” CUDA ë²„ì „ì— ë§ê²Œ:")
        print(f"   pip install torch>=2.6 --index-url https://download.pytorch.org/whl/cu121")
        raise RuntimeError("torch ë²„ì „ì´ 2.6 ë¯¸ë§Œì…ë‹ˆë‹¤. ì—…ê·¸ë ˆì´ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    else:
        print(f"âŒ Error loading SPLADE model: {e}")
        raise

# 2. FastAPI ì•± ìƒì„±
app = FastAPI(title="SPLADE Sparse Encoder API")

# 3. ìš”ì²­/ì‘ë‹µ ëª¨ë¸ ì •ì˜
class EncodeRequest(BaseModel):
    texts: List[str]

class EncodeResponse(BaseModel):
    embeddings: List[List[float]]  # Sparse vectorë¥¼ denseë¡œ ë³€í™˜
    shapes: List[List[int]]  # ê° embeddingì˜ shape

class SimilarityRequest(BaseModel):
    query_embedding: List[float]
    document_embeddings: List[List[float]]

class SimilarityResponse(BaseModel):
    similarities: List[float]

# 4. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
def sparse_to_dense(sparse_vec, vocab_size=30522):
    """
    Sparse tensorë¥¼ dense numpy arrayë¡œ ë³€í™˜
    
    Args:
        sparse_vec: torch.Tensor (sparse ë˜ëŠ” dense)
        vocab_size: vocabulary size (SPLADE-v3ëŠ” 30522)
    
    Returns:
        numpy array
    """
    if isinstance(sparse_vec, torch.Tensor):
        if sparse_vec.is_sparse:
            sparse_vec = sparse_vec.to_dense()
        sparse_vec = sparse_vec.cpu().numpy()
    
    # ë°°ì¹˜ì¸ ê²½ìš° ì²« ë²ˆì§¸ë§Œ ë°˜í™˜
    if len(sparse_vec.shape) > 1:
        return sparse_vec[0]
    return sparse_vec

# 5. ì¸ì½”ë”© ì—”ë“œí¬ì¸íŠ¸
@app.post("/encode_query", response_model=EncodeResponse)
def encode_query(request: EncodeRequest):
    """
    ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¥¼ Sparse Vectorë¡œ ì¸ì½”ë”©
    """
    try:
        query_embeddings = model.encode_query(request.texts)
        
        # Sparse tensorë¥¼ dense numpy arrayë¡œ ë³€í™˜
        dense_embeddings = []
        for emb in query_embeddings:
            dense_emb = sparse_to_dense(emb)
            dense_embeddings.append(dense_emb.tolist())
        
        return EncodeResponse(
            embeddings=dense_embeddings,
            shapes=[list(emb.shape) for emb in query_embeddings]
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Query encoding error: {str(e)}")

@app.post("/encode_document", response_model=EncodeResponse)
def encode_document(request: EncodeRequest):
    """
    ë¬¸ì„œ í…ìŠ¤íŠ¸ë¥¼ Sparse Vectorë¡œ ì¸ì½”ë”©
    """
    try:
        doc_embeddings = model.encode_document(request.texts)
        
        # Sparse tensorë¥¼ dense numpy arrayë¡œ ë³€í™˜
        dense_embeddings = []
        for emb in doc_embeddings:
            dense_emb = sparse_to_dense(emb)
            dense_embeddings.append(dense_emb.tolist())
        
        return EncodeResponse(
            embeddings=dense_embeddings,
            shapes=[list(emb.shape) for emb in doc_embeddings]
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Document encoding error: {str(e)}")

@app.post("/similarity", response_model=SimilarityResponse)
def compute_similarity(request: SimilarityRequest):
    """
    ì¿¼ë¦¬ì™€ ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ ê³„ì‚° (dot product)
    """
    try:
        query_vec = np.array(request.query_embedding)
        similarities = []
        
        for doc_vec in request.document_embeddings:
            doc_vec = np.array(doc_vec)
            similarity = float(np.dot(query_vec, doc_vec))
            similarities.append(similarity)
        
        return SimilarityResponse(similarities=similarities)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Similarity computation error: {str(e)}")

@app.get("/")
def root():
    return {
        "message": "SPLADE Sparse Encoder API is running",
        "device": device,
        "model": "naver/splade-v3",
        "cuda_available": torch.cuda.is_available()
    }

@app.get("/health")
def health():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "healthy",
        "device": device,
        "cuda_available": torch.cuda.is_available()
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
