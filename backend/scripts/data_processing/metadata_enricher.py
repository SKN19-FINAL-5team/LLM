#!/usr/bin/env python3
"""
ë©”íƒ€ë°ì´í„° ë³´ê°• ëª¨ë“ˆ

ì²­í¬ì˜ contentë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì„ ì¶”ì¶œ:
1. í‚¤ì›Œë“œ (ë¹ˆë„ ê¸°ë°˜)
2. ì—”í‹°í‹° (íšŒì‚¬ëª…, ì œí’ˆëª… ë“±)
3. ë²•ë¥  ìš©ì–´
4. ì¹´í…Œê³ ë¦¬ íƒœê·¸
"""

import re
from typing import Dict, List, Set
from collections import Counter


class MetadataEnricher:
    """ì²­í¬ ë©”íƒ€ë°ì´í„° ë³´ê°•"""
    
    def __init__(self):
        # ë²•ë¥  ìš©ì–´ ì‚¬ì „
        self.legal_terms = {
            'ì†Œë¹„ì', 'íŒë§¤ì', 'ê³µê¸‰ì', 'ê³„ì•½', 'í•´ì œ', 'í•´ì§€', 'ì·¨ì†Œ', 'í™˜ê¸‰',
            'ì†í•´ë°°ìƒ', 'ìœ„ì•½ê¸ˆ', 'í’ˆì§ˆë³´ì¦', 'í•˜ì', 'ë¯¼ë²•', 'ì†Œë¹„ìê¸°ë³¸ë²•',
            'ì „ììƒê±°ë˜ë²•', 'ì•½ê´€', 'ì²­ì•½', 'ìŠ¹ë‚™', 'ì´í–‰', 'ë¶ˆì´í–‰', 'ì±„ë¬´',
            'ì±„ê¶Œ', 'ê·€ì±…ì‚¬ìœ ', 'ê³¼ì‹¤', 'ì†í•´', 'ë°°ìƒ', 'ë³´ìƒ', 'ê¸ˆì§€',
            'ì˜ë¬´', 'ê¶Œë¦¬', 'ì±…ì„', 'ìœ„ë°˜', 'ì¡°ì •', 'ì¤‘ì¬', 'í•©ì˜', 'ê²°ì •',
            'ì£¼ë¬¸', 'ê²°ë¡ ', 'íŒë‹¨', 'ê·¼ê±°', 'ë²•ë ¹', 'ì¡°í•­', 'ì¡°ë¬¸', 'ì œ', 'í•­'
        }
        
        # ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ë§¤í•‘
        self.category_keywords = {
            'ì „ììƒê±°ë˜': ['íƒë°°', 'ë°°ì†¡', 'ë°˜í’ˆ', 'êµí™˜', 'í™˜ë¶ˆ', 'ê²°ì œ', 'ì£¼ë¬¸', 'ì·¨ì†Œ'],
            'í†µì‹ ì„œë¹„ìŠ¤': ['íœ´ëŒ€í°', 'ìŠ¤ë§ˆíŠ¸í°', 'ìš”ê¸ˆì œ', 'í†µì‹ ', 'ê°œí†µ', 'í•´ì§€', 'ìœ„ì•½ê¸ˆ'],
            'ìë™ì°¨': ['ìë™ì°¨', 'ì°¨ëŸ‰', 'ì •ë¹„', 'ìˆ˜ë¦¬', 'ì—”ì§„', 'íƒ€ì´ì–´', 'ë¶€í’ˆ'],
            'ê°€ì „ì œí’ˆ': ['ëƒ‰ì¥ê³ ', 'TV', 'ì„¸íƒê¸°', 'ì—ì–´ì»¨', 'ì „ìë ˆì¸ì§€', 'ì²­ì†Œê¸°'],
            'ì˜ë¥˜': ['ì˜·', 'ì˜ë¥˜', 'ë°”ì§€', 'ì…”ì¸ ', 'ì¬í‚·', 'ì½”íŠ¸', 'ì‹ ë°œ'],
            'ì‹í’ˆ': ['ìŒì‹', 'ì‹í’ˆ', 'ë¨¹ê±°ë¦¬', 'ì‹ì¬ë£Œ', 'ë°°ë‹¬ìŒì‹'],
            'ë¶€ë™ì‚°': ['ì•„íŒŒíŠ¸', 'ì£¼íƒ', 'ì „ì„¸', 'ì›”ì„¸', 'ë§¤ë§¤', 'ì„ëŒ€'],
            'ê¸ˆìœµ': ['ëŒ€ì¶œ', 'ì¹´ë“œ', 'ë³´í—˜', 'ì´ì', 'ìˆ˜ìˆ˜ë£Œ', 'í™˜ì „'],
            'ì—¬í–‰': ['í•­ê³µê¶Œ', 'í˜¸í…”', 'ìˆ™ë°•', 'ì—¬í–‰', 'íŒ¨í‚¤ì§€', 'ì·¨ì†Œìˆ˜ìˆ˜ë£Œ'],
            'êµìœ¡': ['í•™ì›', 'ê°•ì˜', 'ìˆ˜ì—…', 'êµì¬', 'í™˜ë¶ˆ'],
            'ì˜ë£Œ': ['ë³‘ì›', 'ì˜ë£Œ', 'ì§„ë£Œ', 'ì¹˜ë£Œ', 'ì•½', 'ì²˜ë°©']
        }
        
        # ì œí’ˆëª…/ë¸Œëœë“œëª… íŒ¨í„´ (íŠ¹ì • ì œí’ˆë§Œ ì¶”ì¶œ)
        self.product_patterns = [
            r'ì•„ì´í°\s?\d+(?:\s?í”„ë¡œ)?',  # ì•„ì´í° 14, ì•„ì´í° 15 í”„ë¡œ
            r'ê°¤ëŸ­ì‹œ\s?[A-Z]?\d+(?:\s?[í”ŒëŸ¬ìŠ¤|ìš¸íŠ¸ë¼])?',  # ê°¤ëŸ­ì‹œ S24, ê°¤ëŸ­ì‹œ S24 ìš¸íŠ¸ë¼
            r'(?:ëƒ‰ì¥ê³ |TV|ì„¸íƒê¸°|ì—ì–´ì»¨|ì²­ì†Œê¸°|ì „ìë ˆì¸ì§€)(?:\s?[ê°€-í£0-9]+)?',  # ê°€ì „ì œí’ˆ
            r'LG\s?[A-Z0-9]+',  # LG ì œí’ˆ
            r'Samsung\s?[A-Z0-9]+',  # Samsung ì œí’ˆ
        ]
        
        # íšŒì‚¬ëª… íŒ¨í„´
        self.company_patterns = [
            r'(?:ì£¼ì‹íšŒì‚¬|ãˆœ)\s?[ê°€-í£]+',
            r'[ê°€-í£]+(?:ì£¼ì‹íšŒì‚¬|ãˆœ)',
            r'[ê°€-í£]+\s?(?:ì½”ë¦¬ì•„|Korea)',
            r'[A-Z][a-z]+\s?(?:Korea|ì½”ë¦¬ì•„)',
        ]
        
        # ë¶ˆìš©ì–´ (í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œ ì œì™¸)
        self.stopwords = {
            'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ë˜ëŠ”', 'ë•Œ', 'ìœ„í•´',
            'ëŒ€í•œ', 'ìˆëŠ”', 'ì—†ëŠ”', 'í•˜ëŠ”', 'ë˜ëŠ”', 'ì•ŠëŠ”', 'ê°™ì€', 'ë‹¤ë¥¸',
            'ì „ì²´', 'ì¼ë¶€', 'ê°', 'ë§¤', 'ì•½', 'ë”', 'ëœ', 'ì¢€', 'ì˜', 'ëª»'
        }
    
    def extract_keywords(self, content: str, top_k: int = 10) -> List[str]:
        """
        í‚¤ì›Œë“œ ì¶”ì¶œ (ë¹ˆë„ ê¸°ë°˜)
        
        Args:
            content: ì²­í¬ ë‚´ìš©
            top_k: ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜
        
        Returns:
            í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        # í•œê¸€, ì˜ë¬¸ë§Œ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
        words = re.findall(r'[ê°€-í£]{2,}|[A-Za-z]{3,}', content)
        
        # ë¶ˆìš©ì–´ ì œê±°
        words = [w for w in words if w not in self.stopwords]
        
        # ë¹ˆë„ ê³„ì‚°
        word_counts = Counter(words)
        
        # ìƒìœ„ kê°œ ì¶”ì¶œ
        keywords = [word for word, count in word_counts.most_common(top_k)]
        
        return keywords
    
    def extract_entities(self, content: str) -> Dict[str, List[str]]:
        """
        ì—”í‹°í‹° ì¶”ì¶œ (íšŒì‚¬ëª…, ì œí’ˆëª…)
        
        Args:
            content: ì²­í¬ ë‚´ìš©
        
        Returns:
            ì—”í‹°í‹° ë”•ì…”ë„ˆë¦¬ {'companies': [...], 'products': [...]}
        """
        entities = {
            'companies': [],
            'products': []
        }
        
        # íšŒì‚¬ëª… ì¶”ì¶œ
        for pattern in self.company_patterns:
            matches = re.findall(pattern, content)
            entities['companies'].extend(matches)
        
        # ì œí’ˆëª… ì¶”ì¶œ
        for pattern in self.product_patterns:
            matches = re.findall(pattern, content)
            entities['products'].extend(matches)
        
        # ì¤‘ë³µ ì œê±°
        entities['companies'] = list(set(entities['companies']))
        entities['products'] = list(set(entities['products']))
        
        return entities
    
    def extract_legal_terms(self, content: str) -> List[str]:
        """
        ë²•ë¥  ìš©ì–´ ì¶”ì¶œ
        
        Args:
            content: ì²­í¬ ë‚´ìš©
        
        Returns:
            ë²•ë¥  ìš©ì–´ ë¦¬ìŠ¤íŠ¸
        """
        found_terms = []
        
        for term in self.legal_terms:
            if term in content:
                found_terms.append(term)
        
        return found_terms
    
    def infer_category(self, content: str) -> List[str]:
        """
        ì¹´í…Œê³ ë¦¬ íƒœê¹… (í‚¤ì›Œë“œ ê¸°ë°˜)
        
        Args:
            content: ì²­í¬ ë‚´ìš©
        
        Returns:
            ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        categories = []
        
        for category, keywords in self.category_keywords.items():
            # í‚¤ì›Œë“œê°€ ë‚´ìš©ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            match_count = sum(1 for kw in keywords if kw in content)
            
            # 2ê°œ ì´ìƒ ë§¤ì¹­ë˜ë©´ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜
            if match_count >= 2:
                categories.append(category)
        
        return categories
    
    def extract_law_references(self, content: str) -> List[str]:
        """
        ë²•ë ¹ ì°¸ì¡° ì¶”ì¶œ (ì˜ˆ: "ë¯¼ë²• ì œ750ì¡°", "ì†Œë¹„ìê¸°ë³¸ë²• ì œ16ì¡°")
        
        Args:
            content: ì²­í¬ ë‚´ìš©
        
        Returns:
            ë²•ë ¹ ì°¸ì¡° ë¦¬ìŠ¤íŠ¸
        """
        # íŒ¨í„´: ë²•ë ¹ëª… + ì œ + ìˆ«ì + ì¡°
        pattern = r'[ê°€-í£]+ë²•\s?ì œ\s?\d+ì¡°(?:\s?ì œ\s?\d+í•­)?'
        matches = re.findall(pattern, content)
        
        return list(set(matches))
    
    def extract_dates(self, content: str) -> List[str]:
        """
        ë‚ ì§œ ì¶”ì¶œ (YYYY-MM-DD, YYYY.MM.DD ë“±)
        
        Args:
            content: ì²­í¬ ë‚´ìš©
        
        Returns:
            ë‚ ì§œ ë¦¬ìŠ¤íŠ¸
        """
        patterns = [
            r'\d{4}[-./]\d{1,2}[-./]\d{1,2}',  # 2024-01-15
            r'\d{4}ë…„\s?\d{1,2}ì›”\s?\d{1,2}ì¼',  # 2024ë…„ 1ì›” 15ì¼
        ]
        
        dates = []
        for pattern in patterns:
            matches = re.findall(pattern, content)
            dates.extend(matches)
        
        return list(set(dates))
    
    def enrich_chunk_metadata(self, chunk: Dict, extract_all: bool = True) -> Dict:
        """
        ì²­í¬ ë©”íƒ€ë°ì´í„° ë³´ê°•
        
        Args:
            chunk: ì²­í¬ ë°ì´í„°
            extract_all: ëª¨ë“  ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì—¬ë¶€
        
        Returns:
            ë©”íƒ€ë°ì´í„°ê°€ ë³´ê°•ëœ ì²­í¬
        """
        content = chunk.get('content', '')
        
        if not content or not content.strip():
            return chunk
        
        # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬)
        metadata = chunk.get('metadata', {})
        
        if extract_all:
            # 1. í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self.extract_keywords(content, top_k=10)
            if keywords:
                metadata['keywords'] = keywords
            
            # 2. ì—”í‹°í‹° ì¶”ì¶œ
            entities = self.extract_entities(content)
            if entities['companies'] or entities['products']:
                metadata['entities'] = entities
            
            # 3. ë²•ë¥  ìš©ì–´ ì¶”ì¶œ
            legal_terms = self.extract_legal_terms(content)
            if legal_terms:
                metadata['legal_terms'] = legal_terms
            
            # 4. ì¹´í…Œê³ ë¦¬ íƒœê¹…
            categories = self.infer_category(content)
            if categories:
                metadata['category_tags'] = categories
            
            # 5. ë²•ë ¹ ì°¸ì¡° ì¶”ì¶œ
            law_refs = self.extract_law_references(content)
            if law_refs:
                metadata['law_references'] = law_refs
            
            # 6. ë‚ ì§œ ì¶”ì¶œ
            dates = self.extract_dates(content)
            if dates:
                metadata['dates'] = dates
        
        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        chunk['metadata'] = metadata
        
        return chunk
    
    def enrich_document_metadata(self, doc_data: Dict, extract_all: bool = True) -> Dict:
        """
        ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ì— ëŒ€í•´ ë©”íƒ€ë°ì´í„° ë³´ê°•
        
        Args:
            doc_data: ë¬¸ì„œ ë°ì´í„°
            extract_all: ëª¨ë“  ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì—¬ë¶€
        
        Returns:
            ë©”íƒ€ë°ì´í„°ê°€ ë³´ê°•ëœ ë¬¸ì„œ
        """
        for chunk in doc_data.get('chunks', []):
            self.enrich_chunk_metadata(chunk, extract_all=extract_all)
        
        return doc_data


def test_enricher():
    """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    enricher = MetadataEnricher()
    
    # í…ŒìŠ¤íŠ¸ ì²­í¬
    test_chunk = {
        'chunk_id': 'test:001',
        'chunk_type': 'judgment',
        'content': '''
        ì†Œë¹„ìê°€ ì£¼ì‹íšŒì‚¬ ì‚¼ì„±ì „ìì—ì„œ êµ¬ë§¤í•œ ê°¤ëŸ­ì‹œ S24 ìŠ¤ë§ˆíŠ¸í°ì— í•˜ìê°€ ë°œìƒí•˜ì—¬
        í™˜ë¶ˆì„ ìš”ì²­í•˜ì˜€ìœ¼ë‚˜ íŒë§¤ìê°€ ì´ë¥¼ ê±°ë¶€í•œ ì‚¬ê±´ì…ë‹ˆë‹¤.
        ë¯¼ë²• ì œ750ì¡°ì— ë”°ë¥´ë©´ ë¶ˆë²•í–‰ìœ„ë¡œ ì¸í•œ ì†í•´ë°°ìƒ ì±…ì„ì´ ìˆìœ¼ë©°,
        ì†Œë¹„ìê¸°ë³¸ë²• ì œ16ì¡°ì—ì„œëŠ” ì†Œë¹„ìì˜ ê¶Œë¦¬ë¥¼ ë³´í˜¸í•˜ê³  ìˆìŠµë‹ˆë‹¤.
        2024ë…„ 1ì›” 15ì¼ì— ê³„ì•½ì´ ì²´ê²°ë˜ì—ˆê³ , ë°°ì†¡ì€ 2024.01.20ì— ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.
        ''',
        'content_length': 200,
        'drop': False
    }
    
    # ë©”íƒ€ë°ì´í„° ë³´ê°•
    enriched = enricher.enrich_chunk_metadata(test_chunk)
    
    print("=" * 80)
    print("ë©”íƒ€ë°ì´í„° ë³´ê°• í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    print(f"\nì›ë³¸ ì²­í¬ ID: {test_chunk['chunk_id']}")
    print(f"ë‚´ìš© ê¸¸ì´: {len(test_chunk['content'])}ì")
    
    metadata = enriched['metadata']
    
    print(f"\nğŸ“Œ í‚¤ì›Œë“œ ({len(metadata.get('keywords', []))}ê°œ):")
    for kw in metadata.get('keywords', []):
        print(f"  - {kw}")
    
    print(f"\nğŸ¢ ì—”í‹°í‹°:")
    entities = metadata.get('entities', {})
    if entities.get('companies'):
        print(f"  íšŒì‚¬ëª…: {', '.join(entities['companies'])}")
    if entities.get('products'):
        print(f"  ì œí’ˆëª…: {', '.join(entities['products'])}")
    
    print(f"\nâš–ï¸  ë²•ë¥  ìš©ì–´ ({len(metadata.get('legal_terms', []))}ê°œ):")
    for term in metadata.get('legal_terms', [])[:10]:
        print(f"  - {term}")
    
    print(f"\nğŸ“‚ ì¹´í…Œê³ ë¦¬ íƒœê·¸:")
    for cat in metadata.get('category_tags', []):
        print(f"  - {cat}")
    
    print(f"\nğŸ“œ ë²•ë ¹ ì°¸ì¡°:")
    for ref in metadata.get('law_references', []):
        print(f"  - {ref}")
    
    print(f"\nğŸ“… ë‚ ì§œ:")
    for date in metadata.get('dates', []):
        print(f"  - {date}")


if __name__ == '__main__':
    test_enricher()
