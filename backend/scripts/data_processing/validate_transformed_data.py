#!/usr/bin/env python3
"""
ë³€í™˜ëœ ë°ì´í„° ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

ë³€í™˜ëœ JSON ë°ì´í„°ë¥¼ ê²€í† í•˜ì—¬ ì„ë² ë”© ì§„í–‰ ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨
"""

import json
import re
from pathlib import Path
from typing import Dict, List, Tuple
from collections import defaultdict

# ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
SCRIPT_DIR = Path(__file__).resolve().parent  # scripts/data_processing/
PROJECT_ROOT = SCRIPT_DIR.parent.parent.parent  # ddoksori_demo/
DATA_DIR = PROJECT_ROOT / "backend" / "data"

# ì²­í¬ íƒ€ì…ë³„ ì²˜ë¦¬ ê·œì¹™ (data_transform_pipeline.pyì™€ ë™ì¼)
CHUNK_PROCESSING_RULES = {
    'decision': {
        'min_length': 50,
        'max_length': 800,
        'drop_if_empty': True
    },
    'reasoning': {
        'min_length': 100,
        'max_length': 1500,
    },
    'judgment': {
        'min_length': 200,
        'max_length': 1500,
    },
    'law': {
        'min_length': 30,
        'max_length': 2000,
        'drop_if_empty': True,
    },
    'law_reference': {
        'min_length': 20,
        'max_length': 2000,
        'drop_if_empty': True
    },
    'resolution_row': {
        'min_length': 50,
        'max_length': 2000,
    },
    'qa_combined': {
        'min_length': 100,
        'max_length': 1500,
    },
    # ê¸°ë³¸ ê·œì¹™
    'default': {
        'min_length': 100,
        'max_length': 1500,
        'drop_if_empty': True
    }
}

# ì²­í¬ íƒ€ì…ë³„ ì²˜ë¦¬ ê·œì¹™
CHUNK_PROCESSING_RULES = {
    'decision': {
        'min_length': 50,
        'max_length': 800,
        'merge_allowed': False,  # ê²°ì •ë¬¸ì€ ë…ë¦½ì„± ìœ ì§€
        'drop_if_empty': True,
        'description': 'ê²°ì •ë¬¸'
    },
    'reasoning': {
        'min_length': 100,
        'max_length': 1500,
        'merge_allowed': True,
        'split_allowed': True,
        'description': 'íŒë‹¨ ê·¼ê±°'
    },
    'judgment': {
        'min_length': 200,
        'max_length': 1500,
        'split_allowed': True,  # ê¸´ íŒë‹¨ ë‚´ìš© ë¶„í• 
        'description': 'íŒë‹¨ ë‚´ìš©'
    },
    'law': {
        'min_length': 30,
        'max_length': 2000,
        'drop_if_empty': True,
        'enrich_with_metadata': True,  # ë©”íƒ€ë°ì´í„°ë¡œ ë‚´ìš© ë³´ê°•
        'description': 'ê´€ë ¨ ë²•ë ¹'
    },
    'summary': {
        'min_length': 50,
        'max_length': 1000,
        'description': 'ìš”ì•½'
    },
    'full_text': {
        'min_length': 100,
        'max_length': 2000,
        'split_allowed': True,
        'description': 'ì „ë¬¸'
    },
    'case_info': {
        'min_length': 30,
        'max_length': 1000,
        'description': 'ì‚¬ê±´ ì •ë³´'
    },
    'default': {
        'min_length': 50,
        'max_length': 1500,
        'description': 'ê¸°íƒ€'
    }
}


def has_meaningful_content(content: str) -> bool:
    """ì˜ë¯¸ ìˆëŠ” ë‚´ìš©ì¸ì§€ ì²´í¬"""
    if not content or not content.strip():
        return False
    
    # ê³µë°± ì œê±°
    cleaned = content.strip()
    
    # 1. ë„ˆë¬´ ì§§ì€ ê²½ìš°
    if len(cleaned) < 5:
        return False
    
    # 2. ì˜ë¯¸ ì—†ëŠ” íŒ¨í„´ë“¤
    meaningless_patterns = [
        r'^[ê°€-í£]\.$',  # ë‹¨ì¼ ë¬¸ì + ë§ˆì¹¨í‘œ (ì˜ˆ: "ê°€.", "ë‚˜.")
        r'^[0-9]+\.$',   # ìˆ«ì + ë§ˆì¹¨í‘œ (ì˜ˆ: "1.", "2.")
        r'^[\s\n\r\t]+$',  # ê³µë°±ë§Œ
        r'^[-=_*#]+$',   # êµ¬ë¶„ì„ ë§Œ
    ]
    
    for pattern in meaningless_patterns:
        if re.match(pattern, cleaned):
            return False
    
    # 3. í•œê¸€/ì˜ë¬¸ ë¬¸ìê°€ ìµœì†Œ 5ì ì´ìƒ ìˆì–´ì•¼ í•¨
    text_chars = re.findall(r'[ê°€-í£a-zA-Z]', cleaned)
    if len(text_chars) < 5:
        return False
    
    return True


def estimate_token_count(text: str) -> int:
    """
    í† í° ìˆ˜ ì¶”ì • (í•œêµ­ì–´ ê¸°ì¤€)
    
    í•œêµ­ì–´ í† í° ë³€í™˜ìœ¨: ì•½ 1.5-2ì = 1í† í°
    ë³´ìˆ˜ì  ì¶”ì •: 1.5ì = 1í† í°
    
    Args:
        text: í† í° ìˆ˜ë¥¼ ì¶”ì •í•  í…ìŠ¤íŠ¸
        
    Returns:
        ì¶”ì •ëœ í† í° ìˆ˜
    """
    return int(len(text) / 1.5)


def validate_token_limits(chunks: List[Dict], max_tokens: int = 512) -> Dict:
    """
    í† í° ì œí•œ ê²€ì¦
    
    Args:
        chunks: ê²€ì¦í•  ì²­í¬ ë¦¬ìŠ¤íŠ¸
        max_tokens: ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸: 512, KURE-v1 ëª¨ë¸ ì œí•œ)
        
    Returns:
        {
            'violations': List[Dict],
            'stats_by_type': Dict,
            'total_violations': int
        }
    """
    violations = []
    stats_by_type = defaultdict(lambda: {
        'count': 0,
        'avg_tokens': 0,
        'max_tokens': 0,
        'violation_count': 0
    })
    
    for chunk in chunks:
        if chunk.get('drop', False):
            continue
        
        chunk_type = chunk.get('chunk_type', 'unknown')
        content = chunk.get('content', '')
        estimated_tokens = estimate_token_count(content)
        
        stats = stats_by_type[chunk_type]
        stats['count'] += 1
        stats['avg_tokens'] = (stats['avg_tokens'] * (stats['count'] - 1) + estimated_tokens) / stats['count']
        stats['max_tokens'] = max(stats['max_tokens'], estimated_tokens)
        
        if estimated_tokens > max_tokens:
            stats['violation_count'] += 1
            violations.append({
                'chunk_id': chunk.get('chunk_id'),
                'chunk_type': chunk_type,
                'char_count': len(content),
                'estimated_tokens': estimated_tokens,
                'excess_tokens': estimated_tokens - max_tokens
            })
    
    return {
        'violations': violations,
        'stats_by_type': dict(stats_by_type),
        'total_violations': len(violations)
    }


def check_encoding_quality(content: str) -> Tuple[bool, str]:
    """ì¸ì½”ë”© ê°€ëŠ¥ ì—¬ë¶€ ë° í’ˆì§ˆ ì²´í¬"""
    if not content:
        return False, "ë¹ˆ ì½˜í…ì¸ "
    
    try:
        # UTF-8 ì¸ì½”ë”© ê°€ëŠ¥ ì—¬ë¶€
        content.encode('utf-8')
    except UnicodeEncodeError as e:
        return False, f"UTF-8 ì¸ì½”ë”© ì˜¤ë¥˜: {e}"
    
    # íŠ¹ìˆ˜ ë¬¸ì ë¹„ìœ¨ ì²´í¬ (ë„ˆë¬´ ë†’ìœ¼ë©´ ê¹¨ì§„ í…ìŠ¤íŠ¸ì¼ ê°€ëŠ¥ì„±)
    total_chars = len(content)
    if total_chars == 0:
        return False, "ë¹ˆ ì½˜í…ì¸ "
    
    # ì¼ë°˜ ë¬¸ì (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸)
    normal_chars = len(re.findall(r'[ê°€-í£a-zA-Z0-9\s.,!?;:()\[\]{}"\'-]', content))
    normal_ratio = normal_chars / total_chars
    
    if normal_ratio < 0.8:  # ì •ìƒ ë¬¸ìê°€ 80% ë¯¸ë§Œì´ë©´ ì˜ì‹¬
        return False, f"ë¹„ì •ìƒ ë¬¸ì ë¹„ìœ¨ ë†’ìŒ ({normal_ratio:.1%})"
    
    return True, "ì •ìƒ"

class TransformedDataValidator:
    """ë³€í™˜ëœ ë°ì´í„° ê²€ì¦"""
    
    def __init__(self, data_dir: Path = None):
        if data_dir is None:
            data_dir = DATA_DIR / "transformed"
        self.data_dir = Path(data_dir)
        self.issues = []
        self.warnings = []
        self.stats = defaultdict(int)
    
    def load_all_data(self) -> Dict[str, Dict]:
        """ëª¨ë“  ë³€í™˜ëœ JSON íŒŒì¼ ë¡œë“œ"""
        data = {}
        
        json_files = list(self.data_dir.glob('*.json'))
        if not json_files:
            print(f"âŒ {self.data_dir}ì— JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return data
        
        for json_file in json_files:
            if json_file.name == 'transformation_summary.json':
                continue
            
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data[json_file.stem] = json.load(f)
                print(f"âœ… ë¡œë“œ: {json_file.name}")
            except Exception as e:
                print(f"âŒ ë¡œë“œ ì‹¤íŒ¨: {json_file.name} - {e}")
        
        return data
    
    def validate_chunk_indices(self, doc_data: Dict) -> bool:
        """chunk_index ê²€ì¦"""
        doc_id = doc_data['doc_id']
        chunks = doc_data['chunks']
        
        is_valid = True
        
        # 1. chunk_indexê°€ 0ë¶€í„° ì‹œì‘í•˜ëŠ”ì§€
        min_index = min(c['chunk_index'] for c in chunks)
        if min_index != 0:
            self.issues.append(
                f"âŒ {doc_id}: chunk_indexê°€ 0ë¶€í„° ì‹œì‘í•˜ì§€ ì•ŠìŒ (ì‹œì‘: {min_index})"
            )
            is_valid = False
        
        # 2. chunk_indexê°€ ì—°ì†ì ì¸ì§€
        indices = sorted(c['chunk_index'] for c in chunks)
        expected_indices = list(range(len(chunks)))
        if indices != expected_indices:
            self.issues.append(
                f"âŒ {doc_id}: chunk_indexê°€ ì—°ì†ì ì´ì§€ ì•ŠìŒ\n"
                f"   Expected: {expected_indices}\n"
                f"   Actual: {indices}"
            )
            is_valid = False
        
        # 3. chunk_totalì´ ì¼ì¹˜í•˜ëŠ”ì§€
        for chunk in chunks:
            if chunk['chunk_total'] != len(chunks):
                self.issues.append(
                    f"âŒ {doc_id}: chunk_total ë¶ˆì¼ì¹˜\n"
                    f"   chunk_id: {chunk['chunk_id']}\n"
                    f"   Expected: {len(chunks)}\n"
                    f"   Actual: {chunk['chunk_total']}"
                )
                is_valid = False
                break
        
        # 4. chunk_index < chunk_total í™•ì¸
        for chunk in chunks:
            if chunk['chunk_index'] >= chunk['chunk_total']:
                self.issues.append(
                    f"âŒ {doc_id}: chunk_index >= chunk_total\n"
                    f"   chunk_id: {chunk['chunk_id']}\n"
                    f"   chunk_index: {chunk['chunk_index']}\n"
                    f"   chunk_total: {chunk['chunk_total']}"
                )
                is_valid = False
                break
        
        return is_valid
    
    def validate_required_fields(self, doc_data: Dict) -> bool:
        """í•„ìˆ˜ í•„ë“œ í™•ì¸"""
        doc_id = doc_data['doc_id']
        is_valid = True
        
        # ë¬¸ì„œ í•„ìˆ˜ í•„ë“œ
        doc_required = ['doc_id', 'doc_type', 'title', 'source_org', 'chunks']
        for field in doc_required:
            if field not in doc_data:
                self.issues.append(f"âŒ {doc_id}: ë¬¸ì„œì— '{field}' í•„ë“œ ì—†ìŒ")
                is_valid = False
        
        # ì²­í¬ í•„ìˆ˜ í•„ë“œ
        chunk_required = ['chunk_id', 'chunk_index', 'chunk_total', 'chunk_type', 'content', 'content_length', 'drop']
        for chunk in doc_data.get('chunks', []):
            for field in chunk_required:
                if field not in chunk:
                    self.issues.append(
                        f"âŒ {doc_id}: ì²­í¬ {chunk.get('chunk_id', 'unknown')}ì— '{field}' í•„ë“œ ì—†ìŒ"
                    )
                    is_valid = False
                    break
        
        return is_valid
    
    def validate_content_quality(self, doc_data: Dict) -> bool:
        """ê°•í™”ëœ ì½˜í…ì¸  í’ˆì§ˆ ê²€ì¦"""
        doc_id = doc_data['doc_id']
        is_valid = True
        
        for chunk in doc_data.get('chunks', []):
            content = chunk.get('content', '')
            content_length = chunk.get('content_length', 0)
            chunk_type = chunk.get('chunk_type', 'default')
            chunk_id = chunk.get('chunk_id', 'unknown')
            should_drop = chunk.get('drop', False)
            
            # ì²­í¬ íƒ€ì…ë³„ ê·œì¹™ ê°€ì ¸ì˜¤ê¸°
            rules = CHUNK_PROCESSING_RULES.get(chunk_type, CHUNK_PROCESSING_RULES['default'])
            min_length = rules['min_length']
            max_length = rules['max_length']
            
            # 1. ë¹ˆ ì½˜í…ì¸  ì²´í¬ (Critical)
            if not content or not content.strip():
                # drop=Trueì¸ ê²½ìš°ëŠ” í—ˆìš©
                if should_drop:
                    self.stats[f'dropped_empty_{chunk_type}'] += 1
                elif rules.get('drop_if_empty', False):
                    self.warnings.append(
                        f"âš ï¸  {doc_id}: ë¹ˆ content (drop=Trueë¡œ ì„¤ì • ê¶Œì¥)\n"
                        f"   chunk_id: {chunk_id}\n"
                        f"   chunk_type: {chunk_type}"
                    )
                else:
                    self.issues.append(
                        f"âŒ {doc_id}: contentê°€ ë¹„ì–´ìˆìŒ\n"
                        f"   chunk_id: {chunk_id}\n"
                        f"   chunk_type: {chunk_type}"
                    )
                    is_valid = False
                continue
            
            # 2. ì¸ì½”ë”© í’ˆì§ˆ ì²´í¬ (Critical)
            encoding_ok, encoding_msg = check_encoding_quality(content)
            if not encoding_ok:
                self.issues.append(
                    f"âŒ {doc_id}: ì¸ì½”ë”© ì˜¤ë¥˜\n"
                    f"   chunk_id: {chunk_id}\n"
                    f"   ì˜¤ë¥˜: {encoding_msg}"
                )
                is_valid = False
                continue
            
            # 3. ì˜ë¯¸ ìˆëŠ” ë‚´ìš© ì²´í¬
            if not has_meaningful_content(content):
                if should_drop:
                    self.stats[f'dropped_meaningless_{chunk_type}'] += 1
                else:
                    self.warnings.append(
                        f"âš ï¸  {doc_id}: ì˜ë¯¸ ì—†ëŠ” ë‚´ìš© (drop=True ê¶Œì¥)\n"
                        f"   chunk_id: {chunk_id}\n"
                        f"   content: {content[:100]}"
                    )
            
            # 4. íƒ€ì…ë³„ ìµœì†Œ ê¸¸ì´ ì²´í¬
            if content_length < min_length and not should_drop:
                severity = "ê²½ê³ " if rules.get('merge_allowed', False) else "ì£¼ì˜"
                self.warnings.append(
                    f"âš ï¸  {doc_id}: ì²­í¬ê°€ ìµœì†Œ ê¸¸ì´ ë¯¸ë‹¬ ({severity})\n"
                    f"   chunk_id: {chunk_id}\n"
                    f"   chunk_type: {chunk_type} (ìµœì†Œ: {min_length}ì)\n"
                    f"   ì‹¤ì œ ê¸¸ì´: {content_length}ì\n"
                    f"   ë‚´ìš©: {content[:100]}"
                )
                self.stats[f'too_short_{chunk_type}'] += 1
            
            # 5. íƒ€ì…ë³„ ìµœëŒ€ ê¸¸ì´ ì²´í¬
            if content_length > max_length:
                severity = "ê¶Œì¥" if rules.get('split_allowed', False) else "ì£¼ì˜"
                self.warnings.append(
                    f"âš ï¸  {doc_id}: ì²­í¬ê°€ ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼ ({severity} ë¶„í• )\n"
                    f"   chunk_id: {chunk_id}\n"
                    f"   chunk_type: {chunk_type} (ìµœëŒ€: {max_length}ì)\n"
                    f"   ì‹¤ì œ ê¸¸ì´: {content_length:,}ì"
                )
                self.stats[f'too_long_{chunk_type}'] += 1
            
            # 6. content_lengthê°€ ì‹¤ì œ ê¸¸ì´ì™€ ë‹¤ë¦„
            actual_length = len(content)
            if abs(actual_length - content_length) > 5:  # 5ì ì´ìƒ ì°¨ì´
                self.warnings.append(
                    f"âš ï¸  {doc_id}: content_length ë¶ˆì¼ì¹˜\n"
                    f"   chunk_id: {chunk_id}\n"
                    f"   Expected: {content_length}\n"
                    f"   Actual: {actual_length}"
                )
            
            # 7. RAG ìµœì  ë²”ìœ„ ì²´í¬ (100-2000ì)
            if not should_drop:
                if 100 <= content_length <= 2000:
                    self.stats['optimal_chunks'] += 1
                elif content_length < 100:
                    self.stats['suboptimal_too_short'] += 1
                else:
                    self.stats['suboptimal_too_long'] += 1
        
        return is_valid
    
    def validate_document(self, doc_data: Dict) -> bool:
        """ë‹¨ì¼ ë¬¸ì„œ ê²€ì¦"""
        doc_id = doc_data.get('doc_id', 'unknown')
        
        # 1. í•„ìˆ˜ í•„ë“œ í™•ì¸
        if not self.validate_required_fields(doc_data):
            return False
        
        # 2. chunk_index ê²€ì¦
        if not self.validate_chunk_indices(doc_data):
            return False
        
        # 3. ì½˜í…ì¸  í’ˆì§ˆ ê²€ì¦
        if not self.validate_content_quality(doc_data):
            return False
        
        # í†µê³„ ìˆ˜ì§‘
        self.stats['total_documents'] += 1
        self.stats['total_chunks'] += len(doc_data.get('chunks', []))
        self.stats[f"doc_type_{doc_data['doc_type']}"] += 1
        self.stats[f"source_org_{doc_data['source_org']}"] += 1
        
        return True
    
    def validate_all(self, data: Dict[str, Dict]) -> Tuple[bool, Dict]:
        """ëª¨ë“  ë°ì´í„° ê²€ì¦"""
        print("\n" + "=" * 80)
        print("ë³€í™˜ ë°ì´í„° ê²€ì¦ ì‹œì‘")
        print("=" * 80)
        
        all_valid = True
        
        for file_name, file_data in data.items():
            print(f"\nğŸ“„ ê²€ì¦: {file_name}")
            
            documents = file_data.get('documents', [])
            print(f"  - ë¬¸ì„œ ìˆ˜: {len(documents)}ê°œ")
            
            for doc_data in documents:
                doc_valid = self.validate_document(doc_data)
                if not doc_valid:
                    all_valid = False
        
        # í† í° ì œí•œ ê²€ì¦
        print("\nğŸ”¬ í† í° ì œí•œ ê²€ì¦ (KURE-v1 ëª¨ë¸: 512 í† í°)")
        all_chunks = []
        for file_name, file_data in data.items():
            for doc_data in file_data.get('documents', []):
                all_chunks.extend(doc_data.get('chunks', []))
        
        token_validation = validate_token_limits(all_chunks)
        
        print(f"  - ì´ ì²­í¬: {len([c for c in all_chunks if not c.get('drop', False)]):,}ê°œ")
        print(f"  - í† í° ì´ˆê³¼: {token_validation['total_violations']:,}ê°œ")
        
        if token_validation['total_violations'] > 0:
            print(f"\n  âš ï¸  í† í° ì´ˆê³¼ ì²­í¬ íƒ€ì…ë³„ ë¶„ì„:")
            for chunk_type, stats in sorted(token_validation['stats_by_type'].items()):
                if stats['violation_count'] > 0:
                    violation_rate = stats['violation_count'] / stats['count'] * 100
                    print(f"    â€¢ {chunk_type}: {stats['violation_count']}/{stats['count']} ({violation_rate:.1f}%)")
                    print(f"      - í‰ê·  í† í°: {stats['avg_tokens']:.0f}")
                    print(f"      - ìµœëŒ€ í† í°: {stats['max_tokens']:.0f}")
        else:
            print(f"  âœ… ëª¨ë“  ì²­í¬ê°€ í† í° ì œí•œ ì¤€ìˆ˜")
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 80)
        print("ê²€ì¦ ê²°ê³¼")
        print("=" * 80)
        
        print(f"\nğŸ“Š ê¸°ë³¸ í†µê³„:")
        print(f"  - ì´ ë¬¸ì„œ: {self.stats['total_documents']:,}ê°œ")
        print(f"  - ì´ ì²­í¬: {self.stats['total_chunks']:,}ê°œ")
        
        # RAG ìµœì í™” í†µê³„
        optimal = self.stats.get('optimal_chunks', 0)
        too_short = self.stats.get('suboptimal_too_short', 0)
        too_long = self.stats.get('suboptimal_too_long', 0)
        total_checked = optimal + too_short + too_long
        
        if total_checked > 0:
            print(f"\nğŸ¯ RAG ìµœì í™” ë¶„ì„ (100-2000ì ê¸°ì¤€):")
            print(f"  - ìµœì  ë²”ìœ„: {optimal:,}ê°œ ({optimal/total_checked*100:.1f}%)")
            print(f"  - ë„ˆë¬´ ì§§ìŒ: {too_short:,}ê°œ ({too_short/total_checked*100:.1f}%)")
            print(f"  - ë„ˆë¬´ ê¹€: {too_long:,}ê°œ ({too_long/total_checked*100:.1f}%)")
        
        # ì²­í¬ íƒ€ì…ë³„ ë¬¸ì œ í†µê³„
        print(f"\nğŸ“ ì²­í¬ íƒ€ì…ë³„ ë¬¸ì œ ìš”ì•½:")
        type_issues = {}
        for key in self.stats:
            if key.startswith('too_short_') or key.startswith('too_long_'):
                parts = key.split('_', 2)
                issue_type = parts[0] + '_' + parts[1]
                chunk_type = parts[2] if len(parts) > 2 else 'unknown'
                
                if chunk_type not in type_issues:
                    type_issues[chunk_type] = {'too_short': 0, 'too_long': 0}
                
                if issue_type == 'too_short':
                    type_issues[chunk_type]['too_short'] = self.stats[key]
                elif issue_type == 'too_long':
                    type_issues[chunk_type]['too_long'] = self.stats[key]
        
        for chunk_type, issues in sorted(type_issues.items()):
            rules = CHUNK_PROCESSING_RULES.get(chunk_type, CHUNK_PROCESSING_RULES['default'])
            desc = rules['description']
            if issues['too_short'] > 0 or issues['too_long'] > 0:
                print(f"  - {desc} ({chunk_type}):")
                if issues['too_short'] > 0:
                    print(f"    â€¢ ë„ˆë¬´ ì§§ìŒ: {issues['too_short']:,}ê°œ")
                if issues['too_long'] > 0:
                    print(f"    â€¢ ë„ˆë¬´ ê¹€: {issues['too_long']:,}ê°œ")
        
        print(f"\nğŸ” ì´ìŠˆ:")
        print(f"  - âŒ Critical ì˜¤ë¥˜: {len(self.issues)}ê°œ")
        print(f"  - âš ï¸  ê²½ê³ : {len(self.warnings)}ê°œ")
        
        if self.issues:
            print(f"\nâŒ Critical ì˜¤ë¥˜ ëª©ë¡ (ìƒìœ„ 10ê°œ):")
            for issue in self.issues[:10]:
                print(f"  {issue}")
            if len(self.issues) > 10:
                print(f"  ... ì™¸ {len(self.issues) - 10}ê°œ")
        
        if self.warnings:
            print(f"\nâš ï¸  ê²½ê³  ëª©ë¡ (ìƒìœ„ 5ê°œ):")
            for warning in self.warnings[:5]:
                print(f"  {warning}")
            if len(self.warnings) > 5:
                print(f"  ... ì™¸ {len(self.warnings) - 5}ê°œ")
        
        # ê°œì„  ê¶Œì¥ì‚¬í•­
        if too_short > total_checked * 0.1:  # 10% ì´ìƒì´ ë„ˆë¬´ ì§§ì€ ê²½ìš°
            print(f"\nğŸ’¡ ê°œì„  ê¶Œì¥ì‚¬í•­:")
            print(f"  - ì§§ì€ ì²­í¬ ë³‘í•©ì„ ê¶Œì¥í•©ë‹ˆë‹¤ (data_transform_pipeline.py ìˆ˜ì •)")
        
        if too_long > total_checked * 0.05:  # 5% ì´ìƒì´ ë„ˆë¬´ ê¸´ ê²½ìš°
            print(f"  - ê¸´ ì²­í¬ ë¶„í• ì„ ê¶Œì¥í•©ë‹ˆë‹¤ (data_transform_pipeline.py ìˆ˜ì •)")
        
        print("\n" + "=" * 80)
        if all_valid and not self.issues:
            print("âœ… ê²€ì¦ í†µê³¼! ì„ë² ë”© ì§„í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
            if self.warnings:
                print("   (ê²½ê³  ì‚¬í•­ì´ ìˆìœ¼ë‚˜ ì¹˜ëª…ì ì´ì§€ ì•ŠìŒ)")
        else:
            print("âŒ ê²€ì¦ ì‹¤íŒ¨! Critical ì˜¤ë¥˜ë¥¼ ìˆ˜ì •í•œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        print("=" * 80)
        
        return all_valid, dict(self.stats)
    
    def show_sample_data(self, data: Dict[str, Dict], n: int = 2):
        """ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("ìƒ˜í”Œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
        print("=" * 80)
        
        for file_name, file_data in data.items():
            documents = file_data.get('documents', [])[:n]
            
            for doc in documents:
                print(f"\nğŸ“„ [{doc['doc_type']}] {doc['doc_id']}")
                print(f"  ì œëª©: {doc['title']}")
                print(f"  ì¶œì²˜: {doc['source_org']}")
                print(f"  ì²­í¬ ìˆ˜: {len(doc['chunks'])}ê°œ")
                
                # ì²« ë²ˆì§¸ ì²­í¬ ë¯¸ë¦¬ë³´ê¸°
                if doc['chunks']:
                    chunk = doc['chunks'][0]
                    print(f"\n  [ì²­í¬ 0] {chunk['chunk_id']}")
                    print(f"    íƒ€ì…: {chunk['chunk_type']}")
                    print(f"    ì¸ë±ìŠ¤: {chunk['chunk_index']}/{chunk['chunk_total']}")
                    print(f"    ê¸¸ì´: {chunk['content_length']}ì")
                    print(f"    drop: {chunk['drop']}")
                    content_preview = chunk['content'][:200].replace('\n', ' ')
                    print(f"    ë‚´ìš©: {content_preview}...")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    validator = TransformedDataValidator()
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("=" * 80)
    print("ë³€í™˜ëœ ë°ì´í„° ë¡œë“œ")
    print("=" * 80)
    data = validator.load_all_data()
    
    if not data:
        print("\nâŒ ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("ë¨¼ì € data_transform_pipeline.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    print(f"\nâœ… {len(data)}ê°œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
    
    # 2. ìƒ˜í”Œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
    validator.show_sample_data(data, n=1)
    
    # 3. ê²€ì¦
    all_valid, stats = validator.validate_all(data)
    
    # 4. ê²°ê³¼ ì €ì¥
    result = {
        'valid': all_valid and not validator.issues,
        'issues_count': len(validator.issues),
        'warnings_count': len(validator.warnings),
        'issues': validator.issues,
        'warnings': validator.warnings,
        'stats': stats
    }
    
    output_path = validator.data_dir / 'validation_result.json'
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ê²€ì¦ ê²°ê³¼ ì €ì¥: {output_path}")
    
    return 0 if result['valid'] else 1

if __name__ == '__main__':
    exit(main())
