#!/usr/bin/env python3
"""
ë°ì´í„° ë³€í™˜ íŒŒì´í”„ë¼ì¸

ì›ë³¸ JSONL ë°ì´í„°ë¥¼ PostgreSQL ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ë³€í™˜í•˜ì—¬:
1. JSON íŒŒì¼ë¡œ ì €ì¥ (ê²€í† ìš©)
2. PostgreSQLì— ì‚½ì…

Features:
- ëª¨ë“  chunk_indexë¥¼ 0-basedë¡œ í†µì¼
- ë³€í™˜ ì „í›„ ê²€ì¦
- ì§„í–‰ ìƒí™© ì €ì¥ (ì¤‘ë‹¨ ì‹œ ì¬ê°œ ê°€ëŠ¥)
- ìƒì„¸í•œ ë¡œê·¸
"""

import json
import os
import glob
from typing import List, Dict, Any
from pathlib import Path
from datetime import datetime
import psycopg2
from psycopg2.extras import execute_batch
from dotenv import load_dotenv

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€í•˜ì—¬ metadata_enricher import ê°€ëŠ¥í•˜ë„ë¡
import sys
current_dir = Path(__file__).resolve().parent
if str(current_dir) not in sys.path:
    sys.path.insert(0, str(current_dir))

from metadata_enricher import MetadataEnricher

load_dotenv()

# ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
SCRIPT_DIR = Path(__file__).resolve().parent  # scripts/data_processing/
PROJECT_ROOT = SCRIPT_DIR.parent.parent.parent  # ddoksori_demo/
DATA_DIR = PROJECT_ROOT / "backend" / "data"

# ì²­í¬ íƒ€ì…ë³„ ì²˜ë¦¬ ê·œì¹™ (ê°œì„ ë¨ - íƒ€ì…ë³„ ìµœì  ê¸¸ì´ ì°¨ë³„í™”)
# í† í° ì œí•œ: KURE-v1 ëª¨ë¸ 512 í† í° = ì•½ 768-1024ì
# ì•ˆì „ ë²”ìœ„: íƒ€ì…ì— ë”°ë¼ 400-800ìë¡œ ì°¨ë³„í™”
CHUNK_PROCESSING_RULES = {
    'decision': {
        'min_length': 100,
        'max_length': 600,  # ê°œì„ : ê°„ê²°í•œ ê²°ì •ë¬¸ì— ìµœì í™” (700 â†’ 600)
        'target_length': 500,  # ê°œì„ : ëª©í‘œ í¬ê¸° ì¶•ì†Œ (600 â†’ 500)
        'merge_allowed': False,  # ê²°ì •ë¬¸ì€ ë…ë¦½ì„± ìœ ì§€
        'split_allowed': True,
        'overlap_size': 100,
        'overlap_mode': 'sentence',  # ì‹ ê·œ: ë¬¸ì¥ ë‹¨ìœ„ ì¤‘ì²©
        'drop_if_empty': True,
        'description': 'ì£¼ë¬¸(ê²°ì •) - ê°„ê²°í•œ ê²°ì •ë¬¸'
    },
    'reasoning': {
        'min_length': 150,
        'max_length': 800,  # ê°œì„ : ìƒì„¸í•œ ë…¼ë¦¬ ì „ê°œ í—ˆìš© (700 â†’ 800)
        'target_length': 700,  # ê°œì„ : ëª©í‘œ í¬ê¸° ì¦ê°€ (600 â†’ 700)
        'merge_allowed': True,
        'split_allowed': True,
        'overlap_size': 150,
        'overlap_mode': 'sentence',
        'description': 'ì´ìœ (ê·¼ê±°) - ìƒì„¸í•œ ë…¼ë¦¬ ì „ê°œ'
    },
    'judgment': {
        'min_length': 150,
        'max_length': 800,  # ê°œì„ : íŒë‹¨ ë‚´ìš©ì— ì¶©ë¶„í•œ ê³µê°„ (700 â†’ 800)
        'target_length': 700,  # ê°œì„ : ëª©í‘œ í¬ê¸° ì¦ê°€ (600 â†’ 700)
        'merge_allowed': True,
        'split_allowed': True,
        'overlap_size': 150,
        'overlap_mode': 'sentence',
        'description': 'íŒë‹¨ - ë²•ì  ê·¼ê±°ì™€ íŒë‹¨'
    },
    'parties_claim': {
        'min_length': 150,
        'max_length': 750,  # ê°œì„ : ë‹¹ì‚¬ì ì£¼ì¥ì— ì ì ˆí•œ ê¸¸ì´
        'target_length': 650,
        'merge_allowed': True,
        'split_allowed': True,
        'overlap_size': 150,
        'overlap_mode': 'sentence',
        'description': 'ë‹¹ì‚¬ì ì£¼ì¥ - ê¸°ì´ˆì‚¬ì‹¤'
    },
    'law': {
        'min_length': 50,
        'max_length': 500,  # ê°œì„ : ì¡°ë¬¸ ë‹¨ìœ„ë¡œ ì§§ê²Œ ìœ ì§€ (700 â†’ 500)
        'target_length': 400,  # ê°œì„ : ëª©í‘œ í¬ê¸° ì¶•ì†Œ (600 â†’ 400)
        'merge_allowed': False,
        'split_allowed': True,
        'overlap_size': 80,  # ê°œì„ : ì¤‘ì²© í¬ê¸° ì¶•ì†Œ (100 â†’ 80)
        'overlap_mode': 'sentence',
        'drop_if_empty': True,
        'enrich_with_metadata': True,
        'description': 'ë²•ë ¹ ì¡°ë¬¸'
    },
    'law_reference': {
        'min_length': 50,
        'max_length': 500,  # ê°œì„ : ë²•ë ¹ ì°¸ì¡°ëŠ” ê°„ê²°í•˜ê²Œ (700 â†’ 500)
        'target_length': 400,
        'merge_allowed': False,
        'split_allowed': True,
        'overlap_size': 80,
        'overlap_mode': 'sentence',
        'drop_if_empty': True,
        'description': 'ë²•ë ¹ ì°¸ì¡°'
    },
    'resolution_row': {
        'min_length': 100,
        'max_length': 700,
        'target_length': 600,
        'merge_allowed': False,
        'split_allowed': True,
        'overlap_size': 100,
        'overlap_mode': 'sentence',
        'description': 'ì†Œë¹„ìë¶„ìŸí•´ê²°ê¸°ì¤€ í–‰'
    },
    'qa_combined': {
        'min_length': 150,
        'max_length': 700,
        'target_length': 600,
        'merge_allowed': False,  # Q&A ìŒì€ ë…ë¦½ì„± ìœ ì§€
        'split_allowed': True,
        'overlap_size': 100,
        'overlap_mode': 'sentence',
        'description': 'ì§ˆì˜ì‘ë‹µ í†µí•©'
    },
    'article': {
        'min_length': 100,
        'max_length': 500,  # ê°œì„ : ì¡°ë¬¸ì€ ì§§ê²Œ (700 â†’ 500)
        'target_length': 400,
        'merge_allowed': False,
        'split_allowed': True,
        'overlap_size': 80,
        'overlap_mode': 'sentence',
        'description': 'ì¡°ë¬¸'
    },
    'paragraph': {
        'min_length': 100,
        'max_length': 600,  # ê°œì„ : í•­ ë‹¨ìœ„ (700 â†’ 600)
        'target_length': 500,
        'merge_allowed': True,
        'split_allowed': True,
        'overlap_size': 100,
        'overlap_mode': 'sentence',
        'description': 'í•­'
    },
    # ê¸°ë³¸ ê·œì¹™
    'default': {
        'min_length': 100,
        'max_length': 700,
        'target_length': 600,
        'merge_allowed': True,
        'split_allowed': True,
        'overlap_size': 100,
        'overlap_mode': 'sentence',
        'drop_if_empty': True,
        'description': 'ê¸°ë³¸ ê·œì¹™'
    }
}

class DataTransformer:
    """ë°ì´í„° ë³€í™˜ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, output_dir: Path = None, use_db: bool = False, enrich_metadata: bool = True):
        """
        Args:
            output_dir: ë³€í™˜ëœ ë°ì´í„°ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ (Noneì´ë©´ ìë™ ì„¤ì •)
            use_db: DBì— ì‚½ì…í• ì§€ ì—¬ë¶€
            enrich_metadata: ë©”íƒ€ë°ì´í„° ë³´ê°• ì—¬ë¶€
        """
        if output_dir is None:
            output_dir = DATA_DIR / "transformed"
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.use_db = use_db
        self.conn = None
        self.cur = None
        
        if use_db:
            self._connect_db()
        
        self.enrich_metadata = enrich_metadata
        if enrich_metadata:
            self.metadata_enricher = MetadataEnricher()
            print("âœ… ë©”íƒ€ë°ì´í„° ë³´ê°• í™œì„±í™”")
        else:
            self.metadata_enricher = None
        
        self.stats = {
            'documents': 0,
            'chunks': 0,
            'skipped': 0,
            'errors': [],
            'enriched_chunks': 0
        }
    
    def _connect_db(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        self.conn = psycopg2.connect(
            host=os.getenv('DB_HOST', 'localhost'),
            port=os.getenv('DB_PORT', '5432'),
            database=os.getenv('DB_NAME', 'ddoksori'),
            user=os.getenv('DB_USER', 'postgres'),
            password=os.getenv('DB_PASSWORD', 'postgres')
        )
        self.cur = self.conn.cursor()
        print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
    
    def _assign_chunk_indices(self, chunks: List[Dict]) -> List[Dict]:
        """
        ì²­í¬ ë¦¬ìŠ¤íŠ¸ì— 0-based ì¸ë±ìŠ¤ í• ë‹¹
        
        âš ï¸ ì¤‘ìš”: ì›ë³¸ ë°ì´í„°ì˜ ì¸ë±ìŠ¤ëŠ” ë¬´ì‹œí•˜ê³  ìƒˆë¡œ í• ë‹¹
        """
        total = len(chunks)
        for idx, chunk in enumerate(chunks):
            chunk['chunk_index'] = idx  # 0, 1, 2, ...
            chunk['chunk_total'] = total
        return chunks
    
    def _get_chunk_rules(self, chunk_type: str) -> Dict:
        """ì²­í¬ íƒ€ì…ì— ë”°ë¥¸ ì²˜ë¦¬ ê·œì¹™ ë°˜í™˜"""
        return CHUNK_PROCESSING_RULES.get(chunk_type, CHUNK_PROCESSING_RULES['default'])
    
    def _estimate_token_count(self, text: str) -> int:
        """
        í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ì¶”ì •
        
        í•œêµ­ì–´ í† í° ë³€í™˜ìœ¨: ì•½ 1.5-2ì = 1í† í°
        ë³´ìˆ˜ì  ì¶”ì •: 1.5ì = 1í† í°
        
        Args:
            text: í† í° ìˆ˜ë¥¼ ì¶”ì •í•  í…ìŠ¤íŠ¸
            
        Returns:
            ì¶”ì •ëœ í† í° ìˆ˜
        """
        char_count = len(text)
        return int(char_count / 1.5)
    
    def _validate_token_limit(self, chunks: List[Dict], max_tokens: int = 512) -> Dict:
        """
        ëª¨ë“  ì²­í¬ì˜ í† í° ìˆ˜ ê²€ì¦
        
        Args:
            chunks: ê²€ì¦í•  ì²­í¬ ë¦¬ìŠ¤íŠ¸
            max_tokens: ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸: 512)
            
        Returns:
            {
                'valid': bool,
                'violations': List[Dict],
                'stats': Dict
            }
        """
        violations = []
        token_counts = []
        
        for chunk in chunks:
            if chunk.get('drop', False):
                continue
            
            content = chunk.get('content', '')
            estimated_tokens = self._estimate_token_count(content)
            token_counts.append(estimated_tokens)
            
            if estimated_tokens > max_tokens:
                violations.append({
                    'chunk_id': chunk.get('chunk_id'),
                    'chunk_type': chunk.get('chunk_type'),
                    'char_count': len(content),
                    'estimated_tokens': estimated_tokens,
                    'excess_tokens': estimated_tokens - max_tokens
                })
        
        return {
            'valid': len(violations) == 0,
            'violations': violations,
            'stats': {
                'total_chunks': len([c for c in chunks if not c.get('drop', False)]),
                'avg_tokens': sum(token_counts) / len(token_counts) if token_counts else 0,
                'max_tokens': max(token_counts) if token_counts else 0,
                'violation_count': len(violations),
                'violation_rate': len(violations) / len(token_counts) if token_counts else 0
            }
        }
    
    def _group_sentences(self, sentences: List[str], target_size: int) -> List[str]:
        """
        ë¬¸ì¥ë“¤ì„ ëª©í‘œ í¬ê¸°ì— ë§ê²Œ ê·¸ë£¹í™”
        
        Args:
            sentences: ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ (êµ¬ë¶„ì í¬í•¨)
            target_size: ëª©í‘œ í¬ê¸°
            
        Returns:
            ê·¸ë£¹í™”ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
        """
        grouped = []
        current_group = []
        current_length = 0
        
        i = 0
        while i < len(sentences):
            sentence = sentences[i]
            sentence_length = len(sentence)
            
            if current_length + sentence_length > target_size and current_group:
                # í˜„ì¬ ê·¸ë£¹ì„ ì €ì¥
                grouped.append(''.join(current_group))
                current_group = []
                current_length = 0
            
            current_group.append(sentence)
            current_length += sentence_length
            i += 1
        
        # ë‚¨ì€ ê·¸ë£¹ ì²˜ë¦¬
        if current_group:
            grouped.append(''.join(current_group))
        
        return grouped
    
    def _split_chunk_semantic(self, chunk: Dict, rules: Dict) -> List[Dict]:
        """
        ì˜ë¯¸ ë‹¨ìœ„ ê¸°ë°˜ ì²­í¬ ë¶„í•  (ê°œì„ )
        
        ë¶„í•  ìš°ì„ ìˆœìœ„:
        1. ì´ì¤‘ ì¤„ë°”ê¿ˆ (ë¬¸ë‹¨ êµ¬ë¶„)
        2. ë¬¸ì¥ êµ¬ë¶„ (ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ)
        3. ì‰¼í‘œ/ì„¸ë¯¸ì½œë¡ 
        
        Args:
            chunk: ë¶„í• í•  ì²­í¬
            rules: ì²˜ë¦¬ ê·œì¹™
            
        Returns:
            ë¶„í• ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        import re
        
        content = chunk.get('content', '')
        target_size = rules.get('target_length', 600)
        max_size = rules.get('max_length', 700)
        
        # 1ìˆœìœ„: ë¬¸ë‹¨ ë‹¨ìœ„ ë¶„í• 
        sections = re.split(r'\n\n+', content)
        
        # 2ìˆœìœ„: ë„ˆë¬´ ê¸´ ë¬¸ë‹¨ì€ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì¶”ê°€ ë¶„í• 
        refined_sections = []
        for section in sections:
            if len(section) > max_size:
                # ë¬¸ì¥ êµ¬ë¶„ìë¡œ ë¶„í•  (êµ¬ë¶„ìë„ í¬í•¨)
                sentences = re.split(r'([.!?]\s+)', section)
                grouped = self._group_sentences(sentences, target_size)
                refined_sections.extend(grouped)
            else:
                refined_sections.append(section)
        
        # ëª©í‘œ í¬ê¸°ì— ë§ê²Œ ì¬ì¡°í•© (ë‹¤ìŒ ë©”ì„œë“œì—ì„œ êµ¬í˜„)
        return self._regroup_sections(chunk, refined_sections, rules)
    
    def _extract_sentences(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ê°œì„ ë¨)
        
        Args:
            text: ë¶„ë¦¬í•  í…ìŠ¤íŠ¸
            
        Returns:
            ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
        """
        import re
        
        # ë¬¸ì¥ êµ¬ë¶„ìë¡œ ë¶„í•  (êµ¬ë¶„ì í¬í•¨)
        # ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ + ê³µë°±/ì¤„ë°”ê¿ˆ
        parts = re.split(r'([.!?](?:\s+|\n+))', text)
        
        # êµ¬ë¶„ìë¥¼ ë¬¸ì¥ì— ë‹¤ì‹œ ë¶™ì´ê¸°
        sentences = []
        for i in range(0, len(parts)-1, 2):
            if i+1 < len(parts):
                sentence = parts[i] + parts[i+1]
                sentences.append(sentence.strip())
        
        # ë§ˆì§€ë§‰ ë¶€ë¶„ ì²˜ë¦¬ (êµ¬ë¶„ìê°€ ì—†ëŠ” ê²½ìš°)
        if len(parts) % 2 == 1 and parts[-1].strip():
            sentences.append(parts[-1].strip())
        
        return [s for s in sentences if s]
    
    def _get_sentence_overlap(self, sentences: List[str], overlap_size: int) -> str:
        """
        ë¬¸ì¥ ë‹¨ìœ„ë¡œ overlap í…ìŠ¤íŠ¸ ìƒì„± (ê°œì„ ë¨)
        
        Args:
            sentences: ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
            overlap_size: ëª©í‘œ overlap í¬ê¸° (ë¬¸ì ìˆ˜)
            
        Returns:
            overlap í…ìŠ¤íŠ¸
        """
        if not sentences:
            return ""
        
        # ë’¤ì—ì„œë¶€í„° ë¬¸ì¥ì„ ì¶”ê°€í•˜ë©´ì„œ overlap_sizeì— ê·¼ì ‘í•˜ê²Œ
        overlap_sentences = []
        current_length = 0
        
        for sentence in reversed(sentences):
            sentence_length = len(sentence)
            if current_length + sentence_length > overlap_size and overlap_sentences:
                break
            overlap_sentences.insert(0, sentence)
            current_length += sentence_length
        
        return ' '.join(overlap_sentences)
    
    def _validate_chunk_quality(self, content: str) -> tuple[bool, str]:
        """
        ì²­í¬ í’ˆì§ˆ ê²€ì¦ (ì‹ ê·œ)
        
        ê²€ì¦ í•­ëª©:
        1. ë¬¸ì¥ ì™„ê²°ì„± (ë§ˆì§€ë§‰ ë¬¸ì¥ì´ ì™„ê²°ë˜ì—ˆëŠ”ì§€)
        2. ìµœì†Œ ê¸¸ì´ ì¶©ì¡±
        3. íŠ¹ìˆ˜ë¬¸ìë§Œìœ¼ë¡œ êµ¬ì„±ë˜ì§€ ì•Šì•˜ëŠ”ì§€
        
        Args:
            content: ì²­í¬ ë‚´ìš©
            
        Returns:
            (is_valid, reason): ìœ íš¨ ì—¬ë¶€ì™€ ì´ìœ 
        """
        if not content or not content.strip():
            return False, "ë¹ˆ ë‚´ìš©"
        
        content = content.strip()
        
        # 1. ìµœì†Œ ê¸¸ì´ ì²´í¬ (20ì ì´ìƒ)
        if len(content) < 20:
            return False, f"ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŒ ({len(content)}ì)"
        
        # 2. íŠ¹ìˆ˜ë¬¸ìë§Œìœ¼ë¡œ êµ¬ì„±ë˜ì§€ ì•Šì•˜ëŠ”ì§€
        import re
        text_only = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', content)
        if len(text_only) < 10:
            return False, "ì˜ë¯¸ ìˆëŠ” í…ìŠ¤íŠ¸ ë¶€ì¡±"
        
        # 3. ë¬¸ì¥ ì™„ê²°ì„± ì²´í¬ (ë§ˆì§€ë§‰ì´ ë¬¸ì¥ ì¢…ê²° ë¶€í˜¸ë¡œ ëë‚˜ëŠ”ì§€)
        last_char = content[-1]
        sentence_enders = ['.', '!', '?', 'ë‹¤', 'ìš”', 'ìŒ', 'ë‹ˆ', 'ì§€', 'ê¹Œ', 'ë‚˜']
        
        # ë§ˆì§€ë§‰ì´ ì¢…ê²° ë¶€í˜¸ê°€ ì•„ë‹ˆë©´ ê²½ê³  (but valid)
        if last_char not in sentence_enders:
            # ì´ê±´ ê²½ê³ ë§Œ í•˜ê³  í†µê³¼
            pass
        
        return True, ""
    
    def _regroup_sections(self, chunk: Dict, sections: List[str], rules: Dict) -> List[Dict]:
        """
        ì„¹ì…˜ë“¤ì„ ëª©í‘œ í¬ê¸°ì— ë§ê²Œ ì¬ì¡°í•©í•˜ë©° Overlapping ì ìš© (ê°œì„ ë¨)
        
        ê°œì„ ì‚¬í•­:
        - ë¬¸ì¥ ë‹¨ìœ„ Overlapping
        - ì²­í¬ í’ˆì§ˆ ê²€ì¦
        
        Args:
            chunk: ì›ë³¸ ì²­í¬
            sections: ë¶„í• ëœ ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸
            rules: ì²˜ë¦¬ ê·œì¹™
            
        Returns:
            ì¬ì¡°í•©ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸ (overlap ì ìš©)
        """
        target_size = rules.get('target_length', 600)
        max_size = rules.get('max_length', 700)
        overlap_size = rules.get('overlap_size', 150)
        overlap_mode = rules.get('overlap_mode', 'char')  # 'char' or 'sentence'
        
        sub_chunks = []
        current_buffer = []
        current_length = 0
        previous_sentences = []  # ì´ì „ ì²­í¬ì˜ ë¬¸ì¥ë“¤ (sentence modeìš©)
        
        for section in sections:
            section = section.strip()
            if not section:
                continue
            
            section_length = len(section)
            
            # ë²„í¼ê°€ ëª©í‘œ í¬ê¸° ê·¼ì²˜ì— ë„ë‹¬í•˜ë©´ ì²­í¬ ìƒì„±
            if current_length + section_length > target_size and current_buffer:
                # í˜„ì¬ ë²„í¼ë¡œ ì²­í¬ ìƒì„±
                chunk_content = '\n\n'.join(current_buffer)
                
                # Overlapping ì ìš©
                if previous_sentences and sub_chunks:
                    if overlap_mode == 'sentence':
                        # ë¬¸ì¥ ë‹¨ìœ„ overlap (ê°œì„ ë¨)
                        overlap_text = self._get_sentence_overlap(previous_sentences, overlap_size)
                        if overlap_text:
                            chunk_content = overlap_text + '\n\n' + chunk_content
                    else:
                        # ê¸°ì¡´ ë¬¸ì ë‹¨ìœ„ overlap
                        overlap_text = chunk_content[:overlap_size] if len(chunk_content) > overlap_size else chunk_content
                        if overlap_text:
                            chunk_content = overlap_text + '\n\n' + chunk_content
                
                # í’ˆì§ˆ ê²€ì¦ (ì‹ ê·œ)
                is_valid, reason = self._validate_chunk_quality(chunk_content)
                if not is_valid:
                    # í’ˆì§ˆì´ ë‚®ìœ¼ë©´ ê²½ê³ ë§Œ í•˜ê³  ê³„ì† ì§„í–‰
                    # (dropí•˜ì§€ ì•ŠìŒ - ë°ì´í„° ì†ì‹¤ ë°©ì§€)
                    if not chunk.get('metadata'):
                        chunk['metadata'] = {}
                    chunk['metadata']['quality_warning'] = reason
                
                sub_chunks.append({
                    **chunk,
                    'content': chunk_content,
                    'content_length': len(chunk_content),
                    'chunk_id': f"{chunk['chunk_id']}_part{len(sub_chunks)+1}",
                    'parent_chunk_id': chunk.get('chunk_id'),
                    'metadata': {
                        **chunk.get('metadata', {}),
                        'is_split': True,
                        'part_number': len(sub_chunks) + 1
                    }
                })
                
                # ë‹¤ìŒ overlapì„ ìœ„í•´ í˜„ì¬ ì²­í¬ì˜ ë¬¸ì¥ ì €ì¥
                previous_sentences = self._extract_sentences(chunk_content)
                
                # ë²„í¼ ì´ˆê¸°í™”
                current_buffer = []
                current_length = 0
            
            current_buffer.append(section)
            current_length += section_length + 2  # \n\n ê³ ë ¤
        
        # ë‚¨ì€ ë²„í¼ ì²˜ë¦¬
        if current_buffer:
            chunk_content = '\n\n'.join(current_buffer)
            if previous_sentences and sub_chunks:
                if overlap_mode == 'sentence':
                    overlap_text = self._get_sentence_overlap(previous_sentences, overlap_size)
                    if overlap_text:
                        chunk_content = overlap_text + '\n\n' + chunk_content
                else:
                    overlap_text = chunk_content[:overlap_size] if len(chunk_content) > overlap_size else chunk_content
                    if overlap_text:
                        chunk_content = overlap_text + '\n\n' + chunk_content
            
            # í’ˆì§ˆ ê²€ì¦
            is_valid, reason = self._validate_chunk_quality(chunk_content)
            if not is_valid:
                if not chunk.get('metadata'):
                    chunk['metadata'] = {}
                chunk['metadata']['quality_warning'] = reason
            
            sub_chunks.append({
                **chunk,
                'content': chunk_content,
                'content_length': len(chunk_content),
                'chunk_id': f"{chunk['chunk_id']}_part{len(sub_chunks)+1}" if sub_chunks else chunk['chunk_id'],
                'parent_chunk_id': chunk.get('chunk_id') if sub_chunks else None,
                'metadata': {
                    **chunk.get('metadata', {}),
                    'is_split': True if sub_chunks else False,
                    'part_number': len(sub_chunks) + 1 if sub_chunks else 1
                }
            })
        
        return sub_chunks if sub_chunks else [chunk]
    
    def _merge_short_chunks(self, chunks: List[Dict]) -> List[Dict]:
        """
        ì§§ì€ ì²­í¬ë¥¼ ì´ì „/ë‹¤ìŒ ì²­í¬ì™€ ë³‘í•©
        
        Args:
            chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ë³‘í•©ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        if not chunks:
            return chunks
        
        merged = []
        buffer = []
        
        for i, chunk in enumerate(chunks):
            content = chunk.get('content', '')
            chunk_type = chunk.get('chunk_type', 'default')
            rules = self._get_chunk_rules(chunk_type)
            
            # drop=Trueì¸ ì²­í¬ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
            if chunk.get('drop', False):
                if buffer:
                    # ë²„í¼ì— ìˆë˜ ì²­í¬ë“¤ì„ ë§ˆì§€ë§‰ merged ì²­í¬ì™€ ë³‘í•©
                    if merged:
                        last_chunk = merged[-1]
                        buffer_content = "\n\n".join([c['content'] for c in buffer])
                        last_chunk['content'] = last_chunk['content'] + "\n\n" + buffer_content
                        last_chunk['content_length'] = len(last_chunk['content'])
                    else:
                        # mergedê°€ ë¹„ì–´ìˆìœ¼ë©´ ë²„í¼ ì²­í¬ë“¤ì„ ì¶”ê°€
                        merged.extend(buffer)
                    buffer = []
                merged.append(chunk)
                continue
            
            # ë³‘í•©ì´ í—ˆìš©ë˜ì§€ ì•ŠëŠ” íƒ€ì…ì´ê±°ë‚˜ ìµœì†Œ ê¸¸ì´ë¥¼ ì¶©ì¡±í•˜ëŠ” ì²­í¬
            if not rules['merge_allowed'] or len(content) >= rules['min_length']:
                # ë²„í¼ê°€ ìˆìœ¼ë©´ í˜„ì¬ ì²­í¬ì™€ ë³‘í•©
                if buffer:
                    merged_content = "\n\n".join([c['content'] for c in buffer] + [content])
                    chunk['content'] = merged_content
                    chunk['content_length'] = len(merged_content)
                    buffer = []
                merged.append(chunk)
            else:
                # ì§§ì€ ì²­í¬ë¥¼ ë²„í¼ì— ì¶”ê°€
                buffer.append(chunk)
        
        # ë‚¨ì€ ë²„í¼ ì²˜ë¦¬
        if buffer:
            if merged:
                # ë§ˆì§€ë§‰ ì²­í¬ì™€ ë³‘í•©
                last_chunk = merged[-1]
                buffer_content = "\n\n".join([c['content'] for c in buffer])
                last_chunk['content'] = last_chunk['content'] + "\n\n" + buffer_content
                last_chunk['content_length'] = len(last_chunk['content'])
            else:
                # mergedê°€ ë¹„ì–´ìˆìœ¼ë©´ ë²„í¼ ì²­í¬ë“¤ì„ ê·¸ëŒ€ë¡œ ì¶”ê°€
                merged.extend(buffer)
        
        return merged
    
    def _split_long_chunks(self, chunks: List[Dict]) -> List[Dict]:
        """
        ê¸´ ì²­í¬ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í•  (ê°œì„ ëœ ë¡œì§ ì‚¬ìš©)
        
        Args:
            chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ë¶„í• ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        result = []
        
        for chunk in chunks:
            content = chunk.get('content', '')
            chunk_type = chunk.get('chunk_type', 'default')
            rules = self._get_chunk_rules(chunk_type)
            
            # drop=Trueì¸ ì²­í¬ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
            if chunk.get('drop', False):
                result.append(chunk)
                continue
            
            # ë¶„í• ì´ í—ˆìš©ë˜ì§€ ì•Šê±°ë‚˜ ìµœëŒ€ ê¸¸ì´ ì´í•˜ì¸ ì²­í¬
            if not rules.get('split_allowed', False) or len(content) <= rules.get('max_length', 700):
                result.append(chunk)
                continue
            
            # ê°œì„ ëœ ì˜ë¯¸ ë‹¨ìœ„ ë¶„í•  ì‚¬ìš©
            sub_chunks = self._split_chunk_semantic(chunk, rules)
            result.extend(sub_chunks)
        
        return result
    
    def _optimize_chunks(self, chunks: List[Dict]) -> List[Dict]:
        """
        ì²­í¬ ìµœì í™” (ë³‘í•© + ë¶„í•  + ê²€ì¦)
        
        Args:
            chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ìµœì í™”ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        # 1. ì§§ì€ ì²­í¬ ë³‘í•©
        chunks = self._merge_short_chunks(chunks)
        
        # 2. ê¸´ ì²­í¬ ë¶„í•  (ê°œì„ ëœ ë¡œì§)
        chunks = self._split_long_chunks(chunks)
        
        # 3. ë¹ˆ ì²­í¬ ì²˜ë¦¬
        for chunk in chunks:
            content = chunk.get('content', '').strip()
            chunk_type = chunk.get('chunk_type', 'default')
            rules = self._get_chunk_rules(chunk_type)
            
            # ë¹ˆ ì²­í¬ì´ê³  drop_if_emptyê°€ Trueì¸ ê²½ìš°
            if not content and rules.get('drop_if_empty', False):
                chunk['drop'] = True
        
        # 4. í† í° ì œí•œ ê²€ì¦
        validation_result = self._validate_token_limit(chunks)
        
        if not validation_result['valid']:
            print(f"  âš ï¸  í† í° ì œí•œ ì´ˆê³¼ ì²­í¬: {validation_result['stats']['violation_count']}ê°œ")
            print(f"  âš ï¸  ì´ˆê³¼ìœ¨: {validation_result['stats']['violation_rate']*100:.1f}%")
            
            # ì´ˆê³¼ ì²­í¬ ì¬ë¶„í•  (ë” ì‘ì€ í¬ê¸°ë¡œ)
            chunks = self._resplit_violations(chunks, validation_result['violations'])
            
            # ì¬ê²€ì¦
            revalidation = self._validate_token_limit(chunks)
            if revalidation['valid']:
                print(f"  âœ… ì¬ë¶„í•  ì™„ë£Œ: í† í° ì œí•œ ì¤€ìˆ˜")
            else:
                print(f"  âš ï¸  ì¬ë¶„í•  í›„ì—ë„ {revalidation['stats']['violation_count']}ê°œ ì´ˆê³¼")
        
        return chunks
    
    def _resplit_violations(self, chunks: List[Dict], violations: List[Dict]) -> List[Dict]:
        """
        í† í° ì œí•œ ì´ˆê³¼ ì²­í¬ë¥¼ ì¬ë¶„í• 
        
        Args:
            chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
            violations: ìœ„ë°˜ ì‚¬í•­ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì¬ë¶„í• ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        violation_ids = {v['chunk_id'] for v in violations}
        result = []
        
        for chunk in chunks:
            if chunk['chunk_id'] in violation_ids:
                # ë” ì‘ì€ í¬ê¸°ë¡œ ì¬ë¶„í•  (target_lengthë¥¼ 50% ì¤„ì„)
                rules = self._get_chunk_rules(chunk.get('chunk_type', 'default'))
                adjusted_rules = {
                    **rules, 
                    'target_length': rules.get('target_length', 600) // 2,
                    'max_length': rules.get('max_length', 700) // 2
                }
                sub_chunks = self._split_chunk_semantic(chunk, adjusted_rules)
                result.extend(sub_chunks)
            else:
                result.append(chunk)
        
        return result
    
    def _validate_chunk_indices(self, doc_id: str, chunks: List[Dict]):
        """chunk_index ê²€ì¦"""
        chunks_sorted = sorted(chunks, key=lambda x: x['chunk_index'])
        
        expected_indices = list(range(len(chunks)))
        actual_indices = [c['chunk_index'] for c in chunks_sorted]
        
        if expected_indices != actual_indices:
            raise ValueError(
                f"âŒ Invalid chunk_index for {doc_id}:\n"
                f"   Expected: {expected_indices}\n"
                f"   Actual: {actual_indices}"
            )
        
        for chunk in chunks:
            if chunk['chunk_total'] != len(chunks):
                raise ValueError(
                    f"âŒ Invalid chunk_total for {chunk.get('chunk_id', 'unknown')}:\n"
                    f"   Expected: {len(chunks)}\n"
                    f"   Actual: {chunk['chunk_total']}"
                )
            
            if chunk['chunk_index'] >= chunk['chunk_total']:
                raise ValueError(
                    f"âŒ chunk_index >= chunk_total for {chunk.get('chunk_id', 'unknown')}:\n"
                    f"   chunk_index: {chunk['chunk_index']}\n"
                    f"   chunk_total: {chunk['chunk_total']}"
                )
    
    def _enrich_document(self, doc_data: Dict) -> Dict:
        """ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ì— ë©”íƒ€ë°ì´í„° ë³´ê°• ì ìš©"""
        if not self.enrich_metadata or not self.metadata_enricher:
            return doc_data
        
        enriched_count = 0
        for chunk in doc_data.get('chunks', []):
            # drop=Trueì¸ ì²­í¬ëŠ” ìŠ¤í‚µ
            if chunk.get('drop', False):
                continue
            
            # ë©”íƒ€ë°ì´í„° ë³´ê°•
            self.metadata_enricher.enrich_chunk_metadata(chunk, extract_all=True)
            enriched_count += 1
        
        self.stats['enriched_chunks'] += enriched_count
        return doc_data
    
    def _save_json(self, data: Dict, filename: str):
        """ë³€í™˜ëœ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        output_path = self.output_dir / filename
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"  ğŸ’¾ ì €ì¥: {output_path}")
    
    def transform_law_data(self, file_path: str) -> Dict:
        """
        ë²•ë ¹ ë°ì´í„° ë³€í™˜
        - ë¬¸ì„œ ë‹¨ìœ„: ë²•ë ¹ë³„ (law_id)
        - ì²­í¬ ë‹¨ìœ„: ì¡°ë¬¸/í•­/í˜¸/ëª©ë³„ (unit_id)
        """
        print(f"\nğŸ“œ ë²•ë ¹ ë°ì´í„° ë³€í™˜: {file_path}")
        
        chunks_by_law = {}
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line)
                law_id = data['law_id']
                
                if law_id not in chunks_by_law:
                    chunks_by_law[law_id] = {
                        'doc_id': f"statute:{law_id}",
                        'doc_type': 'law',
                        'title': data['law_name'],
                        'source_org': 'statute',
                        'category_path': None,
                        'url': None,
                        'metadata': {'law_id': law_id},
                        'chunks': []
                    }
                
                # ì²­í¬ ìƒì„±
                chunk = {
                    'chunk_id': f"statute:{law_id}:{data['unit_id']}",
                    'chunk_type': data['unit_level'],
                    'content': f"[ë²•ë ¹] {data['law_name']}\n[ì¡°ë¬¸] {data['path']}\n\n{data['index_text']}",
                    'content_length': len(data['index_text']),
                    'drop': False,
                    'metadata': {
                        'unit_id': data['unit_id'],
                        'path': data['path'],
                        'article_no': data.get('article_no'),
                        'paragraph_no': data.get('paragraph_no')
                    }
                }
                chunks_by_law[law_id]['chunks'].append(chunk)
        
        # ê° ë²•ë ¹ë³„ë¡œ ì²­í¬ ìµœì í™” í›„ 0-based ì¸ë±ìŠ¤ í• ë‹¹
        result = {'documents': []}
        
        for law_id, doc_data in chunks_by_law.items():
            # ì²­í¬ ìµœì í™”
            doc_data['chunks'] = self._optimize_chunks(doc_data['chunks'])
            
            # 0-based ì¸ë±ìŠ¤ í• ë‹¹
            doc_data['chunks'] = self._assign_chunk_indices(doc_data['chunks'])
            self._validate_chunk_indices(doc_data['doc_id'], doc_data['chunks'])
            
            # ë©”íƒ€ë°ì´í„° ë³´ê°•
            doc_data = self._enrich_document(doc_data)
            
            result['documents'].append(doc_data)
            
            self.stats['documents'] += 1
            self.stats['chunks'] += len(doc_data['chunks'])
            
            print(f"  âœ… {doc_data['title']}: {len(doc_data['chunks'])}ê°œ ì²­í¬ (ìµœì í™” ì™„ë£Œ)")
        
        return result
    
    def transform_law_single_file(self, file_path: str) -> Dict:
        """
        ë‹¨ì¼ ë²•ë ¹ JSONL íŒŒì¼ì„ PostgreSQL í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        (ì—¬ëŸ¬ ë²•ë ¹ì´ í•˜ë‚˜ì˜ íŒŒì¼ì— ìˆì„ ìˆ˜ ìˆìŒ)
        
        Args:
            file_path: ë²•ë ¹ JSONL íŒŒì¼ ê²½ë¡œ
        
        Returns:
            {
                'documents': [...]
            }
        """
        from pathlib import Path
        
        file_name = Path(file_path).stem  # ì˜ˆ: "Consumer_Basic_Law_chunks"
        print(f"\nğŸ“œ ë²•ë ¹ íŒŒì¼ ë³€í™˜: {file_name}")
        
        # ë²•ë ¹ëª… ë§¤í•‘ (íŒŒì¼ëª… -> í•œê¸€ëª…)
        law_name_map = {
            'Civil_Law_chunks': 'ë¯¼ë²•',
            'Commercial_Law_chunks': 'ìƒë²•',
            'Consumer_Basic_Law_chunks': 'ì†Œë¹„ìê¸°ë³¸ë²•',
            'E_Commerce_Consumer_Law_chunks': 'ì „ììƒê±°ë˜ì†Œë¹„ìë³´í˜¸ë²•',
            'Product_Liability_Law_chunks': 'ì œì¡°ë¬¼ì±…ì„ë²•',
            'Terms_Regulation_Law_chunks': 'ì•½ê´€ê·œì œë²•',
            'Installment_Sales_Law_chunks': 'í• ë¶€ê±°ë˜ë²•',
            'Direct_Sales_Law_chunks': 'ë°©ë¬¸íŒë§¤ë²•',
            'Fair_Ads_Law_chunks': 'í‘œì‹œê´‘ê³ ë²•',
            'Content_Industry_Promotion_Law_chunks': 'ì½˜í…ì¸ ì‚°ì—…ì§„í¥ë²•',
            'E_Transaction_Law_chunks': 'ì „ìê±°ë˜ë²•'
        }
        
        # íŒŒì¼ì—ì„œ law_idë³„ë¡œ ê·¸ë£¹í™”
        chunks_by_law = {}
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if not line.strip():
                    continue
                    
                data = json.loads(line)
                law_id = data['law_id']
                law_name = data['law_name']
                
                if law_id not in chunks_by_law:
                    # doc_idë¥¼ íŒŒì¼ëª… ê¸°ë°˜ìœ¼ë¡œ ìƒì„± (law_id ëŒ€ì‹ )
                    doc_id = f"law:{file_name.lower()}"
                    
                    chunks_by_law[law_id] = {
                        'doc_id': doc_id,
                        'doc_type': 'law',
                        'title': law_name,
                        'source_org': 'statute',
                        'category_path': ['ë²•ë ¹', law_name],
                        'url': None,
                        'collected_at': None,
                        'metadata': {
                            'law_id': law_id,
                            'law_name': law_name,
                            'file_name': file_name
                        },
                        'chunks': []
                    }
                
                # ì²­í¬ ìƒì„±
                chunk = {
                    'chunk_id': f"law:{file_name.lower()}::{data['unit_id']}",
                    'chunk_type': data['unit_level'],
                    'content': f"[ë²•ë ¹] {law_name}\n[ì¡°ë¬¸] {data['path']}\n\n{data['index_text']}",
                    'content_length': len(data['index_text']),
                    'drop': False,
                    'metadata': {
                        'unit_id': data['unit_id'],
                        'path': data['path'],
                        'article_no': data.get('article_no'),
                        'paragraph_no': data.get('paragraph_no')
                    }
                }
                chunks_by_law[law_id]['chunks'].append(chunk)
        
        # ê° ë²•ë ¹ë³„ë¡œ ì²­í¬ ìµœì í™” í›„ 0-based ì¸ë±ìŠ¤ í• ë‹¹
        result = {'documents': []}
        
        for law_id, doc_data in chunks_by_law.items():
            # ì²­í¬ ìµœì í™”
            doc_data['chunks'] = self._optimize_chunks(doc_data['chunks'])
            
            # 0-based ì¸ë±ìŠ¤ í• ë‹¹
            doc_data['chunks'] = self._assign_chunk_indices(doc_data['chunks'])
            self._validate_chunk_indices(doc_data['doc_id'], doc_data['chunks'])
            
            # ë©”íƒ€ë°ì´í„° ë³´ê°•
            doc_data = self._enrich_document(doc_data)
            
            result['documents'].append(doc_data)
            
            self.stats['documents'] += 1
            self.stats['chunks'] += len(doc_data['chunks'])
            
            print(f"  âœ… {doc_data['title']}: {len(doc_data['chunks'])}ê°œ ì²­í¬")
        
        return result
    
    def transform_criteria_table1(self, file_path: str) -> Dict:
        """
        ê¸°ì¤€ ë°ì´í„° ë³€í™˜ - table1 (í’ˆëª© ë¶„ë¥˜)
        - ë¬¸ì„œ ë‹¨ìœ„: ì „ì²´ í…Œì´ë¸” í•˜ë‚˜
        - ì²­í¬ ë‹¨ìœ„: ê° í’ˆëª©
        """
        print(f"\nğŸ“‹ ê¸°ì¤€ ë°ì´í„° ë³€í™˜ (table1 - í’ˆëª©): {file_path}")
        
        doc_id = 'criteria:table1'
        chunks = []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if not line.strip():
                    continue
                    
                data = json.loads(line)
                
                # embed_textë¥¼ contentë¡œ ì‚¬ìš©
                content = data.get('embed_text', '')
                metadata_raw = data.get('metadata', {})
                
                # stable_idë¥¼ chunk_idë¡œ ì‚¬ìš©
                chunk_id = data.get('stable_id', f"{doc_id}::item{len(chunks):04d}")
                
                chunk = {
                    'chunk_id': chunk_id,
                    'chunk_type': 'item',
                    'content': content,
                    'content_length': len(content),
                    'drop': False,
                    'metadata': {
                        'item_name': metadata_raw.get('item_name', ''),
                        'category': metadata_raw.get('category', ''),
                        'industry': metadata_raw.get('industry', ''),
                        'item_group': metadata_raw.get('item_group', ''),
                        'aliases': metadata_raw.get('aliases', [])
                    }
                }
                chunks.append(chunk)
        
        # ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
        document = {
            'doc_id': doc_id,
            'doc_type': 'criteria_item_list',
            'title': 'ì†Œë¹„ìë¶„ìŸí•´ê²°ê¸°ì¤€ - ëŒ€ìƒí’ˆëª©',
            'source_org': 'KCA',
            'category_path': ['ê¸°ì¤€', 'í’ˆëª©ë¶„ë¥˜'],
            'url': None,
            'collected_at': None,
            'metadata': {
                'table_type': 'table1',
                'item_count': len(chunks)
            },
            'chunks': []
        }
        
        # ì²­í¬ ìµœì í™” ë° ì¸ë±ìŠ¤ í• ë‹¹
        document['chunks'] = self._optimize_chunks(chunks)
        document['chunks'] = self._assign_chunk_indices(document['chunks'])
        self._validate_chunk_indices(document['doc_id'], document['chunks'])
        
        # ë©”íƒ€ë°ì´í„° ë³´ê°•
        document = self._enrich_document(document)
        
        self.stats['documents'] += 1
        self.stats['chunks'] += len(document['chunks'])
        
        print(f"  âœ… {len(document['chunks'])}ê°œ í’ˆëª© ì²­í¬")
        
        return {'documents': [document]}
    
    def transform_criteria_table3(self, file_path: str) -> Dict:
        """
        ê¸°ì¤€ ë°ì´í„° ë³€í™˜ - table3 (í’ˆì§ˆë³´ì¦ê¸°ê°„)
        - ë¬¸ì„œ ë‹¨ìœ„: ì „ì²´ í…Œì´ë¸” í•˜ë‚˜
        - ì²­í¬ ë‹¨ìœ„: ê° í’ˆëª©ë³„ ë³´ì¦ê¸°ê°„
        """
        print(f"\nğŸ“‹ ê¸°ì¤€ ë°ì´í„° ë³€í™˜ (table3 - í’ˆì§ˆë³´ì¦ê¸°ê°„): {file_path}")
        
        doc_id = 'criteria:table3'
        chunks = []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if not line.strip():
                    continue
                    
                data = json.loads(line)
                
                # embed_textë¥¼ contentë¡œ ì‚¬ìš©
                content = data.get('embed_text', '')
                metadata_raw = data.get('metadata', {})
                
                # stable_idë¥¼ chunk_idë¡œ ì‚¬ìš©
                chunk_id = data.get('stable_id', f"{doc_id}::warranty{len(chunks):04d}")
                
                chunk = {
                    'chunk_id': chunk_id,
                    'chunk_type': 'warranty',
                    'content': content,
                    'content_length': len(content),
                    'drop': False,
                    'metadata': metadata_raw
                }
                chunks.append(chunk)
        
        # ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
        document = {
            'doc_id': doc_id,
            'doc_type': 'criteria_warranty',
            'title': 'ì†Œë¹„ìë¶„ìŸí•´ê²°ê¸°ì¤€ - í’ˆì§ˆë³´ì¦ê¸°ê°„',
            'source_org': 'KCA',
            'category_path': ['ê¸°ì¤€', 'í’ˆì§ˆë³´ì¦ê¸°ê°„'],
            'url': None,
            'collected_at': None,
            'metadata': {
                'table_type': 'table3',
                'item_count': len(chunks)
            },
            'chunks': []
        }
        
        # ì²­í¬ ìµœì í™” ë° ì¸ë±ìŠ¤ í• ë‹¹
        document['chunks'] = self._optimize_chunks(chunks)
        document['chunks'] = self._assign_chunk_indices(document['chunks'])
        self._validate_chunk_indices(document['doc_id'], document['chunks'])
        
        # ë©”íƒ€ë°ì´í„° ë³´ê°•
        document = self._enrich_document(document)
        
        self.stats['documents'] += 1
        self.stats['chunks'] += len(document['chunks'])
        
        print(f"  âœ… {len(document['chunks'])}ê°œ ë³´ì¦ê¸°ê°„ ì²­í¬")
        
        return {'documents': [document]}
    
    def transform_criteria_table4(self, file_path: str) -> Dict:
        """
        ê¸°ì¤€ ë°ì´í„° ë³€í™˜ - table4 (ë‚´êµ¬ì—°í•œ)
        - ë¬¸ì„œ ë‹¨ìœ„: ì „ì²´ í…Œì´ë¸” í•˜ë‚˜
        - ì²­í¬ ë‹¨ìœ„: ê° í’ˆëª©ë³„ ë‚´êµ¬ì—°í•œ
        """
        print(f"\nğŸ“‹ ê¸°ì¤€ ë°ì´í„° ë³€í™˜ (table4 - ë‚´êµ¬ì—°í•œ): {file_path}")
        
        doc_id = 'criteria:table4'
        chunks = []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if not line.strip():
                    continue
                    
                data = json.loads(line)
                
                # embed_textë¥¼ contentë¡œ ì‚¬ìš©
                content = data.get('embed_text', '')
                metadata_raw = data.get('metadata', {})
                
                # stable_idë¥¼ chunk_idë¡œ ì‚¬ìš©
                chunk_id = data.get('stable_id', f"{doc_id}::lifespan{len(chunks):04d}")
                
                chunk = {
                    'chunk_id': chunk_id,
                    'chunk_type': 'lifespan',
                    'content': content,
                    'content_length': len(content),
                    'drop': False,
                    'metadata': metadata_raw
                }
                chunks.append(chunk)
        
        # ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
        document = {
            'doc_id': doc_id,
            'doc_type': 'criteria_lifespan',
            'title': 'ì†Œë¹„ìë¶„ìŸí•´ê²°ê¸°ì¤€ - ë‚´êµ¬ì—°í•œ',
            'source_org': 'KCA',
            'category_path': ['ê¸°ì¤€', 'ë‚´êµ¬ì—°í•œ'],
            'url': None,
            'collected_at': None,
            'metadata': {
                'table_type': 'table4',
                'item_count': len(chunks)
            },
            'chunks': []
        }
        
        # ì²­í¬ ìµœì í™” ë° ì¸ë±ìŠ¤ í• ë‹¹
        document['chunks'] = self._optimize_chunks(chunks)
        document['chunks'] = self._assign_chunk_indices(document['chunks'])
        self._validate_chunk_indices(document['doc_id'], document['chunks'])
        
        # ë©”íƒ€ë°ì´í„° ë³´ê°•
        document = self._enrich_document(document)
        
        self.stats['documents'] += 1
        self.stats['chunks'] += len(document['chunks'])
        
        print(f"  âœ… {len(document['chunks'])}ê°œ ë‚´êµ¬ì—°í•œ ì²­í¬")
        
        return {'documents': [document]}
    
    def transform_criteria_table2(self, file_path: str) -> Dict:
        """
        ê¸°ì¤€ ë°ì´í„° ë³€í™˜ - table2 (í•´ê²°ê¸°ì¤€)
        - ë¬¸ì„œ ë‹¨ìœ„: ì „ì²´ í…Œì´ë¸” í•˜ë‚˜
        - ì²­í¬ ë‹¨ìœ„: ê° í–‰ (row_idxëŠ” ë¬´ì‹œí•˜ê³  0-basedë¡œ ì¬í• ë‹¹)
        """
        print(f"\nğŸ“‹ ê¸°ì¤€ ë°ì´í„° ë³€í™˜ (table2): {file_path}")
        
        doc_id = 'criteria:table2'
        chunks = []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line)
                
                # drop í”Œë˜ê·¸ í™•ì¸
                if data.get('drop', False):
                    self.stats['skipped'] += 1
                    continue
                
                chunk = {
                    'chunk_id': data['chunk_id'],
                    'chunk_type': 'resolution_row',
                    'content': data['text'],
                    'content_length': len(data['text']),
                    'drop': False,
                    'metadata': {
                        'category': data.get('category'),
                        'item_group': data.get('item_group'),
                        'item': data.get('item'),
                        'dispute_type': data.get('dispute_type'),
                        'resolution': data.get('resolution'),
                        'laws': data.get('laws', [])
                    },
                    'category_path': [
                        data.get('category'),
                        data.get('item_group'),
                        data.get('item')
                    ] if data.get('category') else None
                }
                chunks.append(chunk)
        
        # ì²­í¬ ìµœì í™”
        chunks = self._optimize_chunks(chunks)
        
        # 0-based ì¸ë±ìŠ¤ í• ë‹¹
        chunks = self._assign_chunk_indices(chunks)
        
        document = {
            'doc_id': doc_id,
            'doc_type': 'criteria_resolution',
            'title': 'ì†Œë¹„ìë¶„ìŸí•´ê²°ê¸°ì¤€ - í’ˆëª©ë³„ í•´ê²°ê¸°ì¤€',
            'source_org': 'KCA',
            'category_path': None,
            'url': None,
            'metadata': {'source': 'table2'},
            'chunks': chunks
        }
        
        self._validate_chunk_indices(doc_id, chunks)
        
        # ë©”íƒ€ë°ì´í„° ë³´ê°•
        document = self._enrich_document(document)
        
        self.stats['documents'] += 1
        self.stats['chunks'] += len(chunks)
        
        print(f"  âœ… {document['title']}: {len(chunks)}ê°œ ì²­í¬ (ìµœì í™” ì™„ë£Œ)")
        
        return {'documents': [document]}
    
    def transform_mediation_kca(self, file_path: str) -> Dict:
        """
        KCA ë¶„ìŸì¡°ì •ì‚¬ë¡€ ë³€í™˜
        - ë¬¸ì„œ ë‹¨ìœ„: ì‚¬ê±´ë²ˆí˜¸ë³„ (case_no)
        - ì²­í¬ ë‹¨ìœ„: chunk_typeë³„
        """
        print(f"\nâš–ï¸  ë¶„ìŸì¡°ì •ì‚¬ë¡€ ë³€í™˜ (KCA): {file_path}")
        
        cases = {}
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line)
                case_no = data['case_no']
                
                if case_no not in cases:
                    cases[case_no] = {
                        'doc_id': f"kca:mediation:{case_no}",
                        'doc_type': 'mediation_case',
                        'title': f"{case_no} ë¶„ìŸì¡°ì •ì‚¬ë¡€",
                        'source_org': 'KCA',
                        'category_path': None,
                        'url': None,
                        'metadata': {
                            'case_no': case_no,
                            'decision_date': data.get('decision_date'),
                            'agency': 'kca'
                        },
                        'chunks': []
                    }
                
                # ë¹ˆ contentëŠ” drop=Trueë¡œ ì„¤ì • (ì£¼ë¡œ law íƒ€ì… ì²­í¬)
                content = data['text']
                is_empty = len(content.strip()) == 0
                
                chunk = {
                    'chunk_id': f"kca:mediation:{case_no}:{data['chunk_type']}:{len(cases[case_no]['chunks']):04d}",
                    'chunk_type': data['chunk_type'],
                    'content': content,
                    'content_length': len(content),
                    'drop': is_empty,  # ë¹ˆ contentëŠ” drop
                    'metadata': {}
                }
                cases[case_no]['chunks'].append(chunk)
        
        # ê° ì¼€ì´ìŠ¤ë³„ë¡œ ì²­í¬ ìµœì í™” í›„ 0-based ì¸ë±ìŠ¤ í• ë‹¹
        result = {'documents': []}
        
        for case_no, case_data in cases.items():
            # ì²­í¬ ìµœì í™”
            case_data['chunks'] = self._optimize_chunks(case_data['chunks'])
            
            # 0-based ì¸ë±ìŠ¤ í• ë‹¹
            case_data['chunks'] = self._assign_chunk_indices(case_data['chunks'])
            self._validate_chunk_indices(case_data['doc_id'], case_data['chunks'])
            
            # ë©”íƒ€ë°ì´í„° ë³´ê°•
            case_data = self._enrich_document(case_data)
            
            result['documents'].append(case_data)
            
            self.stats['documents'] += 1
            self.stats['chunks'] += len(case_data['chunks'])
        
        print(f"  âœ… {len(cases)}ê°œ ì‚¬ë¡€, ì´ {sum(len(c['chunks']) for c in cases.values())}ê°œ ì²­í¬ (ìµœì í™” ì™„ë£Œ)")
        
        return result
    
    def transform_mediation_ecmc(self, file_path: str) -> Dict:
        """ECMC ë¶„ìŸì¡°ì •ì‚¬ë¡€ ë³€í™˜"""
        print(f"\nâš–ï¸  ë¶„ìŸì¡°ì •ì‚¬ë¡€ ë³€í™˜ (ECMC): {file_path}")
        
        cases = {}
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line)
                case_no = data['case_no']
                
                if case_no not in cases:
                    cases[case_no] = {
                        'doc_id': f"ecmc:mediation:{case_no}",
                        'doc_type': 'mediation_case',
                        'title': f"{case_no} ë¶„ìŸì¡°ì •ì‚¬ë¡€",
                        'source_org': 'ECMC',
                        'category_path': None,
                        'url': None,
                        'metadata': {
                            'case_no': case_no,
                            'decision_date': data.get('decision_date'),
                            'agency': 'ecmc'
                        },
                        'chunks': []
                    }
                
                # ë¹ˆ contentëŠ” drop=Trueë¡œ ì„¤ì •
                content = data['text']
                is_empty = len(content.strip()) == 0
                
                chunk = {
                    'chunk_id': f"ecmc:mediation:{case_no}:{data['chunk_type']}:{len(cases[case_no]['chunks']):04d}",
                    'chunk_type': data['chunk_type'],
                    'content': content,
                    'content_length': len(content),
                    'drop': data.get('drop', False) or is_empty,  # ê¸°ì¡´ drop í”Œë˜ê·¸ ë˜ëŠ” ë¹ˆ content
                    'metadata': {}
                }
                cases[case_no]['chunks'].append(chunk)
        
        # ê° ì¼€ì´ìŠ¤ë³„ë¡œ ì²­í¬ ìµœì í™” í›„ 0-based ì¸ë±ìŠ¤ í• ë‹¹ (drop=true í¬í•¨)
        result = {'documents': []}
        
        for case_no, case_data in cases.items():
            # ì²­í¬ ìµœì í™”
            case_data['chunks'] = self._optimize_chunks(case_data['chunks'])
            
            # 0-based ì¸ë±ìŠ¤ í• ë‹¹
            case_data['chunks'] = self._assign_chunk_indices(case_data['chunks'])
            self._validate_chunk_indices(case_data['doc_id'], case_data['chunks'])
            
            # ë©”íƒ€ë°ì´í„° ë³´ê°•
            case_data = self._enrich_document(case_data)
            
            result['documents'].append(case_data)
            
            self.stats['documents'] += 1
            self.stats['chunks'] += len(case_data['chunks'])
        
        print(f"  âœ… {len(cases)}ê°œ ì‚¬ë¡€, ì´ {sum(len(c['chunks']) for c in cases.values())}ê°œ ì²­í¬ (ìµœì í™” ì™„ë£Œ)")
        
        return result
    
    def transform_counsel_case(self, file_path: str) -> Dict:
        """
        í”¼í•´êµ¬ì œì‚¬ë¡€ ë³€í™˜
        - ì´ë¯¸ doc_id, chunk_id, chunk_indexê°€ ì¡´ì¬
        - ê²€ì¦ë§Œ ìˆ˜í–‰
        """
        print(f"\nğŸ’¬ í”¼í•´êµ¬ì œì‚¬ë¡€ ë³€í™˜: {file_path}")
        
        documents = {}
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line)
                doc_id = data['doc_id']
                
                if doc_id not in documents:
                    documents[doc_id] = {
                        'doc_id': doc_id,
                        'doc_type': 'counsel_case',
                        'title': data['title'],
                        'source_org': 'consumer.go.kr',
                        'category_path': data.get('category_path', []),
                        'url': data.get('metadata', {}).get('url'),
                        'metadata': data.get('metadata', {}),
                        'chunks': []
                    }
                
                chunk = {
                    'chunk_id': data['chunk_id'],
                    'chunk_index': data['chunk_index'],  # ì´ë¯¸ 0-based
                    'chunk_total': data['chunk_total'],
                    'chunk_type': 'qa_combined',
                    'content': data['text'],
                    'content_length': len(data['text']),
                    'drop': False,
                    'metadata': {}
                }
                documents[doc_id]['chunks'].append(chunk)
        
        # ì²­í¬ ìµœì í™” í›„ ê²€ì¦
        result = {'documents': []}
        
        for doc_id, doc_data in documents.items():
            # ì²­í¬ ìµœì í™”
            doc_data['chunks'] = self._optimize_chunks(doc_data['chunks'])
            
            # ì¸ë±ìŠ¤ ì¬í• ë‹¹
            doc_data['chunks'] = self._assign_chunk_indices(doc_data['chunks'])
            
            # ê²€ì¦
            self._validate_chunk_indices(doc_id, doc_data['chunks'])
            
            # ë©”íƒ€ë°ì´í„° ë³´ê°•
            doc_data = self._enrich_document(doc_data)
            
            result['documents'].append(doc_data)
            
            self.stats['documents'] += 1
            self.stats['chunks'] += len(doc_data['chunks'])
        
        print(f"  âœ… {len(documents)}ê°œ ì‚¬ë¡€, ì´ {sum(len(d['chunks']) for d in documents.values())}ê°œ ì²­í¬ (ìµœì í™” ì™„ë£Œ)")
        
        return result
    
    def run_transformation(self):
        """ì „ì²´ ë°ì´í„° ë³€í™˜ ì‹¤í–‰"""
        print("=" * 80)
        print("ë°ì´í„° ë³€í™˜ ì‹œì‘")
        print("=" * 80)
        
        all_results = []
        
        # 1. ë²•ë ¹ ë°ì´í„° (ìƒ˜í”Œë¡œ 1ê°œë§Œ)
        print("\n" + "=" * 80)
        print("1. ë²•ë ¹ ë°ì´í„° ë³€í™˜")
        print("=" * 80)
        
        law_file = DATA_DIR / 'law' / 'Civil_Law_chunks.jsonl'
        if law_file.exists():
            result = self.transform_law_data(str(law_file))
            self._save_json(result, 'law_civil_law.json')
            all_results.append(result)
        else:
            print(f"  âš ï¸  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {law_file}")
            print(f"  í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {Path.cwd()}")
        
        # 2. ê¸°ì¤€ ë°ì´í„°
        print("\n" + "=" * 80)
        print("2. ê¸°ì¤€ ë°ì´í„° ë³€í™˜")
        print("=" * 80)
        
        table2_file = DATA_DIR / 'criteria' / 'table2_resolution_row_chunks.jsonl'
        if table2_file.exists():
            result = self.transform_criteria_table2(str(table2_file))
            self._save_json(result, 'criteria_table2.json')
            all_results.append(result)
        else:
            print(f"  âš ï¸  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {table2_file}")
            print(f"  í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {Path.cwd()}")
        
        # 3. ë¶„ìŸì¡°ì •ì‚¬ë¡€ (ìƒ˜í”Œ)
        print("\n" + "=" * 80)
        print("3. ë¶„ìŸì¡°ì •ì‚¬ë¡€ ë³€í™˜")
        print("=" * 80)
        
        kca_file = DATA_DIR / 'dispute_resolution' / 'kca_final.jsonl'
        if kca_file.exists():
            result = self.transform_mediation_kca(str(kca_file))
            self._save_json(result, 'mediation_kca.json')
            all_results.append(result)
        else:
            print(f"  âš ï¸  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {kca_file}")
            print(f"  í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {Path.cwd()}")
        
        ecmc_file = DATA_DIR / 'dispute_resolution' / 'ecmc_final_rag_chunks_normalized.jsonl'
        if ecmc_file.exists():
            result = self.transform_mediation_ecmc(str(ecmc_file))
            self._save_json(result, 'mediation_ecmc.json')
            all_results.append(result)
        else:
            print(f"  âš ï¸  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ecmc_file}")
            print(f"  í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {Path.cwd()}")
        
        # 4. í”¼í•´êµ¬ì œì‚¬ë¡€ (ìƒ˜í”Œë¡œ 1ê°œë§Œ)
        print("\n" + "=" * 80)
        print("4. í”¼í•´êµ¬ì œì‚¬ë¡€ ë³€í™˜")
        print("=" * 80)
        
        counsel_file = DATA_DIR / 'compensation_case' / 'cs_114_chunks_v2.jsonl'
        if counsel_file.exists():
            result = self.transform_counsel_case(str(counsel_file))
            self._save_json(result, 'counsel_cs_114.json')
            all_results.append(result)
        else:
            print(f"  âš ï¸  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {counsel_file}")
            print(f"  í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {Path.cwd()}")
        
        # ì „ì²´ í†µê³„
        print("\n" + "=" * 80)
        print("ë³€í™˜ ì™„ë£Œ í†µê³„")
        print("=" * 80)
        print(f"  - ì´ ë¬¸ì„œ: {self.stats['documents']:,}ê°œ")
        print(f"  - ì´ ì²­í¬: {self.stats['chunks']:,}ê°œ")
        print(f"  - ìŠ¤í‚µ: {self.stats['skipped']:,}ê°œ")
        if self.enrich_metadata:
            print(f"  - ë©”íƒ€ë°ì´í„° ë³´ê°•: {self.stats['enriched_chunks']:,}ê°œ ì²­í¬")
        print(f"  - ì˜¤ë¥˜: {len(self.stats['errors'])}ê°œ")
        
        if self.stats['errors']:
            print("\nì˜¤ë¥˜ ëª©ë¡:")
            for error in self.stats['errors']:
                print(f"  - {error}")
        
        # í†µí•© í†µê³„ ì €ì¥
        summary = {
            'timestamp': datetime.now().isoformat(),
            'stats': self.stats,
            'files': {
                'law': 1,
                'criteria': 1,
                'mediation': 2,
                'counsel': 1
            }
        }
        self._save_json(summary, 'transformation_summary.json')
        
        print(f"\nâœ… ë³€í™˜ ì™„ë£Œ! ê²°ê³¼ëŠ” {self.output_dir}/ ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return all_results
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.cur:
            self.cur.close()
        if self.conn:
            self.conn.close()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    transformer = DataTransformer(
        use_db=False  # ì¼ë‹¨ JSONë§Œ ìƒì„±
    )
    
    try:
        transformer.run_transformation()
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    finally:
        transformer.close()

if __name__ == '__main__':
    main()
