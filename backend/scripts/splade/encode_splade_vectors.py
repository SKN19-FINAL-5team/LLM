"""
SPLADE Sparse Vector ì‚¬ì „ ì¸ì½”ë”© íŒŒì´í”„ë¼ì¸
ëª¨ë“  chunkì— ëŒ€í•´ SPLADE sparse vectorë¥¼ ìƒì„±í•˜ì—¬ RDBì— ì €ì¥
"""

import os
import sys
import json
import psycopg2
from typing import List, Tuple, Dict, Optional
from pathlib import Path
from dotenv import load_dotenv
from tqdm import tqdm
import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
backend_dir = Path(__file__).parent.parent
project_root = backend_dir.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(backend_dir))

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
env_file = backend_dir / '.env'
if env_file.exists():
    load_dotenv(env_file)
else:
    root_env = backend_dir.parent / '.env'
    if root_env.exists():
        load_dotenv(root_env)
    else:
        load_dotenv()

# SPLADE ëª¨ë“ˆ import
try:
    from scripts.splade.test_splade_naver import NaverSPLADERetriever
    from scripts.splade.test_splade_remote import RemoteSPLADERetriever
    SPLADE_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  SPLADE ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    SPLADE_AVAILABLE = False


class SPLADEEncodingPipeline:
    """SPLADE sparse vector ì¸ì½”ë”© íŒŒì´í”„ë¼ì¸"""
    
    def __init__(
        self,
        db_config: Dict,
        batch_size: int = 32,
        use_remote: bool = False,
        api_url: str = None,
        device: str = None
    ):
        """
        Args:
            db_config: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
            batch_size: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 32)
            use_remote: ì›ê²© API ì„œë²„ ì‚¬ìš© ì—¬ë¶€
            api_url: ì›ê²© API URL (use_remote=Trueì¼ ë•Œ)
            device: ë¡œì»¬ ëª¨ë“œì—ì„œ ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ('cuda' ë˜ëŠ” 'cpu')
        """
        self.db_config = db_config
        self.batch_size = batch_size
        self.conn = None
        self.splade_retriever = None
        
        # SPLADE Retriever ì´ˆê¸°í™”
        if use_remote:
            if api_url is None:
                api_url = os.getenv('SPLADE_API_URL', 'http://localhost:8001')
            try:
                self.splade_retriever = RemoteSPLADERetriever(api_url=api_url)
                print(f"âœ… ì›ê²© SPLADE API ì„œë²„ ì‚¬ìš©: {api_url}")
            except Exception as e:
                print(f"âš ï¸  ì›ê²© API ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
                print("   ë¡œì»¬ ëª¨ë“œë¡œ ì „í™˜ ì‹œë„...")
                use_remote = False
        
        if not use_remote:
            try:
                self.splade_retriever = NaverSPLADERetriever(device=device)
                self.splade_retriever.load_model()
                print(f"âœ… ë¡œì»¬ SPLADE ëª¨ë¸ ì‚¬ìš©: device={self.splade_retriever.device}")
            except Exception as e:
                print(f"âŒ SPLADE ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                raise RuntimeError("SPLADE ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    def connect_db(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        if self.conn is None or self.conn.closed:
            self.conn = psycopg2.connect(**self.db_config)
            self.conn.autocommit = False
    
    def get_chunks_to_encode(
        self,
        doc_type: Optional[str] = None,
        limit: Optional[int] = None,
        skip_encoded: bool = True
    ) -> List[Tuple[str, str]]:
        """
        ì¸ì½”ë”©ì´ í•„ìš”í•œ chunk ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            doc_type: ë¬¸ì„œ íƒ€ì… í•„í„° (Noneì´ë©´ ì „ì²´)
            limit: ìµœëŒ€ ê°œìˆ˜ (Noneì´ë©´ ì „ì²´)
            skip_encoded: ì´ë¯¸ ì¸ì½”ë”©ëœ chunk ê±´ë„ˆë›°ê¸°
        
        Returns:
            (chunk_id, content) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        self.connect_db()
        cur = self.conn.cursor()
        
        where_clauses = []
        params = []
        
        if skip_encoded:
            where_clauses.append("(splade_encoded IS NULL OR splade_encoded = FALSE)")
        
        if doc_type:
            where_clauses.append("d.doc_type = %s")
            params.append(doc_type)
        
        where_clauses.append("c.drop = FALSE")
        where_clauses.append("c.content IS NOT NULL")
        where_clauses.append("LENGTH(TRIM(c.content)) > 0")
        
        where_sql = " AND ".join(where_clauses)
        
        limit_sql = ""
        if limit:
            limit_sql = f"LIMIT {limit}"
        
        sql = f"""
            SELECT c.chunk_id, c.content
            FROM chunks c
            JOIN documents d ON c.doc_id = d.doc_id
            WHERE {where_sql}
            ORDER BY c.chunk_id
            {limit_sql}
        """
        
        cur.execute(sql, params)
        results = cur.fetchall()
        cur.close()
        
        return results
    
    def sparse_vector_to_jsonb(self, sparse_vec: np.ndarray, threshold: float = 0.0) -> Dict[str, float]:
        """
        Sparse vectorë¥¼ JSONB í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        0ì´ ì•„ë‹Œ ê°’ë§Œ ì €ì¥í•˜ì—¬ ê³µê°„ íš¨ìœ¨ì 
        
        Args:
            sparse_vec: Sparse vector (numpy array)
            threshold: ì €ì¥í•  ìµœì†Œ ê°€ì¤‘ì¹˜ ì„ê³„ê°’
        
        Returns:
            {token_id: weight} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
        """
        # 0ì´ ì•„ë‹Œ ì¸ë±ìŠ¤ ì°¾ê¸°
        non_zero_indices = np.where(sparse_vec > threshold)[0]
        
        # JSONB í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ë¬¸ìì—´ í‚¤ë¡œ ì €ì¥)
        result = {}
        for idx in non_zero_indices:
            weight = float(sparse_vec[idx])
            if weight > threshold:
                result[str(idx)] = weight
        
        return result
    
    def encode_batch(self, chunks: List[Tuple[str, str]]) -> List[Dict]:
        """
        ë°°ì¹˜ë¡œ chunk ì¸ì½”ë”©
        
        Args:
            chunks: (chunk_id, content) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            ì¸ì½”ë”© ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ [{chunk_id, sparse_vector, success}, ...]
        """
        if not chunks:
            return []
        
        chunk_ids = [c[0] for c in chunks]
        contents = [c[1] for c in chunks]
        
        results = []
        
        try:
            # ë°°ì¹˜ ì¸ì½”ë”©
            if hasattr(self.splade_retriever, 'encode_documents_batch'):
                # RemoteSPLADERetrieverì˜ ë°°ì¹˜ ì¸ì½”ë”© ì‚¬ìš©
                sparse_vectors = self.splade_retriever.encode_documents_batch(contents)
            else:
                # NaverSPLADERetrieverì˜ ê°œë³„ ì¸ì½”ë”© ì‚¬ìš©
                sparse_vectors = []
                for content in contents:
                    try:
                        vec = self.splade_retriever.encode_document(content)
                        sparse_vectors.append(vec)
                    except Exception as e:
                        print(f"  âš ï¸  ì¸ì½”ë”© ì‹¤íŒ¨ (chunk_id ì¼ë¶€): {e}")
                        sparse_vectors.append(None)
            
            # ê²°ê³¼ ë³€í™˜
            for chunk_id, sparse_vec in zip(chunk_ids, sparse_vectors):
                if sparse_vec is None:
                    results.append({
                        'chunk_id': chunk_id,
                        'sparse_vector': None,
                        'success': False
                    })
                    continue
                
                # JSONB í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                sparse_jsonb = self.sparse_vector_to_jsonb(sparse_vec)
                
                results.append({
                    'chunk_id': chunk_id,
                    'sparse_vector': sparse_jsonb,
                    'success': True
                })
        
        except Exception as e:
            print(f"  âš ï¸  ë°°ì¹˜ ì¸ì½”ë”© ì˜¤ë¥˜: {e}")
            # ê°œë³„ ì¸ì½”ë”©ìœ¼ë¡œ í´ë°±
            for chunk_id, content in chunks:
                try:
                    sparse_vec = self.splade_retriever.encode_document(content)
                    sparse_jsonb = self.sparse_vector_to_jsonb(sparse_vec)
                    results.append({
                        'chunk_id': chunk_id,
                        'sparse_vector': sparse_jsonb,
                        'success': True
                    })
                except Exception as e2:
                    print(f"  âš ï¸  ê°œë³„ ì¸ì½”ë”© ì‹¤íŒ¨ (chunk_id: {chunk_id[:50]}...): {e2}")
                    results.append({
                        'chunk_id': chunk_id,
                        'sparse_vector': None,
                        'success': False
                    })
        
        return results
    
    def save_encoded_vectors(self, encoded_results: List[Dict]):
        """
        ì¸ì½”ë”©ëœ sparse vectorë¥¼ DBì— ì €ì¥
        
        Args:
            encoded_results: encode_batch()ì˜ ê²°ê³¼
        """
        if not encoded_results:
            return
        
        self.connect_db()
        cur = self.conn.cursor()
        
        model_name = 'naver/splade-v3'
        if hasattr(self.splade_retriever, 'model_name'):
            model_name = self.splade_retriever.model_name
        
        success_count = 0
        fail_count = 0
        
        for result in encoded_results:
            chunk_id = result['chunk_id']
            sparse_vector = result['sparse_vector']
            success = result['success']
            
            if success and sparse_vector:
                try:
                    # JSONBë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
                    sparse_jsonb = json.dumps(sparse_vector)
                    
                    cur.execute("""
                        UPDATE chunks
                        SET 
                            splade_sparse_vector = %s::jsonb,
                            splade_model = %s,
                            splade_encoded = TRUE,
                            updated_at = NOW()
                        WHERE chunk_id = %s
                    """, (sparse_jsonb, model_name, chunk_id))
                    
                    success_count += 1
                except Exception as e:
                    print(f"  âš ï¸  DB ì €ì¥ ì‹¤íŒ¨ (chunk_id: {chunk_id[:50]}...): {e}")
                    fail_count += 1
            else:
                # ì‹¤íŒ¨í•œ ê²½ìš° í”Œë˜ê·¸ë§Œ ì—…ë°ì´íŠ¸ (ë‚˜ì¤‘ì— ì¬ì‹œë„ ê°€ëŠ¥)
                try:
                    cur.execute("""
                        UPDATE chunks
                        SET splade_encoded = FALSE
                        WHERE chunk_id = %s
                    """, (chunk_id,))
                    fail_count += 1
                except Exception as e:
                    print(f"  âš ï¸  í”Œë˜ê·¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (chunk_id: {chunk_id[:50]}...): {e}")
        
        self.conn.commit()
        cur.close()
        
        return success_count, fail_count
    
    def encode_all_chunks(
        self,
        doc_type: Optional[str] = None,
        limit: Optional[int] = None,
        resume: bool = True
    ):
        """
        ëª¨ë“  chunkì— ëŒ€í•´ SPLADE ì¸ì½”ë”© ìˆ˜í–‰
        
        Args:
            doc_type: ë¬¸ì„œ íƒ€ì… í•„í„°
            limit: ìµœëŒ€ ì²˜ë¦¬ ê°œìˆ˜
            resume: ì´ë¯¸ ì¸ì½”ë”©ëœ chunk ê±´ë„ˆë›°ê¸°
        """
        print("\n" + "=" * 80)
        print("SPLADE Sparse Vector ì¸ì½”ë”© ì‹œì‘")
        print("=" * 80)
        
        # ì¸ì½”ë”© ëŒ€ìƒ chunk ê°€ì ¸ì˜¤ê¸°
        chunks = self.get_chunks_to_encode(
            doc_type=doc_type,
            limit=limit,
            skip_encoded=resume
        )
        
        total_chunks = len(chunks)
        if total_chunks == 0:
            print("âœ… ì¸ì½”ë”©í•  chunkê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ“Š ì¸ì½”ë”© ëŒ€ìƒ: {total_chunks}ê°œ chunk")
        if doc_type:
            print(f"   ë¬¸ì„œ íƒ€ì…: {doc_type}")
        
        # ë°°ì¹˜ ì²˜ë¦¬
        total_success = 0
        total_fail = 0
        
        with tqdm(total=total_chunks, desc="ì¸ì½”ë”© ì§„í–‰") as pbar:
            for i in range(0, total_chunks, self.batch_size):
                batch = chunks[i:i + self.batch_size]
                
                # ë°°ì¹˜ ì¸ì½”ë”©
                encoded_results = self.encode_batch(batch)
                
                # DB ì €ì¥
                success_count, fail_count = self.save_encoded_vectors(encoded_results)
                
                total_success += success_count
                total_fail += fail_count
                
                pbar.update(len(batch))
                pbar.set_postfix({
                    'ì„±ê³µ': total_success,
                    'ì‹¤íŒ¨': total_fail,
                    'ì§„í–‰ë¥ ': f"{total_success + total_fail}/{total_chunks}"
                })
        
        # ìµœì¢… í†µê³„
        print("\n" + "=" * 80)
        print("ì¸ì½”ë”© ì™„ë£Œ")
        print("=" * 80)
        print(f"âœ… ì„±ê³µ: {total_success}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {total_fail}ê°œ")
        print(f"ğŸ“Š ì´ ì²˜ë¦¬: {total_success + total_fail}ê°œ / {total_chunks}ê°œ")
        
        if total_fail > 0:
            print(f"\nâš ï¸  ì‹¤íŒ¨í•œ chunkëŠ” ë‚˜ì¤‘ì— ì¬ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    def get_statistics(self) -> Dict:
        """ì¸ì½”ë”© í†µê³„ ì •ë³´ ì¡°íšŒ"""
        self.connect_db()
        cur = self.conn.cursor()
        
        # ì „ì²´ í†µê³„
        cur.execute("""
            SELECT 
                COUNT(*) as total_chunks,
                COUNT(CASE WHEN splade_encoded = TRUE THEN 1 END) as encoded_chunks,
                COUNT(CASE WHEN splade_encoded = FALSE OR splade_encoded IS NULL THEN 1 END) as unencoded_chunks
            FROM chunks
            WHERE drop = FALSE
        """)
        total_stats = cur.fetchone()
        
        # ë¬¸ì„œ íƒ€ì…ë³„ í†µê³„
        cur.execute("""
            SELECT 
                d.doc_type,
                COUNT(*) as total,
                COUNT(CASE WHEN c.splade_encoded = TRUE THEN 1 END) as encoded
            FROM chunks c
            JOIN documents d ON c.doc_id = d.doc_id
            WHERE c.drop = FALSE
            GROUP BY d.doc_type
            ORDER BY total DESC
        """)
        doc_type_stats = cur.fetchall()
        
        cur.close()
        
        return {
            'total': {
                'total_chunks': total_stats[0],
                'encoded_chunks': total_stats[1],
                'unencoded_chunks': total_stats[2],
                'encode_rate': (total_stats[1] / total_stats[0] * 100) if total_stats[0] > 0 else 0
            },
            'by_doc_type': [
                {
                    'doc_type': row[0],
                    'total': row[1],
                    'encoded': row[2],
                    'rate': (row[2] / row[1] * 100) if row[1] > 0 else 0
                }
                for row in doc_type_stats
            ]
        }


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='SPLADE Sparse Vector ì¸ì½”ë”© íŒŒì´í”„ë¼ì¸')
    parser.add_argument('--doc-type', type=str, help='ë¬¸ì„œ íƒ€ì… í•„í„° (ì˜ˆ: law, criteria_*)')
    parser.add_argument('--limit', type=int, help='ìµœëŒ€ ì²˜ë¦¬ ê°œìˆ˜')
    parser.add_argument('--batch-size', type=int, default=32, help='ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 32)')
    parser.add_argument('--remote', action='store_true', help='ì›ê²© API ì„œë²„ ì‚¬ìš©')
    parser.add_argument('--api-url', type=str, help='ì›ê²© API URL')
    parser.add_argument('--device', type=str, choices=['cuda', 'cpu'], help='ë¡œì»¬ ëª¨ë“œ ë””ë°”ì´ìŠ¤')
    parser.add_argument('--no-resume', action='store_true', help='ì´ë¯¸ ì¸ì½”ë”©ëœ chunkë„ ì¬ì¸ì½”ë”©')
    parser.add_argument('--stats-only', action='store_true', help='í†µê³„ë§Œ ì¡°íšŒí•˜ê³  ì¢…ë£Œ')
    
    args = parser.parse_args()
    
    # DB ì„¤ì •
    db_config = {
        'host': os.getenv('DB_HOST', 'localhost'),
        'port': int(os.getenv('DB_PORT', 5432)),
        'database': os.getenv('DB_NAME', 'ddoksori'),
        'user': os.getenv('DB_USER', 'postgres'),
        'password': os.getenv('DB_PASSWORD', 'postgres')
    }
    
    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    try:
        pipeline = SPLADEEncodingPipeline(
            db_config=db_config,
            batch_size=args.batch_size,
            use_remote=args.remote,
            api_url=args.api_url,
            device=args.device
        )
    except Exception as e:
        print(f"âŒ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        sys.exit(1)
    
    # í†µê³„ë§Œ ì¡°íšŒ
    if args.stats_only:
        stats = pipeline.get_statistics()
        print("\nğŸ“Š SPLADE ì¸ì½”ë”© í†µê³„")
        print("=" * 80)
        print(f"ì „ì²´ chunk: {stats['total']['total_chunks']}ê°œ")
        print(f"ì¸ì½”ë”© ì™„ë£Œ: {stats['total']['encoded_chunks']}ê°œ")
        print(f"ì¸ì½”ë”© ë¯¸ì™„ë£Œ: {stats['total']['unencoded_chunks']}ê°œ")
        print(f"ì¸ì½”ë”© ì™„ë£Œìœ¨: {stats['total']['encode_rate']:.1f}%")
        print("\në¬¸ì„œ íƒ€ì…ë³„:")
        for dt in stats['by_doc_type']:
            print(f"  {dt['doc_type']}: {dt['encoded']}/{dt['total']} ({dt['rate']:.1f}%)")
        return
    
    # ì¸ì½”ë”© ìˆ˜í–‰
    pipeline.encode_all_chunks(
        doc_type=args.doc_type,
        limit=args.limit,
        resume=not args.no_resume
    )
    
    # ìµœì¢… í†µê³„ ì¶œë ¥
    stats = pipeline.get_statistics()
    print("\nğŸ“Š ìµœì¢… í†µê³„")
    print("=" * 80)
    print(f"ì¸ì½”ë”© ì™„ë£Œìœ¨: {stats['total']['encode_rate']:.1f}%")
    
    # ì—°ê²° ì¢…ë£Œ
    if pipeline.conn:
        pipeline.conn.close()


if __name__ == "__main__":
    main()
