"""
RAG í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

test_multi_stage_rag.pyì˜ ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ê³  ë¶„ì„
"""

import json
import sys
from pathlib import Path
from typing import List, Dict


def load_results(file_path: str = "test_results.json") -> List[Dict]:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ JSON íŒŒì¼ ë¡œë“œ"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        print("ë¨¼ì € test_multi_stage_rag.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        sys.exit(1)


def print_separator(title: str = None):
    """êµ¬ë¶„ì„  ì¶œë ¥"""
    if title:
        print(f"\n{'='*80}")
        print(f"  {title}")
        print(f"{'='*80}\n")
    else:
        print(f"{'='*80}\n")


def analyze_search_distribution(results: List[Dict]):
    """ê²€ìƒ‰ ê²°ê³¼ ë¶„í¬ ë¶„ì„"""
    print_separator("ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ë¶„í¬ ë¶„ì„")
    
    total_tests = len(results)
    
    # ê° ì†ŒìŠ¤ë³„ ì²­í¬ ìˆ˜ í†µê³„
    law_chunks = [r['law_chunks'] for r in results]
    criteria_chunks = [r['criteria_chunks'] for r in results]
    mediation_chunks = [r['mediation_chunks'] for r in results]
    counsel_chunks = [r['counsel_chunks'] for r in results]
    
    print("ğŸ“ˆ ì†ŒìŠ¤ë³„ í‰ê·  ì²­í¬ ìˆ˜:")
    print(f"  - ë²•ë ¹: {sum(law_chunks)/total_tests:.1f}ê°œ (ë²”ìœ„: {min(law_chunks)}~{max(law_chunks)})")
    print(f"  - ê¸°ì¤€: {sum(criteria_chunks)/total_tests:.1f}ê°œ (ë²”ìœ„: {min(criteria_chunks)}~{max(criteria_chunks)})")
    print(f"  - ë¶„ìŸì¡°ì •ì‚¬ë¡€: {sum(mediation_chunks)/total_tests:.1f}ê°œ (ë²”ìœ„: {min(mediation_chunks)}~{max(mediation_chunks)})")
    print(f"  - í”¼í•´êµ¬ì œì‚¬ë¡€: {sum(counsel_chunks)/total_tests:.1f}ê°œ (ë²”ìœ„: {min(counsel_chunks)}~{max(counsel_chunks)})")
    
    # Fallback ì‚¬ìš© ë¹ˆë„
    fallback_count = sum(1 for r in results if r['fallback_triggered'])
    print(f"\nğŸ”„ Fallback ì‚¬ìš©:")
    print(f"  - ë°œë™ íšŸìˆ˜: {fallback_count}/{total_tests} ({fallback_count/total_tests*100:.1f}%)")
    
    if fallback_count > 0:
        print(f"  - Fallback ë°œë™ ì¼€ì´ìŠ¤:")
        for r in results:
            if r['fallback_triggered']:
                print(f"    â€¢ í…ŒìŠ¤íŠ¸ {r['test_id']}: {r['test_name']}")
                print(f"      (ë¶„ìŸì¡°ì •ì‚¬ë¡€: {r['mediation_chunks']}ê°œ, í”¼í•´êµ¬ì œì‚¬ë¡€: {r['counsel_chunks']}ê°œ)")


def analyze_similarity(results: List[Dict]):
    """ìœ ì‚¬ë„ ë¶„ì„"""
    print_separator("ğŸ“Š ìœ ì‚¬ë„ ë¶„ì„")
    
    avg_sims = [r['avg_similarity'] for r in results]
    max_sims = [r['max_similarity'] for r in results]
    min_sims = [r['min_similarity'] for r in results]
    
    print("ğŸ“ˆ ìœ ì‚¬ë„ í†µê³„:")
    print(f"  - ì „ì²´ í‰ê·  ìœ ì‚¬ë„: {sum(avg_sims)/len(avg_sims):.3f}")
    print(f"  - ìµœê³  ìœ ì‚¬ë„ (ëª¨ë“  í…ŒìŠ¤íŠ¸): {max(max_sims):.3f}")
    print(f"  - ìµœì € ìœ ì‚¬ë„ (ëª¨ë“  í…ŒìŠ¤íŠ¸): {min(min_sims):.3f}")
    
    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ë³„ ìœ ì‚¬ë„:")
    for r in sorted(results, key=lambda x: x['avg_similarity'], reverse=True):
        print(f"  â€¢ í…ŒìŠ¤íŠ¸ {r['test_id']}: {r['test_name']}")
        print(f"    í‰ê· ={r['avg_similarity']:.3f}, ìµœëŒ€={r['max_similarity']:.3f}, ìµœì†Œ={r['min_similarity']:.3f}")
    
    # ìœ ì‚¬ë„ í’ˆì§ˆ í‰ê°€
    high_quality = sum(1 for s in avg_sims if s >= 0.7)
    medium_quality = sum(1 for s in avg_sims if 0.5 <= s < 0.7)
    low_quality = sum(1 for s in avg_sims if s < 0.5)
    
    print(f"\nğŸ¯ ìœ ì‚¬ë„ í’ˆì§ˆ ë¶„í¬:")
    print(f"  - ë†’ìŒ (â‰¥0.7): {high_quality}ê±´ ({high_quality/len(results)*100:.1f}%)")
    print(f"  - ì¤‘ê°„ (0.5~0.7): {medium_quality}ê±´ ({medium_quality/len(results)*100:.1f}%)")
    print(f"  - ë‚®ìŒ (<0.5): {low_quality}ê±´ ({low_quality/len(results)*100:.1f}%)")


def analyze_agency_recommendation(results: List[Dict]):
    """ê¸°ê´€ ì¶”ì²œ ë¶„ì„"""
    print_separator("ğŸ“Š ê¸°ê´€ ì¶”ì²œ ë¶„ì„")
    
    total_tests = len(results)
    correct_count = sum(1 for r in results if r['agency_correct'])
    accuracy = correct_count / total_tests * 100
    
    print(f"ğŸ¯ ì¶”ì²œ ì •í™•ë„: {accuracy:.1f}% ({correct_count}/{total_tests})")
    
    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ë³„ ì¶”ì²œ ê²°ê³¼:")
    for r in results:
        status = "âœ“ ì •í™•" if r['agency_correct'] else "âœ— ë¶€ì •í™•"
        print(f"  â€¢ í…ŒìŠ¤íŠ¸ {r['test_id']}: {r['test_name']}")
        print(f"    ì¶”ì²œ ê¸°ê´€: {r['recommended_agency']} ({status})")
        print(f"    ì¶”ì²œ ì ìˆ˜: {r['agency_score']:.3f}")
    
    # ê¸°ê´€ë³„ ì¶”ì²œ ë¹ˆë„
    from collections import Counter
    agency_counts = Counter(r['recommended_agency'] for r in results if r['recommended_agency'])
    
    print(f"\nğŸ“ˆ ê¸°ê´€ë³„ ì¶”ì²œ ë¹ˆë„:")
    for agency, count in agency_counts.most_common():
        print(f"  - {agency}: {count}íšŒ ({count/total_tests*100:.1f}%)")


def analyze_performance(results: List[Dict]):
    """ì„±ëŠ¥ ë¶„ì„"""
    print_separator("ğŸ“Š ì„±ëŠ¥ ë¶„ì„")
    
    elapsed_times = [r['elapsed_time'] for r in results]
    total_chunks = [r['total_chunks'] for r in results]
    
    print("â±ï¸ ê²€ìƒ‰ ì‹œê°„:")
    print(f"  - í‰ê·  ì‹œê°„: {sum(elapsed_times)/len(elapsed_times):.2f}ì´ˆ")
    print(f"  - ìµœì†Œ ì‹œê°„: {min(elapsed_times):.2f}ì´ˆ")
    print(f"  - ìµœëŒ€ ì‹œê°„: {max(elapsed_times):.2f}ì´ˆ")
    
    print(f"\nğŸ“Š ì²­í¬ ë‹¹ ê²€ìƒ‰ ì‹œê°„:")
    avg_time_per_chunk = sum(
        r['elapsed_time'] / r['total_chunks'] 
        for r in results if r['total_chunks'] > 0
    ) / len(results)
    print(f"  - í‰ê· : {avg_time_per_chunk:.3f}ì´ˆ/ì²­í¬")
    
    print(f"\nğŸ“ˆ í…ŒìŠ¤íŠ¸ë³„ ì„±ëŠ¥:")
    for r in sorted(results, key=lambda x: x['elapsed_time']):
        print(f"  â€¢ í…ŒìŠ¤íŠ¸ {r['test_id']}: {r['elapsed_time']:.2f}ì´ˆ ({r['total_chunks']}ê°œ ì²­í¬)")


def generate_recommendations(results: List[Dict]):
    """ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ê°œì„  ì œì•ˆ"""
    print_separator("ğŸ’¡ ê°œì„  ì œì•ˆ")
    
    recommendations = []
    
    # 1. Fallback ë¹ˆë„ ì²´í¬
    fallback_count = sum(1 for r in results if r['fallback_triggered'])
    fallback_rate = fallback_count / len(results)
    
    if fallback_rate > 0.5:
        recommendations.append(
            "âš ï¸ Fallbackì´ 50% ì´ìƒ ë°œë™ë˜ê³  ìˆìŠµë‹ˆë‹¤. "
            "ë¶„ìŸì¡°ì •ì‚¬ë¡€ ë°ì´í„°ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ mediation_thresholdë¥¼ ë‚®ì¶”ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”."
        )
    
    # 2. ìœ ì‚¬ë„ ì²´í¬
    avg_similarities = [r['avg_similarity'] for r in results]
    overall_avg = sum(avg_similarities) / len(avg_similarities)
    
    if overall_avg < 0.5:
        recommendations.append(
            "âš ï¸ ì „ì²´ í‰ê·  ìœ ì‚¬ë„ê°€ 0.5 ë¯¸ë§Œì…ë‹ˆë‹¤. "
            "ì²­í‚¹ ì „ëµì„ ì¬ê²€í† í•˜ê±°ë‚˜ ì„ë² ë”© ëª¨ë¸ì„ íŠœë‹í•˜ì„¸ìš”."
        )
    elif overall_avg >= 0.7:
        recommendations.append(
            "âœ… ìœ ì‚¬ë„ê°€ ìš°ìˆ˜í•©ë‹ˆë‹¤. í˜„ì¬ ì²­í‚¹ ë° ì„ë² ë”© ì „ëµì„ ìœ ì§€í•˜ì„¸ìš”."
        )
    
    # 3. ê¸°ê´€ ì¶”ì²œ ì •í™•ë„ ì²´í¬
    correct_count = sum(1 for r in results if r['agency_correct'])
    accuracy = correct_count / len(results)
    
    if accuracy < 0.75:
        recommendations.append(
            "âš ï¸ ê¸°ê´€ ì¶”ì²œ ì •í™•ë„ê°€ 75% ë¯¸ë§Œì…ë‹ˆë‹¤. "
            "í‚¤ì›Œë“œ ê·œì¹™ì„ ê°œì„ í•˜ê±°ë‚˜ ê°€ì¤‘ì¹˜ ë¹„ìœ¨(rule_weight/result_weight)ì„ ì¡°ì •í•˜ì„¸ìš”."
        )
    elif accuracy == 1.0:
        recommendations.append(
            "âœ… ê¸°ê´€ ì¶”ì²œì´ 100% ì •í™•í•©ë‹ˆë‹¤. í˜„ì¬ ì•Œê³ ë¦¬ì¦˜ì„ ìœ ì§€í•˜ì„¸ìš”."
        )
    
    # 4. ì„±ëŠ¥ ì²´í¬
    elapsed_times = [r['elapsed_time'] for r in results]
    avg_time = sum(elapsed_times) / len(elapsed_times)
    
    if avg_time > 5.0:
        recommendations.append(
            "âš ï¸ í‰ê·  ê²€ìƒ‰ ì‹œê°„ì´ 5ì´ˆë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤. "
            "ë²¡í„° ì¸ë±ìŠ¤ë¥¼ ìµœì í™”í•˜ê±°ë‚˜ top_k ê°’ì„ ì¤„ì´ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”."
        )
    elif avg_time < 2.0:
        recommendations.append(
            "âœ… ê²€ìƒ‰ ì„±ëŠ¥ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤ (í‰ê·  2ì´ˆ ë¯¸ë§Œ)."
        )
    
    # 5. ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ì²´í¬
    total_chunks_list = [r['total_chunks'] for r in results]
    avg_chunks = sum(total_chunks_list) / len(total_chunks_list)
    
    if avg_chunks < 5:
        recommendations.append(
            "âš ï¸ í‰ê·  ê²€ìƒ‰ ê²°ê³¼ê°€ 5ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤. "
            "top_k ê°’ì„ ëŠ˜ë¦¬ê±°ë‚˜ ê²€ìƒ‰ í•„í„°ë¥¼ ì™„í™”í•˜ì„¸ìš”."
        )
    elif avg_chunks > 20:
        recommendations.append(
            "âš ï¸ í‰ê·  ê²€ìƒ‰ ê²°ê³¼ê°€ 20ê°œë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤. "
            "LLM ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. top_k ê°’ì„ ì¤„ì´ì„¸ìš”."
        )
    
    if recommendations:
        for rec in recommendations:
            print(f"\n{rec}")
    else:
        print("\nâœ… ëª¨ë“  ì§€í‘œê°€ ì–‘í˜¸í•©ë‹ˆë‹¤. ì¶”ê°€ ê°œì„  ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print_separator("ğŸ” RAG í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„")
    
    # ê²°ê³¼ íŒŒì¼ ë¡œë“œ
    results = load_results()
    
    print(f"ğŸ“ ë¡œë“œëœ í…ŒìŠ¤íŠ¸: {len(results)}ê±´")
    print(f"ğŸ“… ë¶„ì„ ì‹œì‘\n")
    
    # ê°ì¢… ë¶„ì„ ì‹¤í–‰
    analyze_search_distribution(results)
    analyze_similarity(results)
    analyze_agency_recommendation(results)
    analyze_performance(results)
    generate_recommendations(results)
    
    print_separator("âœ… ë¶„ì„ ì™„ë£Œ")


if __name__ == "__main__":
    main()
