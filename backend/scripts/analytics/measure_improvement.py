#!/usr/bin/env python3
"""
ë°ì´í„° í’ˆì§ˆ ê°œì„  íš¨ê³¼ ì¸¡ì • ìŠ¤í¬ë¦½íŠ¸

ê°œì„  ì „/í›„ ë°ì´í„°ë¥¼ ë¹„êµí•˜ì—¬ í’ˆì§ˆ ê°œì„  íš¨ê³¼ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.
"""

import json
import sys
from pathlib import Path
from typing import Dict, List
from datetime import datetime
from collections import defaultdict, Counter

class ImprovementMeasurer:
    """ê°œì„  íš¨ê³¼ ì¸¡ì •"""
    
    def __init__(self, transformed_dir: str = "backend/data/transformed"):
        """ì´ˆê¸°í™”"""
        self.transformed_dir = Path(transformed_dir)
        self.validation_file = self.transformed_dir / "validation_result.json"
        self.summary_file = self.transformed_dir / "transformation_summary.json"
        
        # ê°œì„  ì „ ê¸°ì¤€ ë°ì´í„° (ê³„íš ë¬¸ì„œì—ì„œ)
        self.baseline = {
            'total_documents': 12150,
            'total_chunks': 14898,
            'critical_issues': 92,
            'warnings': 1797,
            'short_chunks': 1500,  # ~1,500ê°œ
            'long_chunks': 300     # ~300ê°œ
        }
    
    def load_current_data(self) -> Dict:
        """í˜„ì¬ ë°ì´í„° ë¡œë“œ"""
        print("ğŸ“‚ í˜„ì¬ ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # ê²€ì¦ ê²°ê³¼
        with open(self.validation_file, 'r', encoding='utf-8') as f:
            validation = json.load(f)
        
        # ë³€í™˜ ìš”ì•½
        with open(self.summary_file, 'r', encoding='utf-8') as f:
            summary = json.load(f)
        
        print("  âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        
        return {
            'validation': validation,
            'summary': summary
        }
    
    def analyze_chunk_sizes(self) -> Dict:
        """ì²­í¬ í¬ê¸° ë¶„ì„"""
        print("\nğŸ“Š ì²­í¬ í¬ê¸° ë¶„ì„ ì¤‘...")
        
        # ëª¨ë“  ë³€í™˜ëœ íŒŒì¼ì—ì„œ ì²­í¬ í†µê³„ ìˆ˜ì§‘
        chunk_stats = {
            'by_type': defaultdict(list),
            'total': [],
            'short_count': 0,  # < 100ì
            'optimal_count': 0,  # 100-2000ì
            'long_count': 0,  # > 2000ì
            'very_long_count': 0  # > 5000ì
        }
        
        for json_file in self.transformed_dir.glob("*.json"):
            if json_file.name in ['validation_result.json', 'transformation_summary.json']:
                continue
            
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
                if isinstance(data, dict) and 'documents' in data:
                    docs = data['documents']
                elif isinstance(data, list):
                    docs = data
                else:
                    docs = [data]
                
                for doc in docs:
                    for chunk in doc.get('chunks', []):
                        if chunk.get('drop'):
                            continue
                        
                        content_len = chunk.get('content_length', len(chunk.get('content', '')))
                        chunk_type = chunk.get('chunk_type', 'unknown')
                        
                        chunk_stats['by_type'][chunk_type].append(content_len)
                        chunk_stats['total'].append(content_len)
                        
                        # í¬ê¸°ë³„ ì¹´ìš´íŠ¸
                        if content_len < 100:
                            chunk_stats['short_count'] += 1
                        elif content_len <= 2000:
                            chunk_stats['optimal_count'] += 1
                        elif content_len <= 5000:
                            chunk_stats['long_count'] += 1
                        else:
                            chunk_stats['very_long_count'] += 1
            
            except Exception as e:
                print(f"  âš ï¸  {json_file.name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"  âœ… {len(chunk_stats['total'])}ê°œ ì²­í¬ ë¶„ì„ ì™„ë£Œ")
        
        return chunk_stats
    
    def calculate_improvement(self, current_data: Dict, chunk_stats: Dict) -> Dict:
        """ê°œì„ ìœ¨ ê³„ì‚°"""
        print("\nğŸ“ˆ ê°œì„ ìœ¨ ê³„ì‚° ì¤‘...")
        
        validation = current_data['validation']
        summary = current_data['summary']
        
        # í˜„ì¬ í†µê³„
        current = {
            'total_documents': summary['stats']['documents'],
            'total_chunks': summary['stats']['chunks'],
            'critical_issues': validation['issues_count'],
            'warnings': validation['warnings_count'],
            'short_chunks': chunk_stats['short_count'],
            'long_chunks': chunk_stats['long_count'] + chunk_stats['very_long_count']
        }
        
        # ê°œì„ ìœ¨ ê³„ì‚°
        improvements = {}
        for key in ['critical_issues', 'short_chunks', 'long_chunks']:
            baseline_val = self.baseline[key]
            current_val = current[key]
            
            if baseline_val > 0:
                improvement_rate = ((baseline_val - current_val) / baseline_val) * 100
            else:
                improvement_rate = 0
            
            improvements[key] = {
                'baseline': baseline_val,
                'current': current_val,
                'improvement_rate': improvement_rate,
                'delta': baseline_val - current_val
            }
        
        print("  âœ… ê°œì„ ìœ¨ ê³„ì‚° ì™„ë£Œ")
        
        return {
            'baseline': self.baseline,
            'current': current,
            'improvements': improvements,
            'chunk_stats': chunk_stats
        }
    
    def generate_report(self, analysis: Dict) -> str:
        """ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\nğŸ“ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        baseline = analysis['baseline']
        current = analysis['current']
        improvements = analysis['improvements']
        chunk_stats = analysis['chunk_stats']
        
        report = []
        report.append("=" * 100)
        report.append("ë°ì´í„° í’ˆì§ˆ ê°œì„  íš¨ê³¼ ì¸¡ì • ë¦¬í¬íŠ¸")
        report.append("=" * 100)
        report.append(f"ìƒì„± ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # 1. ì „ì²´ í†µê³„ ë¹„êµ
        report.append("1. ì „ì²´ ë°ì´í„° í†µê³„")
        report.append("-" * 100)
        report.append(f"{'ì§€í‘œ':<30} {'ê°œì„  ì „':>15} {'ê°œì„  í›„':>15} {'ë³€í™”ëŸ‰':>15} {'ë¹„ê³ ':<20}")
        report.append("-" * 100)
        
        report.append(f"{'ì´ ë¬¸ì„œ ìˆ˜':<30} {baseline['total_documents']:>15,} "
                     f"{current['total_documents']:>15,} "
                     f"{current['total_documents'] - baseline['total_documents']:>+15,}")
        
        report.append(f"{'ì´ ì²­í¬ ìˆ˜':<30} {baseline['total_chunks']:>15,} "
                     f"{current['total_chunks']:>15,} "
                     f"{current['total_chunks'] - baseline['total_chunks']:>+15,}")
        
        report.append("")
        
        # 2. í’ˆì§ˆ ì§€í‘œ ê°œì„ 
        report.append("2. í’ˆì§ˆ ì§€í‘œ ê°œì„ ")
        report.append("-" * 100)
        report.append(f"{'ì§€í‘œ':<30} {'ê°œì„  ì „':>15} {'ê°œì„  í›„':>15} {'ê°œì„ ëŸ‰':>15} {'ê°œì„ ìœ¨':>15}")
        report.append("-" * 100)
        
        for key, label in [
            ('critical_issues', 'Critical Issues (ë¹ˆ ì²­í¬)'),
            ('short_chunks', 'ì§§ì€ ì²­í¬ (< 100ì)'),
            ('long_chunks', 'ê¸´ ì²­í¬ (> 2,000ì)')
        ]:
            imp = improvements[key]
            report.append(f"{label:<30} {imp['baseline']:>15,} "
                         f"{imp['current']:>15,} "
                         f"{imp['delta']:>+15,} "
                         f"{imp['improvement_rate']:>14.1f}%")
        
        report.append("")
        
        # 3. ì²­í¬ í¬ê¸° ë¶„í¬
        report.append("3. ì²­í¬ í¬ê¸° ë¶„í¬ (ê°œì„  í›„)")
        report.append("-" * 100)
        
        total_active_chunks = len(chunk_stats['total'])
        
        if total_active_chunks > 0:
            report.append(f"{'êµ¬ê°„':<30} {'ê°œìˆ˜':>15} {'ë¹„ìœ¨':>15}")
            report.append("-" * 100)
            report.append(f"{'ì§§ì€ ì²­í¬ (< 100ì)':<30} "
                         f"{chunk_stats['short_count']:>15,} "
                         f"{chunk_stats['short_count']/total_active_chunks*100:>14.1f}%")
            report.append(f"{'ìµœì  ì²­í¬ (100-2,000ì)':<30} "
                         f"{chunk_stats['optimal_count']:>15,} "
                         f"{chunk_stats['optimal_count']/total_active_chunks*100:>14.1f}%")
            report.append(f"{'ê¸´ ì²­í¬ (2,000-5,000ì)':<30} "
                         f"{chunk_stats['long_count']:>15,} "
                         f"{chunk_stats['long_count']/total_active_chunks*100:>14.1f}%")
            report.append(f"{'ë§¤ìš° ê¸´ ì²­í¬ (> 5,000ì)':<30} "
                         f"{chunk_stats['very_long_count']:>15,} "
                         f"{chunk_stats['very_long_count']/total_active_chunks*100:>14.1f}%")
            report.append(f"{'í•©ê³„':<30} {total_active_chunks:>15,} {'100.0%':>15}")
        else:
            report.append("âŒ ë¶„ì„í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        report.append("")
        
        # 4. ì²­í¬ íƒ€ì…ë³„ í†µê³„
        report.append("4. ì²­í¬ íƒ€ì…ë³„ í‰ê·  ê¸¸ì´")
        report.append("-" * 100)
        report.append(f"{'ì²­í¬ íƒ€ì…':<30} {'ê°œìˆ˜':>15} {'í‰ê·  ê¸¸ì´':>15} {'ìµœì†Œ':>15} {'ìµœëŒ€':>15}")
        report.append("-" * 100)
        
        for chunk_type, lengths in sorted(chunk_stats['by_type'].items()):
            if lengths:
                avg_len = sum(lengths) / len(lengths)
                min_len = min(lengths)
                max_len = max(lengths)
                report.append(f"{chunk_type:<30} {len(lengths):>15,} "
                             f"{avg_len:>15,.0f} {min_len:>15,} {max_len:>15,}")
        
        report.append("")
        
        # 5. ì£¼ìš” ê°œì„  ì‚¬í•­ ìš”ì•½
        report.append("5. ì£¼ìš” ê°œì„  ì‚¬í•­ ìš”ì•½")
        report.append("-" * 100)
        
        critical_improvement = improvements['critical_issues']
        if critical_improvement['improvement_rate'] >= 100:
            report.append(f"âœ… Critical Issues ì™„ì „ í•´ê²°: {critical_improvement['baseline']}ê°œ â†’ {critical_improvement['current']}ê°œ")
        else:
            report.append(f"âš ï¸  Critical Issues ë¶€ë¶„ ê°œì„ : {critical_improvement['baseline']}ê°œ â†’ {critical_improvement['current']}ê°œ "
                         f"({critical_improvement['improvement_rate']:.1f}% ê°œì„ )")
        
        short_improvement = improvements['short_chunks']
        report.append(f"âœ… ì§§ì€ ì²­í¬ ê°ì†Œ: {short_improvement['baseline']}ê°œ â†’ {short_improvement['current']}ê°œ "
                     f"({short_improvement['improvement_rate']:.1f}% ê°œì„ )")
        
        long_improvement = improvements['long_chunks']
        report.append(f"âœ… ê¸´ ì²­í¬ ê°ì†Œ: {long_improvement['baseline']}ê°œ â†’ {long_improvement['current']}ê°œ "
                     f"({long_improvement['improvement_rate']:.1f}% ê°œì„ )")
        
        if total_active_chunks > 0:
            optimal_rate = chunk_stats['optimal_count'] / total_active_chunks * 100
            report.append(f"âœ… ìµœì  í¬ê¸° ì²­í¬ ë¹„ìœ¨: {optimal_rate:.1f}%")
        else:
            optimal_rate = 0
            report.append(f"âŒ ìµœì  í¬ê¸° ì²­í¬ ë¹„ìœ¨: ì¸¡ì • ë¶ˆê°€ (ì²­í¬ ì—†ìŒ)")
        
        report.append("")
        
        # 6. ê²€ìƒ‰ í’ˆì§ˆ ì˜ˆìƒ íš¨ê³¼
        report.append("6. ê²€ìƒ‰ í’ˆì§ˆ ì˜ˆìƒ íš¨ê³¼")
        report.append("-" * 100)
        report.append("âœ… ë¹ˆ ì²­í¬ ì œê±°ë¡œ ì¸í•œ ì„ë² ë”© í’ˆì§ˆ í–¥ìƒ")
        report.append("âœ… ì§§ì€ ì²­í¬ ë³‘í•©/ì œê±°ë¡œ ì¸í•œ ë¬¸ë§¥ ì •ë³´ í–¥ìƒ")
        report.append("âœ… ê¸´ ì²­í¬ ë¶„í• ë¡œ ì¸í•œ ê²€ìƒ‰ ì •ë°€ë„ í–¥ìƒ")
        report.append(f"âœ… ìµœì  í¬ê¸° ì²­í¬ ë¹„ìœ¨ ì¦ê°€: {optimal_rate:.1f}%")
        report.append("")
        report.append("ì˜ˆìƒ ê²€ìƒ‰ ì •í™•ë„ ê°œì„ : +15-25% (ì‹¤ì œ ì¸¡ì • í•„ìš”)")
        
        report.append("")
        report.append("=" * 100)
        
        print("  âœ… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
        
        return "\n".join(report)
    
    def save_report(self, report: str, output_file: str = None):
        """ë¦¬í¬íŠ¸ ì €ì¥"""
        if output_file is None:
            output_file = self.transformed_dir / "improvement_report.txt"
        else:
            output_file = Path(output_file)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"\nğŸ’¾ ë¦¬í¬íŠ¸ ì €ì¥: {output_file}")
    
    def run(self):
        """ì¸¡ì • ì‹¤í–‰"""
        print("=" * 100)
        print("ë°ì´í„° í’ˆì§ˆ ê°œì„  íš¨ê³¼ ì¸¡ì • ì‹œì‘")
        print("=" * 100)
        
        try:
            # 1. í˜„ì¬ ë°ì´í„° ë¡œë“œ
            current_data = self.load_current_data()
            
            # 2. ì²­í¬ í¬ê¸° ë¶„ì„
            chunk_stats = self.analyze_chunk_sizes()
            
            # 3. ê°œì„ ìœ¨ ê³„ì‚°
            analysis = self.calculate_improvement(current_data, chunk_stats)
            
            # 4. ë¦¬í¬íŠ¸ ìƒì„±
            report = self.generate_report(analysis)
            
            # 5. ì¶œë ¥ ë° ì €ì¥
            print("\n" + report)
            self.save_report(report)
            
            print("\nâœ… ì¸¡ì • ì™„ë£Œ!")
            return 0
            
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return 1

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    measurer = ImprovementMeasurer()
    return measurer.run()

if __name__ == '__main__':
    sys.exit(main())
