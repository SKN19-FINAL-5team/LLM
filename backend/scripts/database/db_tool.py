#!/usr/bin/env python3
"""
ë°ì´í„°ë² ì´ìŠ¤ í†µí•© ë„êµ¬
DB ìƒíƒœ í™•ì¸, í†µê³„ ìˆ˜ì§‘, ì—°ê²° í…ŒìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„° í™•ì¸, Vector DB ê²€ì‚¬ ê¸°ëŠ¥ì„ í†µí•©

ì‚¬ìš©ë²•:
    python backend/scripts/database/db_tool.py --status
    python backend/scripts/database/db_tool.py --stats
    python backend/scripts/database/db_tool.py --check-law
    python backend/scripts/database/db_tool.py --test-connection
    python backend/scripts/database/db_tool.py --inspect
    python backend/scripts/database/db_tool.py --inspect --check-quality
    python backend/scripts/database/db_tool.py --all
"""

import os
import sys
import json
import psycopg2
import numpy as np
import argparse
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from dotenv import load_dotenv
from psycopg2.extras import RealDictCursor

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
backend_dir = Path(__file__).parent.parent.parent
env_file = backend_dir / '.env'
if env_file.exists():
    load_dotenv(env_file)
else:
    root_env = backend_dir.parent / '.env'
    if root_env.exists():
        load_dotenv(root_env)
    else:
        load_dotenv()

# DB ì—°ê²° ì •ë³´ (ì—¬ëŸ¬ í™˜ê²½ ë³€ìˆ˜ ì´ë¦„ ì§€ì›)
DB_CONFIG = {
    'host': os.getenv('DB_HOST', os.getenv('POSTGRES_HOST', 'localhost')),
    'port': int(os.getenv('DB_PORT', os.getenv('POSTGRES_PORT', 5432))),
    'database': os.getenv('DB_NAME', os.getenv('POSTGRES_DB', 'ddoksori')),
    'user': os.getenv('DB_USER', os.getenv('POSTGRES_USER', 'postgres')),
    'password': os.getenv('DB_PASSWORD', os.getenv('POSTGRES_PASSWORD', 'postgres'))
}


class DatabaseTool:
    """ë°ì´í„°ë² ì´ìŠ¤ í†µí•© ë„êµ¬"""
    
    def __init__(self):
        self.conn = None
        self._connect()
    
    def _connect(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        try:
            self.conn = psycopg2.connect(**DB_CONFIG)
            # pgvector íƒ€ì… ë“±ë¡
            try:
                from pgvector.psycopg2 import register_vector
                register_vector(self.conn)
            except ImportError:
                pass  # pgvectorê°€ ì—†ì–´ë„ ê¸°ë³¸ ê¸°ëŠ¥ì€ ë™ì‘
        except Exception as e:
            print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
    
    def check_status(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸ (ê¸°ì¡´ check_db_status.py ê¸°ëŠ¥)"""
        cur = self.conn.cursor()
        
        print("=" * 60)
        print("ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸")
        print("=" * 60)
        
        # 1. documents í…Œì´ë¸” í†µê³„
        print("\nğŸ“Š Documents í…Œì´ë¸” í†µê³„:")
        cur.execute("""
            SELECT 
                doc_type,
                COUNT(*) as count,
                COUNT(CASE WHEN keywords IS NOT NULL THEN 1 END) as with_keywords,
                COUNT(CASE WHEN search_vector IS NOT NULL THEN 1 END) as with_search_vector
            FROM documents
            GROUP BY doc_type
            ORDER BY doc_type
        """)
        
        print("\n{:<30} {:>10} {:>15} {:>20}".format(
            "Doc Type", "Count", "With Keywords", "With Search Vector"
        ))
        print("-" * 80)
        
        total_docs = 0
        for row in cur.fetchall():
            doc_type, count, with_keywords, with_search_vector = row
            total_docs += count
            print("{:<30} {:>10} {:>15} {:>20}".format(
                doc_type or '(NULL)',
                count,
                with_keywords,
                with_search_vector
            ))
        
        print("-" * 80)
        print(f"ì´ ë¬¸ì„œ ìˆ˜: {total_docs:,}")
        
        # 2. chunks í…Œì´ë¸” í†µê³„
        print("\nğŸ“Š Chunks í…Œì´ë¸” í†µê³„:")
        cur.execute("""
            SELECT 
                d.doc_type,
                COUNT(*) as chunk_count,
                COUNT(CASE WHEN c.embedding IS NOT NULL THEN 1 END) as with_embedding,
                COUNT(CASE WHEN c.importance_score IS NOT NULL THEN 1 END) as with_importance,
                COUNT(CASE WHEN c.drop = TRUE THEN 1 END) as dropped
            FROM chunks c
            JOIN documents d ON c.doc_id = d.doc_id
            GROUP BY d.doc_type
            ORDER BY d.doc_type
        """)
        
        print("\n{:<30} {:>12} {:>18} {:>18} {:>10}".format(
            "Doc Type", "Chunks", "With Embedding", "With Importance", "Dropped"
        ))
        print("-" * 90)
        
        total_chunks = 0
        for row in cur.fetchall():
            doc_type, chunk_count, with_embedding, with_importance, dropped = row
            total_chunks += chunk_count
            print("{:<30} {:>12} {:>18} {:>18} {:>10}".format(
                doc_type or '(NULL)',
                chunk_count,
                with_embedding,
                with_importance,
                dropped
            ))
        
        print("-" * 90)
        print(f"ì´ ì²­í¬ ìˆ˜: {total_chunks:,}")
        
        # 3. ë²•ë ¹ ë°ì´í„° ìƒì„¸ í™•ì¸
        print("\nğŸ“š ë²•ë ¹ ë°ì´í„° ìƒì„¸:")
        cur.execute("""
            SELECT 
                d.doc_id,
                d.title,
                COUNT(c.chunk_id) as chunk_count,
                d.keywords
            FROM documents d
            LEFT JOIN chunks c ON d.doc_id = c.doc_id
            WHERE d.doc_type = 'law'
            GROUP BY d.doc_id, d.title, d.keywords
            ORDER BY d.title
            LIMIT 10
        """)
        
        law_docs = cur.fetchall()
        if law_docs:
            for doc_id, title, chunk_count, keywords in law_docs:
                print(f"\n  â€¢ {title}")
                print(f"    - doc_id: {doc_id}")
                print(f"    - chunks: {chunk_count}")
                kw_display = keywords[:5] if keywords else ["(ì—†ìŒ)"]
                print(f"    - keywords: {kw_display}...")
        else:
            print("\n  âš ï¸  ë²•ë ¹ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤!")
        
        # 4. ë¯¼ë²• ì œ750ì¡° ê²€ìƒ‰
        print("\nğŸ” ë¯¼ë²• ì œ750ì¡° ê²€ìƒ‰:")
        cur.execute("""
            SELECT 
                c.chunk_id,
                c.content,
                d.title,
                d.metadata
            FROM chunks c
            JOIN documents d ON c.doc_id = d.doc_id
            WHERE 
                d.doc_type = 'law'
                AND (
                    d.title ILIKE '%ë¯¼ë²•%'
                    OR d.metadata->>'law_name' ILIKE '%ë¯¼ë²•%'
                )
                AND (
                    c.content ILIKE '%ì œ750ì¡°%'
                    OR c.content ILIKE '%750ì¡°%'
                    OR d.metadata->>'article_no' ILIKE '%750%'
                )
            LIMIT 5
        """)
        
        results = cur.fetchall()
        if results:
            for chunk_id, content, title, metadata in results:
                print(f"\n  âœ… ì°¾ìŒ: {chunk_id}")
                print(f"     ì œëª©: {title}")
                print(f"     ë‚´ìš©: {content[:100]}...")
        else:
            print("\n  âš ï¸  ë¯¼ë²• ì œ750ì¡°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            
            # ë¯¼ë²• ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
            cur.execute("""
                SELECT COUNT(*)
                FROM documents
                WHERE doc_type = 'law' AND title ILIKE '%ë¯¼ë²•%'
            """)
            count = cur.fetchone()[0]
            print(f"\n  ë¯¼ë²• ë¬¸ì„œ ìˆ˜: {count}")
            
            if count > 0:
                cur.execute("""
                    SELECT 
                        d.doc_id,
                        d.title,
                        COUNT(c.chunk_id) as chunk_count
                    FROM documents d
                    LEFT JOIN chunks c ON d.doc_id = c.doc_id
                    WHERE d.doc_type = 'law' AND d.title ILIKE '%ë¯¼ë²•%'
                    GROUP BY d.doc_id, d.title
                """)
                for doc_id, title, chunk_count in cur.fetchall():
                    print(f"    - {title}: {chunk_count}ê°œ ì²­í¬")
        
        # 5. í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ í™•ì¸
        print("\nğŸ“‹ í…Œì´ë¸” ì»¬ëŸ¼ í™•ì¸:")
        
        # documents í…Œì´ë¸”
        cur.execute("""
            SELECT column_name, data_type
            FROM information_schema.columns
            WHERE table_name = 'documents'
            ORDER BY ordinal_position
        """)
        print("\n  documents í…Œì´ë¸”:")
        for col_name, data_type in cur.fetchall():
            print(f"    - {col_name}: {data_type}")
        
        # chunks í…Œì´ë¸”
        cur.execute("""
            SELECT column_name, data_type
            FROM information_schema.columns
            WHERE table_name = 'chunks'
            ORDER BY ordinal_position
        """)
        print("\n  chunks í…Œì´ë¸”:")
        for col_name, data_type in cur.fetchall():
            print(f"    - {col_name}: {data_type}")
        
        print("\n" + "=" * 60)
        print("âœ… í™•ì¸ ì™„ë£Œ")
        print("=" * 60)
        
        cur.close()
    
    def get_stats(self, format='json'):
        """í†µê³„ ìˆ˜ì§‘ ë° ì¶œë ¥ (ê¸°ì¡´ get_db_stats.py ê¸°ëŠ¥)"""
        cur = self.conn.cursor()
        
        try:
            # Get statistics from view (if exists)
            try:
                cur.execute('SELECT * FROM v_data_statistics ORDER BY doc_type, source_org')
                stats = cur.fetchall()
                
                stats_list = []
                for row in stats:
                    doc_type, source_org, doc_count, chunk_count, active_count, avg_len, embedded_count = row
                    stats_list.append({
                        'doc_type': doc_type,
                        'source_org': source_org,
                        'document_count': doc_count,
                        'chunk_count': chunk_count,
                        'active_chunk_count': active_count,
                        'avg_chunk_length': float(avg_len) if avg_len else 0,
                        'embedded_chunk_count': embedded_count
                    })
            except:
                stats_list = []
            
            # Get total counts
            cur.execute('SELECT COUNT(*) FROM documents')
            total_docs = cur.fetchone()[0]
            
            cur.execute('SELECT COUNT(*) FROM chunks WHERE drop=FALSE')
            total_chunks = cur.fetchone()[0]
            
            cur.execute('SELECT COUNT(*) FROM chunks WHERE drop=FALSE AND embedding IS NOT NULL')
            embedded_chunks = cur.fetchone()[0]
            
            # Get doc_type counts
            cur.execute('''
                SELECT doc_type, COUNT(*) 
                FROM documents 
                GROUP BY doc_type 
                ORDER BY doc_type
            ''')
            doc_type_counts = {row[0]: row[1] for row in cur.fetchall()}
            
            result = {
                'total_documents': total_docs,
                'total_active_chunks': total_chunks,
                'total_embedded_chunks': embedded_chunks,
                'embedding_coverage_percent': round(embedded_chunks/total_chunks*100, 2) if total_chunks > 0 else 0,
                'doc_type_counts': doc_type_counts,
                'detailed_stats': stats_list
            }
            
            if format == 'json':
                print(json.dumps(result, ensure_ascii=False, indent=2))
            else:
                print(f"ì´ ë¬¸ì„œ: {total_docs:,}ê°œ")
                print(f"ì´ ì²­í¬: {total_chunks:,}ê°œ")
                print(f"ì„ë² ë”©ëœ ì²­í¬: {embedded_chunks:,}ê°œ ({result['embedding_coverage_percent']}%)")
                print(f"\në¬¸ì„œ ìœ í˜•ë³„:")
                for doc_type, count in doc_type_counts.items():
                    print(f"  {doc_type}: {count:,}ê°œ")
            
            return result
            
        except Exception as e:
            error_result = {'error': str(e)}
            if format == 'json':
                print(json.dumps(error_result, ensure_ascii=False))
            else:
                print(f"âŒ ì˜¤ë¥˜: {e}")
            return error_result
        finally:
            cur.close()
    
    def check_law_metadata(self):
        """ë²•ë ¹ ë©”íƒ€ë°ì´í„° í™•ì¸ (ê¸°ì¡´ check_law_metadata.py ê¸°ëŠ¥)"""
        cur = self.conn.cursor()
        
        print("=" * 80)
        print("ë²•ë ¹ ë°ì´í„° ë©”íƒ€ë°ì´í„° êµ¬ì¡° í™•ì¸")
        print("=" * 80)
        
        # 1. documents í…Œì´ë¸”ì˜ ë¯¼ë²• ë©”íƒ€ë°ì´í„°
        print("\nğŸ“š Documents í…Œì´ë¸” - ë¯¼ë²•:")
        cur.execute("""
            SELECT doc_id, title, metadata
            FROM documents
            WHERE doc_type = 'law' AND title ILIKE '%ë¯¼ë²•%'
            LIMIT 1
        """)
        
        row = cur.fetchone()
        if row:
            doc_id, title, metadata = row
            print(f"\n  doc_id: {doc_id}")
            print(f"  title: {title}")
            print(f"\n  metadata:")
            if metadata:
                print(json.dumps(metadata, indent=4, ensure_ascii=False))
            else:
                print("    (NULL)")
        
        # 2. chunks í…Œì´ë¸”ì˜ 750ì¡° ê´€ë ¨ ì²­í¬
        print("\n\nğŸ“„ Chunks í…Œì´ë¸” - ì œ750ì¡°:")
        cur.execute("""
            SELECT 
                c.chunk_id,
                c.chunk_type,
                c.content,
                d.metadata
            FROM chunks c
            JOIN documents d ON c.doc_id = d.doc_id
            WHERE 
                d.doc_type = 'law'
                AND d.title ILIKE '%ë¯¼ë²•%'
                AND c.content ILIKE '%750ì¡°%'
            LIMIT 3
        """)
        
        rows = cur.fetchall()
        for i, row in enumerate(rows, 1):
            chunk_id, chunk_type, content, doc_metadata = row
            print(f"\n{i}. chunk_id: {chunk_id}")
            print(f"   chunk_type: {chunk_type}")
            print(f"   content (ì²« 200ì):\n   {content[:200]}...")
            print(f"\n   document metadata:")
            if doc_metadata:
                print(json.dumps(doc_metadata, indent=4, ensure_ascii=False))
        
        # 3. chunk_id íŒ¨í„´ ë¶„ì„
        print("\n\nğŸ” Chunk ID íŒ¨í„´ ë¶„ì„:")
        cur.execute("""
            SELECT 
                chunk_id,
                chunk_type,
                LEFT(content, 100) as content_preview
            FROM chunks c
            JOIN documents d ON c.doc_id = d.doc_id
            WHERE d.doc_type = 'law' AND d.title ILIKE '%ë¯¼ë²•%'
            ORDER BY c.chunk_index
            LIMIT 10
        """)
        
        rows = cur.fetchall()
        for chunk_id, chunk_type, preview in rows:
            print(f"\n  {chunk_id}")
            print(f"    type: {chunk_type}")
            print(f"    preview: {preview}...")
        
        # 4. ì›ë³¸ JSONL íŒŒì¼ ìƒ˜í”Œ í™•ì¸
        print("\n\nğŸ“ ì›ë³¸ JSONL íŒŒì¼ ìƒ˜í”Œ:")
        jsonl_path = backend_dir / "data" / "law" / "Civil_Law_chunks.jsonl"
        
        try:
            if jsonl_path.exists():
                with open(jsonl_path, 'r', encoding='utf-8') as f:
                    # 750ì¡° ê´€ë ¨ ë¼ì¸ ì°¾ê¸°
                    for line in f:
                        data = json.loads(line)
                        if '750ì¡°' in data.get('index_text', '') or data.get('article_no') == 'ì œ750ì¡°':
                            print("\n  âœ… ì°¾ìŒ:")
                            print(json.dumps(data, indent=4, ensure_ascii=False))
                            break
            else:
                print(f"\n  âš ï¸  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {jsonl_path}")
        except Exception as e:
            print(f"\n  âš ï¸  íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        
        print("\n" + "=" * 80)
        print("âœ… í™•ì¸ ì™„ë£Œ")
        print("=" * 80)
        
        cur.close()
    
    def test_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ (ê¸°ì¡´ test_db_connection.py ê¸°ëŠ¥)"""
        print("=" * 80)
        print("Docker DB ì—°ê²° í…ŒìŠ¤íŠ¸")
        print("=" * 80)
        
        # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        print("\nğŸ“‹ í™˜ê²½ ë³€ìˆ˜ í™•ì¸:")
        print(f"  DB_HOST: {DB_CONFIG['host']}")
        print(f"  DB_PORT: {DB_CONFIG['port']}")
        print(f"  DB_NAME: {DB_CONFIG['database']}")
        print(f"  DB_USER: {DB_CONFIG['user']}")
        print(f"  DB_PASSWORD: {'*' * len(DB_CONFIG['password']) if DB_CONFIG['password'] else '(empty)'}")
        
        # ì—°ê²° í™•ì¸
        print("\nğŸ”Œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸...")
        if self.conn:
            print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ!")
            
            cur = self.conn.cursor()
            
            # pgvector í™•ì¥ í™•ì¸
            print("\nğŸ“¦ pgvector í™•ì¥ í™•ì¸...")
            cur.execute("SELECT * FROM pg_extension WHERE extname = 'vector';")
            if cur.fetchone():
                print("âœ… pgvector í™•ì¥ ì„¤ì¹˜ë¨")
            else:
                print("âš ï¸  pgvector í™•ì¥ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            print("\nğŸ“Š í…Œì´ë¸” ì¡´ì¬ í™•ì¸...")
            tables = ['documents', 'chunks']
            for table in tables:
                cur.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_schema = 'public' 
                        AND table_name = %s
                    );
                """, (table,))
                exists = cur.fetchone()[0]
                if exists:
                    print(f"  âœ… {table} í…Œì´ë¸” ì¡´ì¬")
                else:
                    print(f"  âŒ {table} í…Œì´ë¸” ì—†ìŒ")
            
            # documents í…Œì´ë¸” í†µê³„
            print("\nğŸ“ˆ Documents í…Œì´ë¸” í†µê³„:")
            cur.execute("""
                SELECT 
                    doc_type,
                    COUNT(*) as count
                FROM documents
                GROUP BY doc_type
                ORDER BY doc_type;
            """)
            rows = cur.fetchall()
            if rows:
                total = 0
                for doc_type, count in rows:
                    print(f"  {doc_type or '(NULL)'}: {count:,}ê°œ")
                    total += count
                print(f"  ì´ ë¬¸ì„œ ìˆ˜: {total:,}ê°œ")
            else:
                print("  âš ï¸  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # chunks í…Œì´ë¸” í†µê³„
            print("\nğŸ“ˆ Chunks í…Œì´ë¸” í†µê³„:")
            cur.execute("""
                SELECT 
                    d.doc_type,
                    COUNT(*) as chunk_count,
                    COUNT(CASE WHEN c.embedding IS NOT NULL THEN 1 END) as with_embedding
                FROM chunks c
                JOIN documents d ON c.doc_id = d.doc_id
                GROUP BY d.doc_type
                ORDER BY d.doc_type;
            """)
            rows = cur.fetchall()
            if rows:
                total_chunks = 0
                for doc_type, chunk_count, with_embedding in rows:
                    print(f"  {doc_type or '(NULL)'}: {chunk_count:,}ê°œ (ì„ë² ë”©: {with_embedding:,}ê°œ)")
                    total_chunks += chunk_count
                print(f"  ì´ ì²­í¬ ìˆ˜: {total_chunks:,}ê°œ")
            else:
                print("  âš ï¸  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ
            print("\nğŸ” ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ:")
            
            # ë²•ë ¹ ìƒ˜í”Œ
            cur.execute("""
                SELECT d.doc_id, d.doc_type, d.metadata->>'law_name' as law_name, c.content
                FROM documents d
                JOIN chunks c ON d.doc_id = c.doc_id
                WHERE d.doc_type = 'law'
                LIMIT 1;
            """)
            law_sample = cur.fetchone()
            if law_sample:
                print("  âœ… ë²•ë ¹ ìƒ˜í”Œ:")
                print(f"    - ë²•ë ¹ëª…: {law_sample[2]}")
                print(f"    - ë‚´ìš©: {law_sample[3][:100]}...")
            else:
                print("  âš ï¸  ë²•ë ¹ ë°ì´í„° ì—†ìŒ")
            
            # ê¸°ì¤€ ìƒ˜í”Œ
            cur.execute("""
                SELECT d.doc_id, d.doc_type, d.metadata->>'item' as item, c.content
                FROM documents d
                JOIN chunks c ON d.doc_id = c.doc_id
                WHERE d.doc_type LIKE 'criteria%%'
                LIMIT 1;
            """)
            criteria_sample = cur.fetchone()
            if criteria_sample:
                print("  âœ… ê¸°ì¤€ ìƒ˜í”Œ:")
                print(f"    - í’ˆëª©: {criteria_sample[2]}")
                print(f"    - ë‚´ìš©: {criteria_sample[3][:100]}...")
            else:
                print("  âš ï¸  ê¸°ì¤€ ë°ì´í„° ì—†ìŒ")
            
            # Full-Text Search í…ŒìŠ¤íŠ¸
            print("\nğŸ” Full-Text Search ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸...")
            test_query = "ë¯¼ë²•"
            cur.execute("""
                SELECT 
                    c.chunk_id,
                    c.content,
                    ts_rank_cd(
                        to_tsvector('simple', c.content),
                        plainto_tsquery('simple', %s)
                    ) AS rank
                FROM chunks c
                JOIN documents d ON c.doc_id = d.doc_id
                WHERE 
                    d.doc_type = 'law'
                    AND to_tsvector('simple', c.content) @@ plainto_tsquery('simple', %s)
                ORDER BY rank DESC
                LIMIT 1;
            """, (test_query, test_query))
            fts_result = cur.fetchone()
            if fts_result:
                print(f"  âœ… Full-Text Search ì •ìƒ ë™ì‘")
                print(f"    - ê²€ìƒ‰ì–´: '{test_query}'")
                print(f"    - ë§¤ì¹­ëœ ë‚´ìš©: {fts_result[1][:100]}...")
            else:
                print(f"  âš ï¸  '{test_query}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            
            cur.close()
            
            print("\n" + "=" * 80)
            print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
            print("=" * 80)
            return True
        else:
            print("âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
            return False
    
    def inspect_vectordb(self, export_samples=False, check_quality=False):
        """Vector DB ìƒì„¸ ê²€ì‚¬ (ê¸°ì¡´ inspect_vectordb.py ê¸°ëŠ¥)"""
        cur = self.conn.cursor(cursor_factory=RealDictCursor)
        
        try:
            # ì „ì²´ ê°œìš”
            print("=" * 80)
            print("ğŸ“Š Vector DB ê°œìš”")
            print("=" * 80)
            
            cur.execute("""
                SELECT 
                    COUNT(DISTINCT d.doc_id) as total_docs,
                    COUNT(c.chunk_id) as total_chunks,
                    COUNT(c.embedding) as embedded_chunks,
                    COUNT(CASE WHEN c.drop = TRUE THEN 1 END) as dropped_chunks,
                    AVG(c.content_length) as avg_chunk_length,
                    MIN(c.content_length) as min_chunk_length,
                    MAX(c.content_length) as max_chunk_length
                FROM documents d
                LEFT JOIN chunks c ON d.doc_id = c.doc_id
            """)
            stats = cur.fetchone()
            
            print(f"\nğŸ“„ ë¬¸ì„œ ë° ì²­í¬ í†µê³„:")
            print(f"  ì´ ë¬¸ì„œ:           {stats['total_docs']:,}ê°œ")
            print(f"  ì´ ì²­í¬:           {stats['total_chunks']:,}ê°œ")
            print(f"  ì„ë² ë”©ëœ ì²­í¬:     {stats['embedded_chunks']:,}ê°œ")
            print(f"  ì œì™¸ëœ ì²­í¬:       {stats['dropped_chunks']:,}ê°œ")
            
            if stats['total_chunks'] > 0:
                embed_rate = (stats['embedded_chunks'] / stats['total_chunks']) * 100
                print(f"  ì„ë² ë”© ì™„ë£Œìœ¨:     {embed_rate:.2f}%")
            
            print(f"\nğŸ“ ì²­í¬ ê¸¸ì´ í†µê³„:")
            print(f"  í‰ê· :             {stats['avg_chunk_length']:.0f}ì")
            print(f"  ìµœì†Œ:             {stats['min_chunk_length']:,}ì")
            print(f"  ìµœëŒ€:             {stats['max_chunk_length']:,}ì")
            
            # ë²¡í„° ì°¨ì› í™•ì¸
            try:
                cur.execute("""
                    SELECT embedding
                    FROM chunks
                    WHERE embedding IS NOT NULL
                    LIMIT 1
                """)
                result = cur.fetchone()
                if result:
                    embedding = result['embedding']
                    if hasattr(embedding, '__len__'):
                        dimension = len(embedding)
                    else:
                        dimension = 1024
                    
                    print(f"\nğŸ”¢ ë²¡í„° ì •ë³´:")
                    print(f"  ì°¨ì›:             {dimension}")
                    print(f"  ëª¨ë¸:             KURE-v1 (Korean Universal Representation)")
            except Exception as e:
                print(f"\nâš ï¸  ë²¡í„° ì°¨ì› í™•ì¸ ì‹¤íŒ¨: {e}")
            
            # ë°ì´í„° ë¶„í¬ í†µê³„
            print("\n" + "=" * 80)
            print("ğŸ“ˆ ë°ì´í„° ë¶„í¬ í†µê³„")
            print("=" * 80)
            
            # ë¬¸ì„œ ìœ í˜•ë³„
            print("\nğŸ“ ë¬¸ì„œ ìœ í˜•ë³„ ë¶„í¬:")
            cur.execute("""
                SELECT 
                    doc_type,
                    COUNT(DISTINCT d.doc_id) as doc_count,
                    COUNT(c.chunk_id) as chunk_count,
                    COUNT(c.embedding) as embedded_count
                FROM documents d
                LEFT JOIN chunks c ON d.doc_id = c.doc_id
                GROUP BY doc_type
                ORDER BY doc_count DESC
            """)
            
            print(f"{'ë¬¸ì„œ ìœ í˜•':<25} {'ë¬¸ì„œ ìˆ˜':>12} {'ì²­í¬ ìˆ˜':>12} {'ì„ë² ë”©':>12}")
            print("-" * 80)
            for row in cur.fetchall():
                print(f"{row['doc_type']:<25} {row['doc_count']:>12,} {row['chunk_count']:>12,} {row['embedded_count']:>12,}")
            
            # ì²­í¬ íƒ€ì…ë³„
            print("\nğŸ·ï¸  ì²­í¬ íƒ€ì…ë³„ ë¶„í¬:")
            cur.execute("""
                SELECT 
                    chunk_type,
                    COUNT(*) as count,
                    AVG(content_length) as avg_length,
                    COUNT(embedding) as embedded_count
                FROM chunks
                WHERE drop = FALSE
                GROUP BY chunk_type
                ORDER BY count DESC
            """)
            
            print(f"{'ì²­í¬ íƒ€ì…':<25} {'ê°œìˆ˜':>12} {'í‰ê·  ê¸¸ì´':>12} {'ì„ë² ë”©':>12}")
            print("-" * 80)
            for row in cur.fetchall():
                print(f"{row['chunk_type']:<25} {row['count']:>12,} {row['avg_length']:>11.0f}ì {row['embedded_count']:>12,}")
            
            # ì¶œì²˜ë³„
            print("\nğŸ¢ ì¶œì²˜ë³„ ë¶„í¬:")
            cur.execute("""
                SELECT 
                    source_org,
                    COUNT(DISTINCT d.doc_id) as doc_count,
                    COUNT(c.chunk_id) as chunk_count
                FROM documents d
                LEFT JOIN chunks c ON d.doc_id = c.doc_id
                GROUP BY source_org
                ORDER BY doc_count DESC
            """)
            
            print(f"{'ì¶œì²˜':<25} {'ë¬¸ì„œ ìˆ˜':>12} {'ì²­í¬ ìˆ˜':>12}")
            print("-" * 80)
            for row in cur.fetchall():
                source = row['source_org'] or '(null)'
                print(f"{source:<25} {row['doc_count']:>12,} {row['chunk_count']:>12,}")
            
            # ì €ì¥ì†Œ ì •ë³´
            print("\n" + "=" * 80)
            print("ğŸ’¾ ì €ì¥ì†Œ ì •ë³´")
            print("=" * 80)
            
            cur.execute("""
                SELECT 
                    pg_size_pretty(pg_total_relation_size('documents')) as documents_size,
                    pg_size_pretty(pg_total_relation_size('chunks')) as chunks_size,
                    pg_size_pretty(pg_database_size(current_database())) as total_db_size
            """)
            sizes = cur.fetchone()
            
            print(f"\nğŸ“Š í…Œì´ë¸” í¬ê¸°:")
            print(f"  documents:        {sizes['documents_size']}")
            print(f"  chunks:           {sizes['chunks_size']}")
            print(f"  ì „ì²´ DB:          {sizes['total_db_size']}")
            
            # ì¸ë±ìŠ¤ ì •ë³´
            cur.execute("""
                SELECT 
                    indexrelname as indexname,
                    pg_size_pretty(pg_relation_size(indexrelid)) as index_size
                FROM pg_stat_user_indexes
                WHERE schemaname = 'public'
                ORDER BY pg_relation_size(indexrelid) DESC
            """)
            
            print(f"\nğŸ” ì¸ë±ìŠ¤ í¬ê¸°:")
            for row in cur.fetchall():
                print(f"  {row['indexname']:<40} {row['index_size']}")
            
            # ì„ë² ë”© í’ˆì§ˆ ë¶„ì„
            if check_quality:
                print("\n" + "=" * 80)
                print("ğŸ” ì„ë² ë”© í’ˆì§ˆ ìƒì„¸ ë¶„ì„")
                print("=" * 80)
                
                cur.execute("""
                    SELECT 
                        chunk_id,
                        chunk_type,
                        content_length,
                        embedding
                    FROM chunks
                    WHERE embedding IS NOT NULL AND drop = FALSE
                    ORDER BY RANDOM()
                    LIMIT 500
                """)
                
                samples = cur.fetchall()
                
                if not samples:
                    print("âš ï¸  ì„ë² ë”©ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    print(f"\nğŸ“Š ë¶„ì„ ìƒ˜í”Œ: {len(samples)}ê°œ")
                    
                    quality_issues = defaultdict(list)
                    norm_values = []
                    variance_values = []
                    
                    for sample in samples:
                        embedding = sample['embedding']
                        
                        if isinstance(embedding, str):
                            embedding = embedding.strip('[]')
                            embedding = [float(x) for x in embedding.split(',')]
                        
                        vec = np.array(embedding, dtype=float)
                        
                        norm = np.linalg.norm(vec)
                        variance = np.var(vec)
                        
                        norm_values.append(norm)
                        variance_values.append(variance)
                        
                        if norm < 0.1:
                            quality_issues['low_norm'].append(sample['chunk_id'])
                        if variance < 0.001:
                            quality_issues['low_variance'].append(sample['chunk_id'])
                        if np.isnan(vec).any():
                            quality_issues['has_nan'].append(sample['chunk_id'])
                        if np.isinf(vec).any():
                            quality_issues['has_inf'].append(sample['chunk_id'])
                    
                    print(f"\nğŸ“ˆ ë²¡í„° í’ˆì§ˆ ì§€í‘œ:")
                    print(f"  Norm í‰ê· :        {np.mean(norm_values):.4f}")
                    print(f"  Norm í‘œì¤€í¸ì°¨:    {np.std(norm_values):.4f}")
                    print(f"  Norm ë²”ìœ„:        {np.min(norm_values):.4f} ~ {np.max(norm_values):.4f}")
                    print(f"  Variance í‰ê· :    {np.mean(variance_values):.6f}")
                    print(f"  Variance ë²”ìœ„:    {np.min(variance_values):.6f} ~ {np.max(variance_values):.6f}")
                    
                    print(f"\nâš ï¸  í’ˆì§ˆ ì´ìŠˆ:")
                    if not any(quality_issues.values()):
                        print("  âœ… í’ˆì§ˆ ì´ìŠˆ ì—†ìŒ!")
                    else:
                        for issue_type, chunk_ids in quality_issues.items():
                            count = len(chunk_ids)
                            rate = (count / len(samples)) * 100
                            print(f"  {issue_type}: {count}ê°œ ({rate:.2f}%)")
                            if chunk_ids[:3]:
                                print(f"    ìƒ˜í”Œ: {', '.join(chunk_ids[:3])}")
            
            # ìƒ˜í”Œ ë°ì´í„° ì¶”ì¶œ
            if export_samples:
                output_dir = Path("./vectordb_samples")
                output_dir.mkdir(exist_ok=True)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                print("\n" + "=" * 80)
                print("ğŸ“¦ ìƒ˜í”Œ ë°ì´í„° ì¶”ì¶œ")
                print("=" * 80)
                print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
                
                cur.execute("""
                    SELECT DISTINCT chunk_type
                    FROM chunks
                    WHERE embedding IS NOT NULL AND drop = FALSE
                """)
                chunk_types = [row['chunk_type'] for row in cur.fetchall()]
                
                samples = {}
                for chunk_type in chunk_types:
                    cur.execute("""
                        SELECT 
                            c.chunk_id,
                            c.chunk_type,
                            c.content,
                            c.content_length,
                            d.doc_type,
                            d.title,
                            d.source_org
                        FROM chunks c
                        JOIN documents d ON c.doc_id = d.doc_id
                        WHERE c.chunk_type = %s AND c.embedding IS NOT NULL AND c.drop = FALSE
                        ORDER BY RANDOM()
                        LIMIT 10
                    """, (chunk_type,))
                    
                    samples[chunk_type] = [dict(row) for row in cur.fetchall()]
                
                output_file = output_dir / f"vectordb_samples_{timestamp}.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(samples, f, ensure_ascii=False, indent=2)
                
                print(f"âœ… ìƒ˜í”Œ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_file}")
                print(f"   ì´ {len(chunk_types)}ê°œ ì²­í¬ íƒ€ì…, ê° 10ê°œì”© ì¶”ì¶œ")
            
            print("\n" + "=" * 80)
            print("âœ… ê²€ì‚¬ ì™„ë£Œ!")
            print("=" * 80)
            
        finally:
            cur.close()
    
    def close(self):
        """ì—°ê²° ì¢…ë£Œ"""
        if self.conn:
            self.conn.close()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='ë°ì´í„°ë² ì´ìŠ¤ í†µí•© ë„êµ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python db_tool.py --status              # DB ìƒíƒœ í™•ì¸
  python db_tool.py --stats               # í†µê³„ ìˆ˜ì§‘ (JSON)
  python db_tool.py --check-law           # ë²•ë ¹ ë©”íƒ€ë°ì´í„° í™•ì¸
  python db_tool.py --test-connection     # ì—°ê²° í…ŒìŠ¤íŠ¸
  python db_tool.py --inspect             # Vector DB ê²€ì‚¬
  python db_tool.py --inspect --check-quality  # Vector DB ê²€ì‚¬ + í’ˆì§ˆ ë¶„ì„
  python db_tool.py --inspect --export-samples # Vector DB ê²€ì‚¬ + ìƒ˜í”Œ ì¶”ì¶œ
  python db_tool.py --all                 # ëª¨ë“  ì²´í¬ ì‹¤í–‰
        """
    )
    
    parser.add_argument('--status', action='store_true',
                       help='DB ìƒíƒœ í™•ì¸')
    parser.add_argument('--stats', action='store_true',
                       help='í†µê³„ ìˆ˜ì§‘ ë° JSON ì¶œë ¥')
    parser.add_argument('--check-law', action='store_true',
                       help='ë²•ë ¹ ë©”íƒ€ë°ì´í„° í™•ì¸')
    parser.add_argument('--test-connection', action='store_true',
                       help='DB ì—°ê²° í…ŒìŠ¤íŠ¸')
    parser.add_argument('--inspect', action='store_true',
                       help='Vector DB ìƒì„¸ ê²€ì‚¬')
    parser.add_argument('--check-quality', action='store_true',
                       help='ì„ë² ë”© í’ˆì§ˆ ìƒì„¸ ë¶„ì„ (--inspectì™€ í•¨ê»˜ ì‚¬ìš©)')
    parser.add_argument('--export-samples', action='store_true',
                       help='ìƒ˜í”Œ ë°ì´í„° ì¶”ì¶œ (--inspectì™€ í•¨ê»˜ ì‚¬ìš©)')
    parser.add_argument('--all', action='store_true',
                       help='ëª¨ë“  ì²´í¬ ì‹¤í–‰')
    
    args = parser.parse_args()
    
    # ì•„ë¬´ ì˜µì…˜ë„ ì—†ìœ¼ë©´ ë„ì›€ë§ ì¶œë ¥
    if not any([args.status, args.stats, args.check_law, args.test_connection, 
                args.inspect, args.all]):
        parser.print_help()
        return
    
    tool = DatabaseTool()
    
    try:
        if args.all:
            # ëª¨ë“  ì²´í¬ ì‹¤í–‰
            tool.test_connection()
            print("\n")
            tool.check_status()
            print("\n")
            tool.get_stats(format='text')
            print("\n")
            tool.check_law_metadata()
            print("\n")
            tool.inspect_vectordb()
        else:
            if args.status:
                tool.check_status()
            if args.stats:
                tool.get_stats()
            if args.check_law:
                tool.check_law_metadata()
            if args.test_connection:
                tool.test_connection()
            if args.inspect:
                tool.inspect_vectordb(
                    export_samples=args.export_samples,
                    check_quality=args.check_quality
                )
    
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    finally:
        tool.close()


if __name__ == "__main__":
    main()
