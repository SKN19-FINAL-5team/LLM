#!/usr/bin/env python3
"""
í”¼í•´êµ¬ì œ ì‚¬ë¡€ ë°ì´í„° Golden Set ìƒì„± ìŠ¤í¬ë¦½íŠ¸

ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í”¼í•´êµ¬ì œ ì‚¬ë¡€ ë°ì´í„°ë¥¼ ìƒ˜í”Œë§í•˜ì—¬
ì¿¼ë¦¬ì™€ ê´€ë ¨ ì²­í¬ë¥¼ ì¶”ì¶œí•˜ì—¬ golden set JSON íŒŒì¼ ìƒì„±
"""

import os
import sys
import json
import psycopg2
from typing import List, Dict
from pathlib import Path
from dotenv import load_dotenv
import argparse
from datetime import datetime
import re

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, 'backend'))

from app.rag import VectorRetriever

load_dotenv()


class CompensationGoldenSetGenerator:
    """í”¼í•´êµ¬ì œ ì‚¬ë¡€ ë°ì´í„° Golden Set ìƒì„±ê¸°"""
    
    def __init__(self, db_config: Dict):
        self.db_config = db_config
        self.conn = None
        self.retriever = VectorRetriever(db_config)
    
    def connect_db(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        if self.conn is None or self.conn.closed:
            self.conn = psycopg2.connect(**self.db_config)
    
    def close_db(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ"""
        if self.conn:
            self.conn.close()
        self.retriever.close()
    
    def extract_query_from_chunk(self, content: str) -> str:
        """
        ì²­í¬ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ì¿¼ë¦¬ ìƒì„±
        
        Args:
            content: ì²­í¬ ë‚´ìš©
        
        Returns:
            ìƒì„±ëœ ì¿¼ë¦¬ ë¬¸ìì—´
        """
        query_parts = []
        content_clean = content.strip()
        
        # ì§ˆë¬¸/ë‹µë³€ í˜•ì‹ì—ì„œ ì§ˆë¬¸ ë¶€ë¶„ ì¶”ì¶œ ì‹œë„
        if '[ì§ˆë¬¸]' in content_clean or 'Q:' in content_clean:
            # ì§ˆë¬¸ ë¶€ë¶„ ì¶”ì¶œ
            q_match = re.search(r'\[ì§ˆë¬¸\](.*?)(?:\[ë‹µë³€\]|$)', content_clean, re.DOTALL)
            if not q_match:
                q_match = re.search(r'Q:\s*(.*?)(?:A:|$)', content_clean, re.DOTALL)
            if q_match:
                question = q_match.group(1).strip()
                # ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
                keywords = re.findall(r'[ê°€-í£]{2,}', question[:100])
                if keywords:
                    query_parts.extend(keywords[:5])
        
        # ë¬¸ì˜ ê´€ë ¨ í‚¤ì›Œë“œ
        inquiry_keywords = ['ë¬¸ì˜', 'ì§ˆë¬¸', 'ìƒë‹´', 'í™˜ë¶ˆ', 'êµí™˜', 'ìˆ˜ë¦¬', 'ë°°ìƒ', 
                           'ë³´ì¦', 'ê¸°ê°„', 'ê¸°ì¤€', 'ê³„ì‚°', 'ë°©ë²•']
        found_keywords = [k for k in inquiry_keywords if k in content_clean]
        if found_keywords:
            query_parts.extend(found_keywords[:3])
        
        # ì¼ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        if not query_parts:
            keywords = re.findall(r'[ê°€-í£]{2,4}', content_clean[:200])
            keywords = [k for k in keywords if len(k) >= 2][:5]
            query_parts.extend(keywords)
        
        query = ' '.join(query_parts) if query_parts else content_clean[:50]
        return query.strip()
    
    def find_related_chunks(self, query: str, source_chunk_id: str, top_k: int = 10) -> List[str]:
        """
        ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ì²­í¬ ì°¾ê¸° (ë²¡í„° ê²€ìƒ‰ ì‚¬ìš©)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            source_chunk_id: ì›ë³¸ ì²­í¬ ID (ì œì™¸)
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
        
        Returns:
            ê´€ë ¨ ì²­í¬ ID ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ì²­í¬ ì°¾ê¸°
            results = self.retriever.search(query=query, top_k=top_k * 2)
            
            # doc_typeì´ 'consumer_counsel_case' ë˜ëŠ” 'counsel_case'ì¸ ê²ƒë§Œ í•„í„°ë§ ë° source_chunk_id ì œì™¸
            related_chunk_ids = []
            for result in results:
                source = result.get('source', '')
                if source in ('consumer_counsel_case', 'counsel_case') and result.get('chunk_uid') != source_chunk_id:
                    related_chunk_ids.append(result.get('chunk_uid'))
                    if len(related_chunk_ids) >= top_k:
                        break
            
            return related_chunk_ids
        except Exception as e:
            print(f"  âš ï¸  ê´€ë ¨ ì²­í¬ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def generate_golden_set(self, num_samples: int = 20) -> List[Dict]:
        """
        Golden Set ìƒì„±
        
        Args:
            num_samples: ìƒì„±í•  ìƒ˜í”Œ ìˆ˜
        
        Returns:
            Golden set ë¦¬ìŠ¤íŠ¸
        """
        self.connect_db()
        
        print(f"\nğŸ’° í”¼í•´êµ¬ì œ ì‚¬ë¡€ ë°ì´í„° Golden Set ìƒì„±")
        print(f"ìƒ˜í”Œ ìˆ˜: {num_samples}ê°œ")
        print("=" * 80)
        
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í”¼í•´êµ¬ì œ ì‚¬ë¡€ ì²­í¬ ìƒ˜í”Œë§
        cur = self.conn.cursor()
        cur.execute("""
            SELECT 
                c.chunk_id,
                c.doc_id,
                c.content,
                c.chunk_type,
                d.title,
                d.metadata->>'case_sn' as case_sn,
                d.source_org as agency,
                d.doc_type
            FROM chunks c
            JOIN documents d ON c.doc_id = d.doc_id
            WHERE 
                d.doc_type IN ('consumer_counsel_case', 'counsel_case')
                AND c.drop = FALSE
                AND c.embedding IS NOT NULL
                AND c.content IS NOT NULL
                AND LENGTH(c.content) > 50
            ORDER BY RANDOM()
            LIMIT %s
        """, (num_samples,))
        
        samples = cur.fetchall()
        cur.close()
        
        if not samples:
            print("âŒ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        print(f"âœ… {len(samples)}ê°œ ìƒ˜í”Œ ì¶”ì¶œ ì™„ë£Œ")
        
        golden_set = []
        
        for idx, (chunk_id, doc_id, content, chunk_type, title, case_sn, agency, doc_type) in enumerate(samples, 1):
            print(f"\n[{idx}/{len(samples)}] ì²˜ë¦¬ ì¤‘: {chunk_id[:50]}...")
            
            # ì¿¼ë¦¬ ìƒì„±
            query = self.extract_query_from_chunk(content)
            print(f"  ìƒì„±ëœ ì¿¼ë¦¬: {query}")
            
            # ê´€ë ¨ ì²­í¬ ì°¾ê¸°
            related_chunk_ids = self.find_related_chunks(query, chunk_id, top_k=5)
            print(f"  ê´€ë ¨ ì²­í¬: {len(related_chunk_ids)}ê°œ")
            
            # ì›ë³¸ ì²­í¬ë„ í¬í•¨
            all_chunk_ids = [chunk_id] + related_chunk_ids
            all_doc_ids = [doc_id]
            
            # ê´€ë ¨ ë¬¸ì„œ ID ìˆ˜ì§‘
            for rel_chunk_id in related_chunk_ids:
                cur = self.conn.cursor()
                cur.execute("SELECT doc_id FROM chunks WHERE chunk_id = %s", (rel_chunk_id,))
                rel_doc = cur.fetchone()
                cur.close()
                if rel_doc and rel_doc[0] not in all_doc_ids:
                    all_doc_ids.append(rel_doc[0])
            
            golden_item = {
                "query": query,
                "expected_chunk_ids": all_chunk_ids[:10],  # ìµœëŒ€ 10ê°œ
                "expected_doc_ids": all_doc_ids[:5],  # ìµœëŒ€ 5ê°œ
                "metadata": {
                    "source_chunk_id": chunk_id,
                    "source_doc_id": doc_id,
                    "chunk_type": chunk_type,
                    "doc_type": doc_type,
                    "case_sn": case_sn,
                    "agency": agency,
                    "title": title,
                    "content_preview": content[:200] if content else None
                }
            }
            
            golden_set.append(golden_item)
        
        return golden_set
    
    def save_golden_set(self, golden_set: List[Dict], output_file: Path):
        """Golden Setì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
        output_data = {
            "metadata": {
                "data_type": "compensation",
                "doc_type": "consumer_counsel_case,counsel_case",
                "generated_at": datetime.now().isoformat(),
                "num_samples": len(golden_set)
            },
            "golden_set": golden_set
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… Golden Set ì €ì¥ ì™„ë£Œ: {output_file}")
        print(f"   ì´ {len(golden_set)}ê°œ ìƒ˜í”Œ")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='í”¼í•´êµ¬ì œ ì‚¬ë¡€ ë°ì´í„° Golden Set ìƒì„±')
    parser.add_argument('--num-samples', type=int, default=20,
                       help='ìƒì„±í•  ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸ê°’: 20)')
    parser.add_argument('--output', type=str, default='golden_set_compensation.json',
                       help='ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: golden_set_compensation.json)')
    args = parser.parse_args()
    
    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ
    db_config = {
        'host': os.getenv('DB_HOST', 'localhost'),
        'port': int(os.getenv('DB_PORT', 5432)),
        'database': os.getenv('DB_NAME', 'ddoksori'),
        'user': os.getenv('DB_USER', 'postgres'),
        'password': os.getenv('DB_PASSWORD', 'postgres')
    }
    
    # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    script_dir = Path(__file__).parent
    output_file = script_dir / args.output
    
    # Golden Set ìƒì„±
    generator = CompensationGoldenSetGenerator(db_config)
    
    try:
        golden_set = generator.generate_golden_set(num_samples=args.num_samples)
        
        if golden_set:
            generator.save_golden_set(golden_set, output_file)
        else:
            print("âŒ Golden Set ìƒì„± ì‹¤íŒ¨")
            sys.exit(1)
    
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    finally:
        generator.close_db()


if __name__ == "__main__":
    main()
