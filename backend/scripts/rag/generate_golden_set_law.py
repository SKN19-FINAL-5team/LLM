#!/usr/bin/env python3
"""
ë²•ë ¹ ë°ì´í„° Golden Set ìƒì„± ìŠ¤í¬ë¦½íŠ¸

ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë²•ë ¹ ë°ì´í„°ë¥¼ ìƒ˜í”Œë§í•˜ì—¬
ì¿¼ë¦¬ì™€ ê´€ë ¨ ì²­í¬ë¥¼ ì¶”ì¶œí•˜ì—¬ golden set JSON íŒŒì¼ ìƒì„±
"""

import os
import sys
import json
import psycopg2
from typing import List, Dict
from pathlib import Path
from dotenv import load_dotenv
import argparse
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, 'backend'))

from app.rag import VectorRetriever

load_dotenv()


class LawGoldenSetGenerator:
    """ë²•ë ¹ ë°ì´í„° Golden Set ìƒì„±ê¸°"""
    
    def __init__(self, db_config: Dict):
        self.db_config = db_config
        self.conn = None
        self.retriever = VectorRetriever(db_config)
    
    def connect_db(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        if self.conn is None or self.conn.closed:
            self.conn = psycopg2.connect(**self.db_config)
    
    def close_db(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ"""
        if self.conn:
            self.conn.close()
        self.retriever.close()
    
    def extract_query_from_chunk(self, content: str, law_name: str = None, article_no: str = None) -> str:
        """
        ì²­í¬ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ì¿¼ë¦¬ ìƒì„±
        
        Args:
            content: ì²­í¬ ë‚´ìš©
            law_name: ë²•ë ¹ëª… (ì„ íƒ)
            article_no: ì¡°ë¬¸ë²ˆí˜¸ (ì„ íƒ)
        
        Returns:
            ìƒì„±ëœ ì¿¼ë¦¬ ë¬¸ìì—´
        """
        # ë‚´ìš©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        content_clean = content.strip()
        
        # ë²•ë ¹ëª…ê³¼ ì¡°ë¬¸ë²ˆí˜¸ê°€ ìˆìœ¼ë©´ í¬í•¨
        query_parts = []
        if law_name:
            query_parts.append(law_name)
        if article_no:
            query_parts.append(article_no)
        
        # ë‚´ìš©ì˜ ì•ë¶€ë¶„ì„ ìš”ì•½í•˜ì—¬ ì¿¼ë¦¬ ìƒì„±
        # ì²« ë¬¸ì¥ì´ë‚˜ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œ
        sentences = content_clean.split('ã€‚')[:2]  # ì²« ë‘ ë¬¸ì¥
        if not sentences:
            sentences = content_clean.split('.')[:2]
        
        # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (2-4ê¸€ì í•œê¸€ ë‹¨ì–´)
        import re
        keywords = re.findall(r'[ê°€-í£]{2,4}', content_clean[:200])
        keywords = [k for k in keywords if len(k) >= 2][:5]  # ìƒìœ„ 5ê°œ
        
        # ì¿¼ë¦¬ ìƒì„±
        if query_parts:
            query = ' '.join(query_parts)
            if keywords:
                query += ' ' + ' '.join(keywords[:3])
        else:
            query = ' '.join(keywords[:5]) if keywords else content_clean[:50]
        
        return query.strip()
    
    def find_related_chunks(self, query: str, source_chunk_id: str, top_k: int = 10) -> List[str]:
        """
        ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ì²­í¬ ì°¾ê¸° (ë²¡í„° ê²€ìƒ‰ ì‚¬ìš©)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            source_chunk_id: ì›ë³¸ ì²­í¬ ID (ì œì™¸)
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
        
        Returns:
            ê´€ë ¨ ì²­í¬ ID ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ì²­í¬ ì°¾ê¸°
            results = self.retriever.search(query=query, top_k=top_k * 2)
            
            # doc_type='law' í•„í„°ë§ ë° source_chunk_id ì œì™¸
            related_chunk_ids = []
            for result in results:
                if result.get('source') == 'law' and result.get('chunk_uid') != source_chunk_id:
                    related_chunk_ids.append(result.get('chunk_uid'))
                    if len(related_chunk_ids) >= top_k:
                        break
            
            return related_chunk_ids
        except Exception as e:
            print(f"  âš ï¸  ê´€ë ¨ ì²­í¬ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def generate_golden_set(self, num_samples: int = 20) -> List[Dict]:
        """
        Golden Set ìƒì„±
        
        Args:
            num_samples: ìƒì„±í•  ìƒ˜í”Œ ìˆ˜
        
        Returns:
            Golden set ë¦¬ìŠ¤íŠ¸
        """
        self.connect_db()
        
        print(f"\nğŸ“š ë²•ë ¹ ë°ì´í„° Golden Set ìƒì„±")
        print(f"ìƒ˜í”Œ ìˆ˜: {num_samples}ê°œ")
        print("=" * 80)
        
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë²•ë ¹ ì²­í¬ ìƒ˜í”Œë§
        cur = self.conn.cursor()
        cur.execute("""
            SELECT 
                c.chunk_id,
                c.doc_id,
                c.content,
                c.chunk_type,
                d.title,
                d.metadata->>'law_name' as law_name,
                d.metadata->>'article_no' as article_no
            FROM chunks c
            JOIN documents d ON c.doc_id = d.doc_id
            WHERE 
                d.doc_type = 'law'
                AND c.drop = FALSE
                AND c.embedding IS NOT NULL
                AND c.content IS NOT NULL
                AND LENGTH(c.content) > 50
            ORDER BY RANDOM()
            LIMIT %s
        """, (num_samples,))
        
        samples = cur.fetchall()
        cur.close()
        
        if not samples:
            print("âŒ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        print(f"âœ… {len(samples)}ê°œ ìƒ˜í”Œ ì¶”ì¶œ ì™„ë£Œ")
        
        golden_set = []
        
        for idx, (chunk_id, doc_id, content, chunk_type, title, law_name, article_no) in enumerate(samples, 1):
            print(f"\n[{idx}/{len(samples)}] ì²˜ë¦¬ ì¤‘: {chunk_id[:50]}...")
            
            # ì¿¼ë¦¬ ìƒì„±
            query = self.extract_query_from_chunk(content, law_name, article_no)
            print(f"  ìƒì„±ëœ ì¿¼ë¦¬: {query}")
            
            # ê´€ë ¨ ì²­í¬ ì°¾ê¸°
            related_chunk_ids = self.find_related_chunks(query, chunk_id, top_k=5)
            print(f"  ê´€ë ¨ ì²­í¬: {len(related_chunk_ids)}ê°œ")
            
            # ì›ë³¸ ì²­í¬ë„ í¬í•¨
            all_chunk_ids = [chunk_id] + related_chunk_ids
            all_doc_ids = [doc_id]
            
            # ê´€ë ¨ ë¬¸ì„œ ID ìˆ˜ì§‘
            for rel_chunk_id in related_chunk_ids:
                cur = self.conn.cursor()
                cur.execute("SELECT doc_id FROM chunks WHERE chunk_id = %s", (rel_chunk_id,))
                rel_doc = cur.fetchone()
                cur.close()
                if rel_doc and rel_doc[0] not in all_doc_ids:
                    all_doc_ids.append(rel_doc[0])
            
            golden_item = {
                "query": query,
                "expected_chunk_ids": all_chunk_ids[:10],  # ìµœëŒ€ 10ê°œ
                "expected_doc_ids": all_doc_ids[:5],  # ìµœëŒ€ 5ê°œ
                "metadata": {
                    "source_chunk_id": chunk_id,
                    "source_doc_id": doc_id,
                    "chunk_type": chunk_type,
                    "law_name": law_name,
                    "article_no": article_no,
                    "title": title,
                    "content_preview": content[:200] if content else None
                }
            }
            
            golden_set.append(golden_item)
        
        return golden_set
    
    def save_golden_set(self, golden_set: List[Dict], output_file: Path):
        """Golden Setì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
        output_data = {
            "metadata": {
                "data_type": "law",
                "doc_type": "law",
                "generated_at": datetime.now().isoformat(),
                "num_samples": len(golden_set)
            },
            "golden_set": golden_set
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… Golden Set ì €ì¥ ì™„ë£Œ: {output_file}")
        print(f"   ì´ {len(golden_set)}ê°œ ìƒ˜í”Œ")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ë²•ë ¹ ë°ì´í„° Golden Set ìƒì„±')
    parser.add_argument('--num-samples', type=int, default=20,
                       help='ìƒì„±í•  ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸ê°’: 20)')
    parser.add_argument('--output', type=str, default='golden_set_law.json',
                       help='ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: golden_set_law.json)')
    args = parser.parse_args()
    
    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ
    db_config = {
        'host': os.getenv('DB_HOST', 'localhost'),
        'port': int(os.getenv('DB_PORT', 5432)),
        'database': os.getenv('DB_NAME', 'ddoksori'),
        'user': os.getenv('DB_USER', 'postgres'),
        'password': os.getenv('DB_PASSWORD', 'postgres')
    }
    
    # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    script_dir = Path(__file__).parent
    output_file = script_dir / args.output
    
    # Golden Set ìƒì„±
    generator = LawGoldenSetGenerator(db_config)
    
    try:
        golden_set = generator.generate_golden_set(num_samples=args.num_samples)
        
        if golden_set:
            generator.save_golden_set(golden_set, output_file)
        else:
            print("âŒ Golden Set ìƒì„± ì‹¤íŒ¨")
            sys.exit(1)
    
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    finally:
        generator.close_db()


if __name__ == "__main__":
    main()
