#!/usr/bin/env python3
"""
ë°ì´í„° ì„ë² ë”© íŒŒì´í”„ë¼ì¸ (ì›ê²© API ë²„ì „)

ë³€í™˜ëœ JSON ë°ì´í„°ë¥¼ PostgreSQL + pgvectorì— ì €ì¥í•˜ê³ 
RunPod GPUë¥¼ í†µí•´ ì„ë² ë”© ìƒì„±

Features:
- ë³€í™˜ëœ JSON ë°ì´í„° ì½ê¸°
- documents, chunks í…Œì´ë¸”ì— ì‚½ì…
- drop=True ì²­í¬ ìë™ ì œì™¸
- ë°°ì¹˜ ì„ë² ë”© ìƒì„± (ë¹ˆ content ìë™ í•„í„°ë§)
- ì§„í–‰ ìƒí™© ì €ì¥ (ì¤‘ë‹¨ ì‹œ ì¬ê°œ ê°€ëŠ¥)
"""

import os
import json
import psycopg2
from psycopg2.extras import execute_values, RealDictCursor
from tqdm import tqdm
from typing import List, Dict, Tuple
import requests
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv
import numpy as np

load_dotenv()

# ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
SCRIPT_DIR = Path(__file__).resolve().parent  # scripts/embedding/
PROJECT_ROOT = SCRIPT_DIR.parent.parent.parent  # ddoksori_demo/
DATA_DIR = PROJECT_ROOT / "backend" / "data"


class EmbeddingPipeline:
    """ì„ë² ë”© íŒŒì´í”„ë¼ì¸ (ê°œì„ ë¨ - í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì¶”ê°€)"""
    
    def __init__(self, db_config: Dict[str, str], embed_api_url: str, load_only: bool = False):
        """
        Args:
            db_config: PostgreSQL ì—°ê²° ì •ë³´
            embed_api_url: ì›ê²© ì„ë² ë”© API URL
            load_only: Trueì´ë©´ ë°ì´í„°ë§Œ ë¡œë“œí•˜ê³  ì„ë² ë”©ì€ ìƒì„±í•˜ì§€ ì•ŠìŒ
        """
        self.db_config = db_config
        self.embed_api_url = embed_api_url
        self.load_only = load_only
        self.conn = None
        self.batch_size = 32  # ì„ë² ë”© ë°°ì¹˜ í¬ê¸°
        
        # í†µê³„
        self.stats = {
            'documents': 0,
            'chunks': 0,
            'chunks_skipped': 0,  # drop=True
            'chunks_embedded': 0,
            'chunks_empty': 0,  # ë¹ˆ content
            'chunks_preprocessed': 0,  # ì „ì²˜ë¦¬ëœ ì²­í¬ (ì‹ ê·œ)
            'low_quality_texts': 0,  # ì €í’ˆì§ˆ í…ìŠ¤íŠ¸ (ì‚¬ì „ í•„í„°ë§, ì‹ ê·œ)
            'low_quality_embeddings': 0,  # ì €í’ˆì§ˆ ì„ë² ë”©
            'quality_warnings': [],  # í’ˆì§ˆ ê²½ê³  ëª©ë¡
            'errors': []
        }
        
        # API ì—°ê²° í…ŒìŠ¤íŠ¸ (load_only ëª¨ë“œì—ì„œëŠ” ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰)
        self.api_available = self._test_api_connection(skip_if_failed=load_only)
    
    def _test_api_connection(self, skip_if_failed=False):
        """ì„ë² ë”© API ì—°ê²° í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ”Œ ì„ë² ë”© API ì—°ê²° í…ŒìŠ¤íŠ¸: {self.embed_api_url}")
        try:
            base_url = self.embed_api_url.rsplit('/', 1)[0]
            response = requests.get(base_url, timeout=10)
            response.raise_for_status()
            print(f"âœ… API ì—°ê²° ì„±ê³µ: {response.json()}")
            return True
        except requests.exceptions.RequestException as e:
            if skip_if_failed:
                print(f"âš ï¸  API ì—°ê²° ì‹¤íŒ¨ (ë°ì´í„°ë§Œ ë¡œë“œ ëª¨ë“œ): {e}")
                print("   ë°ì´í„°ë§Œ ë¡œë“œí•˜ê³  ì„ë² ë”©ì€ ë‚˜ì¤‘ì— ìƒì„±í•˜ì„¸ìš”.")
                return False
            print(f"âŒ API ì—°ê²° ì‹¤íŒ¨: {e}")
            print("\në‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:")
            print("1. SSH í„°ë„: ssh -L 8001:localhost:8000 [user]@[host] -p [port]")
            print("2. RunPod ì„œë²„: uvicorn runpod_embed_server:app --host 0.0.0.0 --port 8000")
            raise
    
    def preprocess_text(self, text: str) -> str:
        """
        ì„ë² ë”© ì „ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ì‹ ê·œ)
        
        ì „ì²˜ë¦¬ í•­ëª©:
        1. ê³¼ë„í•œ ê³µë°± ì •ë¦¬
        2. ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì •ë¦¬ (3ê°œ ì´ìƒ â†’ 2ê°œ)
        3. íŠ¹ìˆ˜ë¬¸ì ì •ê·œí™”
        4. ì•ë’¤ ê³µë°± ì œê±°
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            
        Returns:
            ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸
        """
        if not text:
            return text
        
        import re
        
        # 1. ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r' +', ' ', text)
        
        # 2. ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì •ë¦¬ (3ê°œ ì´ìƒ â†’ 2ê°œ)
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # 3. íƒ­ì„ ê³µë°±ìœ¼ë¡œ
        text = text.replace('\t', ' ')
        
        # 4. íŠ¹ìˆ˜ ìœ ë‹ˆì½”ë“œ ê³µë°± ì •ê·œí™”
        text = text.replace('\u3000', ' ')  # ì „ê° ê³µë°±
        text = text.replace('\xa0', ' ')  # Non-breaking space
        
        # 5. ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        return text
    
    def validate_text_quality(self, text: str) -> tuple[bool, str]:
        """
        í…ìŠ¤íŠ¸ í’ˆì§ˆ ì‚¬ì „ ê²€ì¦ (ì‹ ê·œ)
        
        ì„ë² ë”© ìƒì„± ì „ì— í…ìŠ¤íŠ¸ í’ˆì§ˆì„ ê²€ì‚¬í•˜ì—¬ 
        ì €í’ˆì§ˆ í…ìŠ¤íŠ¸ëŠ” ì¡°ê¸°ì— í•„í„°ë§
        
        ê²€ì¦ í•­ëª©:
        1. ìµœì†Œ ê¸¸ì´ (20ì ì´ìƒ)
        2. ì˜ë¯¸ ìˆëŠ” ë¬¸ì ë¹„ìœ¨ (30% ì´ìƒ)
        3. ë°˜ë³µ ë¬¸ì ê³¼ë‹¤ (ê°™ì€ ë¬¸ì 80% ì´ìƒ ë°˜ë³µ ê¸ˆì§€)
        4. íŠ¹ì • íŒ¨í„´ (URLë§Œ, ìˆ«ìë§Œ ë“±)
        
        Args:
            text: ê²€ì¦í•  í…ìŠ¤íŠ¸
            
        Returns:
            (is_valid, reason): ìœ íš¨ ì—¬ë¶€ì™€ ì´ìœ 
        """
        if not text or not text.strip():
            return False, "ë¹ˆ í…ìŠ¤íŠ¸"
        
        text = text.strip()
        
        # 1. ìµœì†Œ ê¸¸ì´ ì²´í¬
        if len(text) < 20:
            return False, f"ë„ˆë¬´ ì§§ìŒ ({len(text)}ì)"
        
        # 2. ì˜ë¯¸ ìˆëŠ” ë¬¸ì ë¹„ìœ¨
        import re
        meaningful_chars = re.findall(r'[ê°€-í£a-zA-Z0-9]', text)
        if len(meaningful_chars) / len(text) < 0.3:
            return False, f"ì˜ë¯¸ ìˆëŠ” ë¬¸ì ë¶€ì¡± ({len(meaningful_chars)}/{len(text)})"
        
        # 3. ë°˜ë³µ ë¬¸ì ê³¼ë‹¤ ì²´í¬
        from collections import Counter
        char_counts = Counter(text)
        most_common_char, most_common_count = char_counts.most_common(1)[0]
        if most_common_count / len(text) > 0.8:
            return False, f"ë°˜ë³µ ë¬¸ì ê³¼ë‹¤ ('{most_common_char}' {most_common_count}íšŒ)"
        
        # 4. URLë§Œìœ¼ë¡œ êµ¬ì„±ë˜ì—ˆëŠ”ì§€
        urls = re.findall(r'https?://[^\s]+', text)
        url_length = sum(len(url) for url in urls)
        if url_length / len(text) > 0.8:
            return False, "URLë§Œìœ¼ë¡œ êµ¬ì„±ë¨"
        
        # 5. ìˆ«ìë§Œìœ¼ë¡œ êµ¬ì„±ë˜ì—ˆëŠ”ì§€
        digits = re.findall(r'\d', text)
        if len(digits) / len(text) > 0.9:
            return False, "ìˆ«ìë§Œìœ¼ë¡œ êµ¬ì„±ë¨"
        
        return True, ""
    
    def connect_db(self):
        """PostgreSQL ì—°ê²°"""
        if self.conn:
            return
        
        print("\nğŸ”Œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°...")
        self.conn = psycopg2.connect(**self.db_config)
        
        # pgvector íƒ€ì… ë“±ë¡ (vectorë¥¼ ìë™ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜)
        try:
            from pgvector.psycopg2 import register_vector
            register_vector(self.conn)
        except ImportError:
            print("  âš ï¸  pgvector íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. vector íƒ€ì…ì„ ìˆ˜ë™ìœ¼ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.")
        
        print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
    
    def close_db(self):
        """DB ì—°ê²° ì¢…ë£Œ"""
        if self.conn:
            self.conn.close()
            self.conn = None
    
    def load_json_file(self, file_path: Path) -> Dict:
        """JSON íŒŒì¼ ë¡œë“œ"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def insert_documents(self, documents: List[Dict]):
        """documents í…Œì´ë¸”ì— ë¬¸ì„œ ì‚½ì…"""
        if not documents:
            return
        
        print(f"\nğŸ“„ ë¬¸ì„œ ì‚½ì…: {len(documents)}ê°œ")
        
        with self.conn.cursor() as cur:
            insert_query = """
                INSERT INTO documents (
                    doc_id, doc_type, title, source_org, 
                    category_path, url, metadata
                )
                VALUES %s
                ON CONFLICT (doc_id) DO UPDATE SET
                    title = EXCLUDED.title,
                    metadata = EXCLUDED.metadata
            """
            
            values = [
                (
                    doc['doc_id'],
                    doc['doc_type'],
                    doc['title'],
                    doc.get('source_org'),
                    doc.get('category_path'),
                    doc.get('url'),
                    json.dumps(doc.get('metadata', {}))
                )
                for doc in documents
            ]
            
            execute_values(cur, insert_query, values)
            self.conn.commit()
            self.stats['documents'] += len(documents)
            print(f"âœ… {len(documents)}ê°œ ë¬¸ì„œ ì‚½ì… ì™„ë£Œ")
    
    def insert_chunks(self, doc_id: str, chunks: List[Dict]) -> List[Tuple[str, str]]:
        """
        chunks í…Œì´ë¸”ì— ì²­í¬ ì‚½ì… (ê°œì„ ë¨ - ì „ì²˜ë¦¬ ë° í’ˆì§ˆ ê²€ì¦ ì¶”ê°€)
        
        Returns:
            List[(chunk_id, content)]: ì„ë² ë”© ìƒì„±ì´ í•„ìš”í•œ ì²­í¬ ëª©ë¡
        """
        if not chunks:
            return []
        
        # drop=True ì²­í¬ í•„í„°ë§
        valid_chunks = [c for c in chunks if not c.get('drop', False)]
        skipped = len(chunks) - len(valid_chunks)
        
        if skipped > 0:
            self.stats['chunks_skipped'] += skipped
        
        if not valid_chunks:
            return []
        
        with self.conn.cursor() as cur:
            insert_query = """
                INSERT INTO chunks (
                    chunk_id, doc_id, chunk_index, chunk_total,
                    chunk_type, content, content_length, drop
                )
                VALUES %s
                ON CONFLICT (chunk_id) DO UPDATE SET
                    content = EXCLUDED.content
            """
            
            values = [
                (
                    chunk['chunk_id'],
                    doc_id,
                    chunk['chunk_index'],
                    chunk['chunk_total'],
                    chunk['chunk_type'],
                    chunk['content'],
                    chunk['content_length'],
                    chunk.get('drop', False)
                )
                for chunk in valid_chunks
            ]
            
            execute_values(cur, insert_query, values)
            self.conn.commit()
            self.stats['chunks'] += len(valid_chunks)
        
        # ì„ë² ë”© ìƒì„±ì´ í•„ìš”í•œ ì²­í¬ ì¤€ë¹„ (ì „ì²˜ë¦¬ ë° í’ˆì§ˆ ê²€ì¦ ì ìš©)
        chunks_to_embed = []
        
        for chunk in valid_chunks:
            content = chunk['content']
            
            # ë¹ˆ content ì²´í¬
            if not content or not content.strip():
                self.stats['chunks_empty'] += 1
                continue
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ì‹ ê·œ)
            preprocessed_content = self.preprocess_text(content)
            self.stats['chunks_preprocessed'] += 1
            
            # í…ìŠ¤íŠ¸ í’ˆì§ˆ ì‚¬ì „ ê²€ì¦ (ì‹ ê·œ)
            is_valid, reason = self.validate_text_quality(preprocessed_content)
            
            if not is_valid:
                # ì €í’ˆì§ˆ í…ìŠ¤íŠ¸ëŠ” ì„ë² ë”© ìƒì„±í•˜ì§€ ì•ŠìŒ
                self.stats['low_quality_texts'] += 1
                warning_msg = (
                    f"ì €í’ˆì§ˆ í…ìŠ¤íŠ¸ í•„í„°ë§: {chunk['chunk_id']}\n"
                    f"  ì´ìœ : {reason}\n"
                    f"  ì›ë³¸ ê¸¸ì´: {len(content)}ì\n"
                    f"  ë¯¸ë¦¬ë³´ê¸°: {content[:100]}..."
                )
                self.stats['quality_warnings'].append(warning_msg)
                
                # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
                if self.stats['low_quality_texts'] <= 5:
                    print(f"\nâš ï¸  {warning_msg}")
                
                continue
            
            chunks_to_embed.append((chunk['chunk_id'], preprocessed_content))
        
        return chunks_to_embed
    
    def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        ì›ê²© APIë¥¼ í†µí•´ ì„ë² ë”© ìƒì„±
        
        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        try:
            response = requests.post(
                self.embed_api_url,
                json={"texts": texts},
                timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
            )
            response.raise_for_status()
            return response.json()['embeddings']
        except requests.exceptions.RequestException as e:
            print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def is_low_quality_embedding(self, embedding: List[float]) -> Tuple[bool, str]:
        """
        ì €í’ˆì§ˆ ì„ë² ë”© ê°ì§€
        
        Args:
            embedding: ì„ë² ë”© ë²¡í„° (ë¦¬ìŠ¤íŠ¸, numpy ë°°ì—´, ë˜ëŠ” ë¬¸ìì—´)
            
        Returns:
            (is_low_quality, reason): ì €í’ˆì§ˆ ì—¬ë¶€ì™€ ì´ìœ 
        """
        # ë²¡í„° ë³€í™˜ (ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¬¸ìì—´ë¡œ ì˜¬ ìˆ˜ ìˆìŒ)
        if isinstance(embedding, str):
            # PostgreSQL vector íƒ€ì…ì´ ë¬¸ìì—´ë¡œ ë°˜í™˜ë˜ëŠ” ê²½ìš° íŒŒì‹±
            # í˜•ì‹: "[0.1,0.2,0.3]" ë˜ëŠ” "np.str_('[0.1,0.2,0.3]')"
            embedding_str = embedding.strip()
            
            # numpy string wrapper ì œê±°
            if embedding_str.startswith('np.str_('):
                embedding_str = embedding_str[8:-1]  # "np.str_(" ì™€ ")" ì œê±°
            
            # ì‘ì€ë”°ì˜´í‘œ ì œê±°
            embedding_str = embedding_str.strip("'\"")
            
            # ëŒ€ê´„í˜¸ ì œê±°í•˜ê³  ì‰¼í‘œë¡œ ë¶„ë¦¬
            embedding_str = embedding_str.strip('[]')
            values = [x.strip() for x in embedding_str.split(',') if x.strip()]
            
            try:
                embedding = [float(x) for x in values]
            except ValueError as e:
                return True, f"ë²¡í„° íŒŒì‹± ì‹¤íŒ¨: {str(e)[:50]}"
        
        try:
            vec = np.array(embedding, dtype=float)
        except Exception as e:
            return True, f"numpy ë°°ì—´ ë³€í™˜ ì‹¤íŒ¨: {str(e)[:50]}"
        
        # ì²´í¬ 1: Normì´ ë„ˆë¬´ ì‘ìŒ (ì˜ë¯¸ ì—†ëŠ” ë²¡í„°)
        norm = np.linalg.norm(vec)
        if norm < 0.1:
            return True, f"normì´ ë„ˆë¬´ ì‘ìŒ ({norm:.4f})"
        
        # ì²´í¬ 2: ë¶„ì‚°ì´ ë„ˆë¬´ ì‘ìŒ (ëª¨ë“  ê°’ì´ ìœ ì‚¬)
        variance = np.var(vec)
        if variance < 0.001:
            return True, f"ë¶„ì‚°ì´ ë„ˆë¬´ ì‘ìŒ ({variance:.6f})"
        
        # ì²´í¬ 3: NaNì´ë‚˜ Inf ê°’ ì¡´ì¬
        if np.isnan(vec).any() or np.isinf(vec).any():
            return True, "NaN ë˜ëŠ” Inf ê°’ í¬í•¨"
        
        # ì²´í¬ 4: ë²¡í„°ê°€ ë„ˆë¬´ í¬ì†Œí•¨ (ëŒ€ë¶€ë¶„ì˜ ê°’ì´ 0ì— ê°€ê¹Œì›€)
        near_zero = np.abs(vec) < 0.001
        if near_zero.sum() / len(vec) > 0.9:
            return True, f"í¬ì†Œ ë²¡í„° ({near_zero.sum()}/{len(vec)} ê°’ì´ ~0)"
        
        return False, ""
    
    def embed_chunks(self, chunks_to_embed: List[Tuple[str, str]]):
        """ì²­í¬ì— ëŒ€í•œ ì„ë² ë”© ìƒì„± ë° í’ˆì§ˆ ì²´í¬"""
        if not chunks_to_embed:
            return
        
        print(f"\nğŸ”® ì„ë² ë”© ìƒì„±: {len(chunks_to_embed)}ê°œ ì²­í¬")
        
        # ë°°ì¹˜ ì²˜ë¦¬
        with self.conn.cursor() as cur:
            for i in tqdm(range(0, len(chunks_to_embed), self.batch_size)):
                batch = chunks_to_embed[i:i + self.batch_size]
                chunk_ids = [c[0] for c in batch]
                texts = [c[1] for c in batch]
                
                try:
                    # ì„ë² ë”© ìƒì„±
                    embeddings = self.generate_embeddings(texts)
                    
                    # DB ì—…ë°ì´íŠ¸ ë° í’ˆì§ˆ ì²´í¬
                    update_query = """
                        UPDATE chunks
                        SET embedding = %s::vector
                        WHERE chunk_id = %s
                    """
                    
                    for chunk_id, embedding, text in zip(chunk_ids, embeddings, texts):
                        # í’ˆì§ˆ ì²´í¬
                        is_low_quality, reason = self.is_low_quality_embedding(embedding)
                        
                        if is_low_quality:
                            self.stats['low_quality_embeddings'] += 1
                            warning_msg = (
                                f"ì €í’ˆì§ˆ ì„ë² ë”© ê°ì§€: {chunk_id}\n"
                                f"  ì´ìœ : {reason}\n"
                                f"  í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}ì\n"
                                f"  í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {text[:100]}..."
                            )
                            self.stats['quality_warnings'].append(warning_msg)
                            
                            # ìƒì„¸ ê²½ê³ ëŠ” ì²« 5ê°œë§Œ ì¶œë ¥
                            if self.stats['low_quality_embeddings'] <= 5:
                                print(f"\nâš ï¸  {warning_msg}")
                        
                        # ì„ë² ë”© ì €ì¥ (ì €í’ˆì§ˆì´ì–´ë„ ì¼ë‹¨ ì €ì¥)
                        cur.execute(update_query, (embedding, chunk_id))
                    
                    self.conn.commit()
                    self.stats['chunks_embedded'] += len(batch)
                    
                except Exception as e:
                    error_msg = f"ë°°ì¹˜ {i//self.batch_size + 1} ì„ë² ë”© ì‹¤íŒ¨: {e}"
                    print(f"âŒ {error_msg}")
                    self.stats['errors'].append(error_msg)
                    self.conn.rollback()
        
        print(f"âœ… {self.stats['chunks_embedded']}ê°œ ì²­í¬ ì„ë² ë”© ì™„ë£Œ")
        
        # ì €í’ˆì§ˆ ì„ë² ë”© ìš”ì•½
        if self.stats['low_quality_embeddings'] > 0:
            quality_rate = (self.stats['low_quality_embeddings'] / self.stats['chunks_embedded']) * 100
            print(f"âš ï¸  ì €í’ˆì§ˆ ì„ë² ë”©: {self.stats['low_quality_embeddings']}ê°œ ({quality_rate:.1f}%)")
    
    def process_json_file(self, json_file: Path):
        """JSON íŒŒì¼ ì²˜ë¦¬"""
        print("\n" + "=" * 80)
        print(f"íŒŒì¼ ì²˜ë¦¬: {json_file.name}")
        print("=" * 80)
        
        # JSON ë¡œë“œ
        data = self.load_json_file(json_file)
        documents = data.get('documents', [])
        
        if not documents:
            print("âš ï¸  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í‚µ.")
            return
        
        print(f"ğŸ“Š ë¡œë“œëœ ë¬¸ì„œ: {len(documents)}ê°œ")
        
        # ë¬¸ì„œ ì‚½ì…
        self.insert_documents(documents)
        
        # ê° ë¬¸ì„œì˜ ì²­í¬ ì²˜ë¦¬
        all_chunks_to_embed = []
        
        for doc in tqdm(documents, desc="ì²­í¬ ì‚½ì…"):
            chunks = doc.get('chunks', [])
            chunks_to_embed = self.insert_chunks(doc['doc_id'], chunks)
            all_chunks_to_embed.extend(chunks_to_embed)
        
        print(f"âœ… ì²­í¬ ì‚½ì… ì™„ë£Œ: {self.stats['chunks']}ê°œ")
        if self.stats['chunks_skipped'] > 0:
            print(f"â­ï¸  ìŠ¤í‚µëœ ì²­í¬ (drop=True): {self.stats['chunks_skipped']}ê°œ")
        if self.stats['chunks_empty'] > 0:
            print(f"âš ï¸  ë¹ˆ content ì²­í¬: {self.stats['chunks_empty']}ê°œ")
        
        # ì„ë² ë”© ìƒì„±
        if self.load_only:
            print(f"ğŸ“ ë°ì´í„°ë§Œ ë¡œë“œ ëª¨ë“œ: {len(all_chunks_to_embed):,}ê°œ ì²­í¬ê°€ ì„ë² ë”© ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤.")
            print("   ë‚˜ì¤‘ì— ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„ë² ë”©ì„ ìƒì„±í•˜ì„¸ìš”:")
            print("   conda run -n dsr python backend/scripts/embedding/embedding_tool.py --generate-local")
        elif all_chunks_to_embed:
            if not self.api_available:
                print("âš ï¸  APIê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ë°ì´í„°ë§Œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                print("   ë‚˜ì¤‘ì— ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„ë² ë”©ì„ ìƒì„±í•˜ì„¸ìš”:")
                print("   conda run -n dsr python backend/scripts/embedding/embedding_tool.py --generate-local")
            else:
                self.embed_chunks(all_chunks_to_embed)
        else:
            print("âš ï¸  ì„ë² ë”©í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    def process_all_files(self, data_dir: Path = None):
        """ëª¨ë“  JSON íŒŒì¼ ì²˜ë¦¬"""
        if data_dir is None:
            data_dir = DATA_DIR / "transformed"
        
        print("\n" + "=" * 80)
        print(f"ë°ì´í„° ë””ë ‰í† ë¦¬: {data_dir}")
        print("=" * 80)
        
        # JSON íŒŒì¼ ì°¾ê¸° (transformation_summary.json ì œì™¸)
        json_files = [
            f for f in data_dir.glob('*.json')
            if f.name != 'transformation_summary.json' and f.name != 'validation_result.json'
        ]
        
        if not json_files:
            print(f"âŒ {data_dir}ì— JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"ğŸ“ ë°œê²¬ëœ íŒŒì¼: {len(json_files)}ê°œ")
        for f in json_files:
            print(f"  - {f.name}")
        
        # DB ì—°ê²°
        self.connect_db()
        
        # ê° íŒŒì¼ ì²˜ë¦¬
        for json_file in json_files:
            try:
                self.process_json_file(json_file)
            except Exception as e:
                error_msg = f"{json_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}"
                print(f"âŒ {error_msg}")
                self.stats['errors'].append(error_msg)
                import traceback
                traceback.print_exc()
        
        # ìµœì¢… í†µê³„
        self.print_stats()
        
        # ì—°ê²° ì¢…ë£Œ
        self.close_db()
    
    def print_stats(self):
        """í†µê³„ ì¶œë ¥ (ê°œì„ ë¨)"""
        print("\n" + "=" * 80)
        print("ğŸ“Š ì²˜ë¦¬ ì™„ë£Œ í†µê³„")
        print("=" * 80)
        print(f"ë¬¸ì„œ:                 {self.stats['documents']:,}ê°œ")
        print(f"ì²­í¬ (ì‚½ì…):          {self.stats['chunks']:,}ê°œ")
        print(f"ì²­í¬ (ìŠ¤í‚µ/drop):     {self.stats['chunks_skipped']:,}ê°œ")
        print(f"ì²­í¬ (ë¹ˆ content):    {self.stats['chunks_empty']:,}ê°œ")
        
        # ì „ì²˜ë¦¬ í†µê³„ (ì‹ ê·œ)
        print(f"\n[ì „ì²˜ë¦¬]")
        print(f"ì „ì²˜ë¦¬ ì™„ë£Œ:          {self.stats['chunks_preprocessed']:,}ê°œ")
        print(f"ì €í’ˆì§ˆ í…ìŠ¤íŠ¸ í•„í„°:   {self.stats['low_quality_texts']:,}ê°œ")
        if self.stats['chunks_preprocessed'] > 0:
            filter_rate = (self.stats['low_quality_texts'] / self.stats['chunks_preprocessed']) * 100
            print(f"  í•„í„°ë§ ë¹„ìœ¨:        {filter_rate:.1f}%")
        
        # ì„ë² ë”© í†µê³„
        print(f"\n[ì„ë² ë”©]")
        print(f"ì„ë² ë”© ìƒì„±:          {self.stats['chunks_embedded']:,}ê°œ")
        
        # í’ˆì§ˆ í†µê³„
        if self.stats['chunks_embedded'] > 0:
            quality_rate = (self.stats['low_quality_embeddings'] / self.stats['chunks_embedded']) * 100
            print(f"ì €í’ˆì§ˆ ì„ë² ë”©:        {self.stats['low_quality_embeddings']:,}ê°œ ({quality_rate:.1f}%)")
        
        if self.stats['quality_warnings']:
            print(f"\nâš ï¸  í’ˆì§ˆ ê²½ê³ : {len(self.stats['quality_warnings'])}ê°œ")
            # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
            for warning in self.stats['quality_warnings'][:3]:
                print(f"  {warning}")
            if len(self.stats['quality_warnings']) > 3:
                print(f"  ... ì™¸ {len(self.stats['quality_warnings']) - 3}ê°œ")
        
        if self.stats['errors']:
            print(f"\nâŒ ì˜¤ë¥˜: {len(self.stats['errors'])}ê°œ")
            for error in self.stats['errors'][:10]:  # ìµœëŒ€ 10ê°œë§Œ ì¶œë ¥
                print(f"  - {error}")
            if len(self.stats['errors']) > 10:
                print(f"  ... ì™¸ {len(self.stats['errors']) - 10}ê°œ")
    
    def verify_data(self):
        """ë°ì´í„° ì‚½ì… ë° ì„ë² ë”© í™•ì¸"""
        print("\n" + "=" * 80)
        print("ğŸ” ë°ì´í„° ê²€ì¦")
        print("=" * 80)
        
        with self.conn.cursor(cursor_factory=RealDictCursor) as cur:
            # ë¬¸ì„œ í†µê³„
            cur.execute("""
                SELECT doc_type, COUNT(*) as count
                FROM documents
                GROUP BY doc_type
                ORDER BY count DESC
            """)
            print("\nğŸ“„ ë¬¸ì„œ í†µê³„:")
            for row in cur.fetchall():
                print(f"  {row['doc_type']}: {row['count']:,}ê°œ")
            
            # ì²­í¬ í†µê³„
            cur.execute("""
                SELECT 
                    COUNT(*) as total,
                    COUNT(embedding) as embedded,
                    COUNT(*) - COUNT(embedding) as not_embedded
                FROM chunks
                WHERE drop = FALSE
            """)
            chunk_stats = cur.fetchone()
            print(f"\nğŸ“¦ ì²­í¬ í†µê³„:")
            print(f"  ì „ì²´:           {chunk_stats['total']:,}ê°œ")
            print(f"  ì„ë² ë”© ì™„ë£Œ:    {chunk_stats['embedded']:,}ê°œ")
            print(f"  ì„ë² ë”© ë¯¸ì™„ë£Œ:  {chunk_stats['not_embedded']:,}ê°œ")
            
            if chunk_stats['total'] > 0:
                embed_rate = (chunk_stats['embedded'] / chunk_stats['total']) * 100
                print(f"  ì„ë² ë”© ë¹„ìœ¨:    {embed_rate:.1f}%")
            
            # chunk_typeë³„ í†µê³„
            cur.execute("""
                SELECT chunk_type, COUNT(*) as count
                FROM chunks
                WHERE drop = FALSE
                GROUP BY chunk_type
                ORDER BY count DESC
                LIMIT 10
            """)
            print(f"\nğŸ·ï¸  ì²­í¬ íƒ€ì…ë³„ (ìƒìœ„ 10ê°œ):")
            for row in cur.fetchall():
                print(f"  {row['chunk_type']}: {row['count']:,}ê°œ")
            
            # dropëœ ì²­í¬
            cur.execute("SELECT COUNT(*) FROM chunks WHERE drop = TRUE")
            drop_result = cur.fetchone()
            dropped = drop_result['count'] if drop_result else 0
            if dropped > 0:
                print(f"\nâ­ï¸  ì œì™¸ëœ ì²­í¬ (drop=True): {dropped:,}ê°œ")
            
            # ì„ë² ë”©ë˜ì§€ ì•Šì€ ì²­í¬ ìƒ˜í”Œ (5ê°œ)
            if chunk_stats['not_embedded'] > 0:
                cur.execute("""
                    SELECT chunk_id, doc_id, content_length, 
                           LEFT(content, 50) as content_preview
                    FROM chunks
                    WHERE embedding IS NULL AND drop = FALSE
                    LIMIT 5
                """)
                print(f"\nâš ï¸  ì„ë² ë”© ë¯¸ì™„ë£Œ ì²­í¬ ìƒ˜í”Œ:")
                for row in cur.fetchall():
                    print(f"  {row['chunk_id']}")
                    print(f"    ê¸¸ì´: {row['content_length']}ì")
                    print(f"    ë‚´ìš©: {row['content_preview']}...")
            
            # ì„ë² ë”© í’ˆì§ˆ í†µê³„ (ìƒ˜í”Œë§)
            if chunk_stats['embedded'] > 0:
                print(f"\nğŸ” ì„ë² ë”© í’ˆì§ˆ ë¶„ì„ (ìƒ˜í”Œ 100ê°œ):")
                cur.execute("""
                    SELECT embedding
                    FROM chunks
                    WHERE embedding IS NOT NULL AND drop = FALSE
                    ORDER BY RANDOM()
                    LIMIT 100
                """)
                
                low_quality_count = 0
                quality_issues = []
                
                for row in cur.fetchall():
                    embedding = row['embedding']
                    is_low_quality, reason = self.is_low_quality_embedding(embedding)
                    if is_low_quality:
                        low_quality_count += 1
                        quality_issues.append(reason)
                
                sample_size = min(100, chunk_stats['embedded'])
                print(f"  ìƒ˜í”Œ í¬ê¸°:          {sample_size}ê°œ")
                print(f"  ì €í’ˆì§ˆ ì„ë² ë”©:      {low_quality_count}ê°œ")
                
                if low_quality_count > 0:
                    quality_rate = (low_quality_count / sample_size) * 100
                    print(f"  ì €í’ˆì§ˆ ë¹„ìœ¨:        {quality_rate:.1f}%")
                    
                    # ì´ìŠˆë³„ ì§‘ê³„
                    from collections import Counter
                    issue_counter = Counter(quality_issues)
                    print(f"  ì£¼ìš” ì´ìŠˆ:")
                    for issue, count in issue_counter.most_common(3):
                        print(f"    - {issue}: {count}ê°œ")
                else:
                    print(f"  í’ˆì§ˆ ìƒíƒœ:          ì–‘í˜¸ âœ…")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ë°ì´í„° ì„ë² ë”© íŒŒì´í”„ë¼ì¸')
    parser.add_argument('--load-only', action='store_true', 
                       help='ë°ì´í„°ë§Œ ë¡œë“œí•˜ê³  ì„ë² ë”©ì€ ìƒì„±í•˜ì§€ ì•ŠìŒ')
    args = parser.parse_args()
    
    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ
    db_config = {
        'host': os.getenv('DB_HOST', 'localhost'),
        'port': int(os.getenv('DB_PORT', 5432)),
        'database': os.getenv('DB_NAME', 'ddoksori'),
        'user': os.getenv('DB_USER', 'postgres'),
        'password': os.getenv('DB_PASSWORD', 'postgres')
    }
    
    embed_api_url = os.getenv('EMBED_API_URL', 'http://localhost:8001/embed')
    
    print("=" * 80)
    if args.load_only:
        print("ğŸ“¥ ë°ì´í„° ë¡œë“œë§Œ ìˆ˜í–‰ (ì„ë² ë”© ì œì™¸)")
    else:
        print("ğŸš€ ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("=" * 80)
    print(f"ë°ì´í„°ë² ì´ìŠ¤: {db_config['host']}:{db_config['port']}/{db_config['database']}")
    print(f"ì„ë² ë”© API: {embed_api_url}")
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    try:
        pipeline = EmbeddingPipeline(db_config, embed_api_url, load_only=args.load_only)
        pipeline.process_all_files()
        
        # ê²€ì¦
        pipeline.connect_db()
        pipeline.verify_data()
        pipeline.close_db()
        
        print("\n" + "=" * 80)
        print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print("=" * 80)
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        exit(1)


if __name__ == "__main__":
    main()
