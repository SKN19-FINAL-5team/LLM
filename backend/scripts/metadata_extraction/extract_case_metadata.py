#!/usr/bin/env python3
"""
ì‚¬ë¡€ ë°ì´í„° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ìŠ¤í¬ë¦½íŠ¸

ë¶„ìŸì¡°ì •ì‚¬ë¡€ ë° í”¼í•´êµ¬ì œì‚¬ë¡€ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
"""

import json
import re
import psycopg2
from psycopg2.extras import execute_batch
from pathlib import Path
import sys
from typing import List, Dict
from collections import Counter
from datetime import datetime

sys.path.append(str(Path(__file__).parent.parent))

from dotenv import load_dotenv
import os

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
backend_dir = Path(__file__).parent.parent.parent
env_file = backend_dir / '.env'
if env_file.exists():
    load_dotenv(env_file)

# DB ì—°ê²° ì •ë³´
DB_CONFIG = {
    'dbname': os.getenv('POSTGRES_DB', 'ddoksori'),
    'user': os.getenv('POSTGRES_USER', 'maroco'),
    'password': os.getenv('POSTGRES_PASSWORD', ''),
    'host': os.getenv('POSTGRES_HOST', 'localhost'),
    'port': os.getenv('POSTGRES_PORT', '5432')
}


class CaseMetadataExtractor:
    """ì‚¬ë¡€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸°"""
    
    # ì‚¬ë¡€ ê´€ë ¨ ì¤‘ìš” í‚¤ì›Œë“œ
    CASE_KEYWORDS = {
        'ì†Œë¹„ì': 1.3, 'ì‚¬ì—…ì': 1.3, 'íŒë§¤ì': 1.2, 'êµ¬ë§¤ì': 1.2,
        'ë¶„ìŸ': 1.5, 'ì¡°ì •': 1.4, 'ì¤‘ì¬': 1.3, 'í•©ì˜': 1.3,
        'í™˜ë¶ˆ': 1.5, 'êµí™˜': 1.5, 'ìˆ˜ë¦¬': 1.4, 'ë³´ìƒ': 1.4,
        'í•˜ì': 1.4, 'ë¶ˆëŸ‰': 1.3, 'ê²°í•¨': 1.3, 'íŒŒì†': 1.3,
        'ê³„ì•½': 1.3, 'ì²­ì•½': 1.2, 'ìŠ¹ë‚™': 1.2, 'í•´ì œ': 1.3,
    }
    
    # Chunk Typeë³„ ì¤‘ìš”ë„
    CHUNK_TYPE_IMPORTANCE = {
        'judgment': 2.0,           # íŒë‹¨ - ê°€ì¥ ì¤‘ìš”
        'decision': 2.0,           # ê²°ì •
        'parties_claim': 1.3,      # ë‹¹ì‚¬ì ì£¼ì¥
        'case_overview': 1.2,      # ì‚¬ê±´ ê°œìš”
        'qa_combined': 1.5,        # Q&A ê²°í•©
        'question': 1.0,           # ì§ˆë¬¸
        'answer': 1.8,             # ë‹µë³€ - ì¤‘ìš”
    }
    
    def __init__(self, db_config: Dict):
        self.db_config = db_config
        self.conn = None
        self.cur = None
    
    def connect_db(self):
        """DB ì—°ê²°"""
        self.conn = psycopg2.connect(**self.db_config)
        self.cur = self.conn.cursor()
        print(f"âœ… DB ì—°ê²° ì„±ê³µ: {self.db_config['dbname']}")
    
    def close_db(self):
        """DB ì—°ê²° ì¢…ë£Œ"""
        if self.cur:
            self.cur.close()
        if self.conn:
            self.conn.close()
        print("âœ… DB ì—°ê²° ì¢…ë£Œ")
    
    def extract_keywords_from_case(self, content: str, metadata: Dict) -> List[str]:
        """
        ì‚¬ë¡€ ë°ì´í„°ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            content: ì²­í¬ í…ìŠ¤íŠ¸
            metadata: ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
        
        Returns:
            ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        keywords = set()
        
        # 1. ë©”íƒ€ë°ì´í„°ì—ì„œ ì •ë³´ ì¶”ì¶œ
        if metadata:
            # ì‚¬ê±´ ë²ˆí˜¸
            case_no = metadata.get('case_no') or metadata.get('case_sn')
            if case_no:
                keywords.add(f"ì‚¬ê±´ë²ˆí˜¸:{case_no}")
            
            # ê²°ì • ë‚ ì§œ (ì—°ë„ë§Œ)
            decision_date = metadata.get('decision_date')
            if decision_date:
                year = str(decision_date)[:4] if len(str(decision_date)) >= 4 else None
                if year and year.isdigit():
                    keywords.add(f"{year}ë…„")
        
        # 2. í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ
        text_clean = re.sub(r'[^\w\sê°€-í£]', ' ', content)
        words = text_clean.split()
        
        word_freq = Counter()
        for word in words:
            word = word.strip()
            if len(word) >= 2:
                weight = self.CASE_KEYWORDS.get(word, 1.0)
                word_freq[word] += weight
        
        # ìƒìœ„ í‚¤ì›Œë“œ
        top_keywords = [word for word, _ in word_freq.most_common(10)]
        keywords.update(top_keywords)
        
        # 3. íŠ¹ì • íŒ¨í„´ ì¶”ì¶œ
        # ê¸ˆì•¡ íŒ¨í„´
        amounts = re.findall(r'\d+ë§Œ?\s?ì›', content)
        if amounts:
            keywords.add('ê¸ˆì•¡í¬í•¨')
        
        # ë‚ ì§œ íŒ¨í„´
        dates = re.findall(r'\d{4}[ë…„.-]\s?\d{1,2}[ì›”.-]\s?\d{1,2}ì¼?', content)
        if dates:
            keywords.add('ë‚ ì§œí¬í•¨')
        
        return list(keywords)[:20]  # ìµœëŒ€ 20ê°œ
    
    def extract_metadata_for_case_docs(self, batch_size: int = 500):
        """ì‚¬ë¡€ ë¬¸ì„œë“¤ì˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        print("\nğŸ” ì‚¬ë¡€ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œì‘...")
        
        # ì‚¬ë¡€ ë¬¸ì„œ ì¡°íšŒ (mediation_case, counsel_case, consumer_relief_case ë“±)
        try:
            self.cur.execute("""
                SELECT d.doc_id, d.title, d.metadata, c.content
                FROM documents d
                JOIN chunks c ON d.doc_id = c.doc_id
                WHERE (d.doc_type LIKE '%case%' OR d.doc_type LIKE '%mediation%' OR d.doc_type LIKE '%counsel%')
                    AND d.keywords IS NULL
                    AND c.chunk_index = 0
                ORDER BY d.doc_id
                LIMIT 10000  -- í•œ ë²ˆì— ìµœëŒ€ 10000ê±´ì”© ì²˜ë¦¬
            """)
        except Exception as e:
            print(f"âš ï¸  ì¿¼ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return
        
        try:
            case_docs = self.cur.fetchall()
        except Exception as e:
            print(f"âš ï¸  ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return
        
        total = len(case_docs)
        
        if total == 0:
            print("âš ï¸  ì²˜ë¦¬í•  ì‚¬ë¡€ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"ğŸ“Š ì´ {total}ê±´ì˜ ì‚¬ë¡€ ë¬¸ì„œ ë°œê²¬ (ë°°ì¹˜ ì²˜ë¦¬)")
        
        updates = []
        for idx, row in enumerate(case_docs, 1):
            try:
                # ì•ˆì „í•˜ê²Œ ì–¸íŒ¨í‚¹
                if len(row) != 4:
                    print(f"âš ï¸  ì˜ˆìƒì¹˜ ëª»í•œ row êµ¬ì¡° (ê¸¸ì´: {len(row)}): {row[:2] if len(row) >= 2 else row}")
                    continue
                
                doc_id, title, metadata, content = row
                
                if idx % 1000 == 0:
                    print(f"  ì²˜ë¦¬ ì¤‘: {idx}/{total} ({idx/total*100:.1f}%)")
                
                keywords = self.extract_keywords_from_case(content, metadata or {})
                updates.append((keywords, doc_id))
                
                if len(updates) >= batch_size:
                    self._update_keywords(updates)
                    updates = []
            except Exception as e:
                print(f"âš ï¸  í–‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (idx={idx}): {e}")
                continue
        
        if updates:
            self._update_keywords(updates)
        
        print(f"âœ… ì‚¬ë¡€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ: {total}ê±´")
    
    def _update_keywords(self, updates: List[tuple]):
        """í‚¤ì›Œë“œ ë°°ì¹˜ ì—…ë°ì´íŠ¸"""
        execute_batch(self.cur, """
            UPDATE documents
            SET keywords = %s,
                updated_at = NOW()
            WHERE doc_id = %s
        """, updates, page_size=500)
        self.conn.commit()
    
    def calculate_chunk_importance(self):
        """
        ì‚¬ë¡€ ì²­í¬ì˜ ì¤‘ìš”ë„ ê³„ì‚°
        
        chunk_typeì— ë”°ë¼ ì¤‘ìš”ë„ ë¶€ì—¬
        """
        print("\nğŸ” ì‚¬ë¡€ ì²­í¬ ì¤‘ìš”ë„ ê³„ì‚° ì‹œì‘...")
        
        try:
            # ê° chunk_typeë³„ë¡œ importance ì„¤ì •
            updates = []
            for chunk_type, importance in self.CHUNK_TYPE_IMPORTANCE.items():
                updates.append((importance, chunk_type))
            
            execute_batch(self.cur, """
                UPDATE chunks
                SET importance_score = %s
                WHERE chunk_type = %s
                    AND doc_id IN (
                        SELECT doc_id FROM documents 
                        WHERE doc_type LIKE '%case%' OR doc_type LIKE '%mediation%' OR doc_type LIKE '%counsel%'
                    )
            """, updates)
            
            updated = self.cur.rowcount
            self.conn.commit()
            
            print(f"âœ… ì‚¬ë¡€ ì²­í¬ ì¤‘ìš”ë„ ê³„ì‚° ì™„ë£Œ: {updated}ê±´")
        except Exception as e:
            print(f"âš ï¸  ì¤‘ìš”ë„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            self.conn.rollback()
    
    def run(self):
        """ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        try:
            self.connect_db()
            self.extract_metadata_for_case_docs()
            self.calculate_chunk_importance()
            
            # í†µê³„ ì¶œë ¥
            self.cur.execute("""
                SELECT 
                    COUNT(*) as total,
                    COUNT(*) FILTER (WHERE keywords IS NOT NULL) as with_keywords
                FROM documents
                WHERE doc_type LIKE '%case%' OR doc_type LIKE '%mediation%' OR doc_type LIKE '%counsel%'
            """)
            stats = self.cur.fetchone()
            
            print("\n" + "="*50)
            print("ğŸ“Š ì‚¬ë¡€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
            print("="*50)
            print(f"  ì „ì²´ ì‚¬ë¡€ ë¬¸ì„œ: {stats[0]}ê±´")
            print(f"  í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {stats[1]}ê±´")
            if stats[0] > 0:
                print(f"  ì™„ë£Œìœ¨: {stats[1]/stats[0]*100:.1f}%")
            print("="*50)
            
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            if self.conn:
                self.conn.rollback()
            raise
        finally:
            self.close_db()


if __name__ == '__main__':
    print("="*50)
    print("ì‚¬ë¡€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ")
    print("="*50)
    
    extractor = CaseMetadataExtractor(DB_CONFIG)
    extractor.run()
