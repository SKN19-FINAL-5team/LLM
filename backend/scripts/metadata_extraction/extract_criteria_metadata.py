#!/usr/bin/env python3
"""
ê¸°ì¤€ ë°ì´í„° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ìŠ¤í¬ë¦½íŠ¸

ê¸°ì¤€ ë°ì´í„°(í’ˆëª©, ë¶„ìŸí•´ê²°ê¸°ì¤€, ë³´ì¦ê¸°ê°„ ë“±)ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
"""

import json
import re
import psycopg2
from psycopg2.extras import execute_batch
from pathlib import Path
import sys
from typing import List, Dict

sys.path.append(str(Path(__file__).parent.parent))

from dotenv import load_dotenv
import os

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
backend_dir = Path(__file__).parent.parent.parent
env_file = backend_dir / '.env'
if env_file.exists():
    load_dotenv(env_file)

# DB ì—°ê²° ì •ë³´
DB_CONFIG = {
    'dbname': os.getenv('POSTGRES_DB', 'ddoksori'),
    'user': os.getenv('POSTGRES_USER', 'maroco'),
    'password': os.getenv('POSTGRES_PASSWORD', ''),
    'host': os.getenv('POSTGRES_HOST', 'localhost'),
    'port': os.getenv('POSTGRES_PORT', '5432')
}


class CriteriaMetadataExtractor:
    """ê¸°ì¤€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸°"""
    
    # ë¶„ìŸ ìœ í˜• í‚¤ì›Œë“œ
    DISPUTE_TYPES = {
        'í™˜ë¶ˆ': 1.5, 'êµí™˜': 1.5, 'ìˆ˜ë¦¬': 1.5,
        'ë¶€íŒ¨': 1.3, 'ë³€ì§ˆ': 1.3, 'íŒŒì†': 1.3, 'ë¶ˆëŸ‰': 1.3,
        'ì§€ì—°': 1.2, 'ë¯¸ë°°ì†¡': 1.3, 'ì˜¤ë°°ì†¡': 1.3,
        'í•˜ì': 1.4, 'ê²°í•¨': 1.4, 'ì˜¤ì‘ë™': 1.3,
        'ì·¨ì†Œ': 1.2, 'ì² íšŒ': 1.2, 'ë°˜í’ˆ': 1.3,
    }
    
    def __init__(self, db_config: Dict):
        self.db_config = db_config
        self.conn = None
        self.cur = None
    
    def connect_db(self):
        """DB ì—°ê²°"""
        self.conn = psycopg2.connect(**self.db_config)
        self.cur = self.conn.cursor()
        print(f"âœ… DB ì—°ê²° ì„±ê³µ: {self.db_config['dbname']}")
    
    def close_db(self):
        """DB ì—°ê²° ì¢…ë£Œ"""
        if self.cur:
            self.cur.close()
        if self.conn:
            self.conn.close()
        print("âœ… DB ì—°ê²° ì¢…ë£Œ")
    
    def extract_keywords_from_criteria(self, content: str, metadata: Dict) -> List[str]:
        """
        ê¸°ì¤€ ë°ì´í„°ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            content: ì²­í¬ í…ìŠ¤íŠ¸
            metadata: ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
        
        Returns:
            ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        keywords = set()
        
        # 1. ë©”íƒ€ë°ì´í„°ì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ
        if metadata:
            # í’ˆëª©ëª…
            item_name = metadata.get('item_name') or metadata.get('item')
            if item_name:
                keywords.add(item_name)
            
            # ë³„ì¹­ (aliases)
            aliases = metadata.get('aliases', [])
            if isinstance(aliases, list):
                keywords.update(aliases[:5])  # ìµœëŒ€ 5ê°œ
            elif isinstance(aliases, str):
                try:
                    aliases_list = json.loads(aliases)
                    keywords.update(aliases_list[:5])
                except:
                    pass
            
            # ì¹´í…Œê³ ë¦¬
            if metadata.get('category'):
                keywords.add(metadata['category'])
            
            # ì‚°ì—… ë¶„ë¥˜
            if metadata.get('industry'):
                keywords.add(metadata['industry'])
            
            # í’ˆëª© ê·¸ë£¹
            if metadata.get('item_group'):
                keywords.add(metadata['item_group'])
            
            # ë¶„ìŸ ìœ í˜•
            dispute_type = metadata.get('dispute_type')
            if dispute_type:
                keywords.add(dispute_type)
        
        # 2. í…ìŠ¤íŠ¸ì—ì„œ í’ˆëª©ëª… ì¶”ì¶œ (ì •ê·œì‹)
        # íŒ¨í„´: "í’ˆëª©: XXX", "[í’ˆëª©] XXX" ë“±
        item_patterns = re.findall(r'(?:í’ˆëª©|item)[:\s]+([ê°€-í£a-zA-Z0-9\s,]+)', content, re.IGNORECASE)
        for pattern in item_patterns[:3]:
            items = [item.strip() for item in pattern.split(',')]
            keywords.update(items[:5])
        
        # 3. ë¶„ìŸ ìœ í˜• í‚¤ì›Œë“œ ê²€ì¶œ
        for dispute_keyword in self.DISPUTE_TYPES.keys():
            if dispute_keyword in content:
                keywords.add(dispute_keyword)
        
        return list(keywords)[:25]  # ìµœëŒ€ 25ê°œ
    
    def extract_metadata_for_criteria_docs(self, batch_size: int = 100):
        """ê¸°ì¤€ ë¬¸ì„œë“¤ì˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        print("\nğŸ” ê¸°ì¤€ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œì‘...")
        
        # ê¸°ì¤€ ë¬¸ì„œ ì¡°íšŒ (doc_typeì´ 'criteria'ë¡œ ì‹œì‘í•˜ê±°ë‚˜ 'guideline'ë¡œ ì‹œì‘)
        try:
            self.cur.execute("""
                SELECT d.doc_id, d.title, d.metadata, c.content
                FROM documents d
                JOIN chunks c ON d.doc_id = c.doc_id
                WHERE (d.doc_type LIKE 'criteria%' OR d.doc_type LIKE 'guideline%')
                    AND d.keywords IS NULL
                    AND c.chunk_index = 0
                ORDER BY d.doc_id
            """)
            
            criteria_docs = self.cur.fetchall()
        except Exception as e:
            print(f"âš ï¸  ì¿¼ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return
        
        total = len(criteria_docs)
        
        if total == 0:
            print("âš ï¸  ì²˜ë¦¬í•  ê¸°ì¤€ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"ğŸ“Š ì´ {total}ê±´ì˜ ê¸°ì¤€ ë¬¸ì„œ ë°œê²¬")
        
        updates = []
        for idx, row in enumerate(criteria_docs, 1):
            try:
                if len(row) != 4:
                    print(f"âš ï¸  ì˜ˆìƒì¹˜ ëª»í•œ row êµ¬ì¡° (ê¸¸ì´: {len(row)})")
                    continue
                
                doc_id, title, metadata, content = row
                
                if idx % 50 == 0:
                    print(f"  ì²˜ë¦¬ ì¤‘: {idx}/{total} ({idx/total*100:.1f}%)")
                
                keywords = self.extract_keywords_from_criteria(content, metadata or {})
                updates.append((keywords, doc_id))
                
                if len(updates) >= batch_size:
                    self._update_keywords(updates)
                    updates = []
            except Exception as e:
                print(f"âš ï¸  í–‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (idx={idx}): {e}")
                continue
        
        if updates:
            self._update_keywords(updates)
        
        print(f"âœ… ê¸°ì¤€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ: {total}ê±´")
    
    def _update_keywords(self, updates: List[tuple]):
        """í‚¤ì›Œë“œ ë°°ì¹˜ ì—…ë°ì´íŠ¸"""
        execute_batch(self.cur, """
            UPDATE documents
            SET keywords = %s,
                updated_at = NOW()
            WHERE doc_id = %s
        """, updates)
        self.conn.commit()
    
    def calculate_chunk_importance(self):
        """
        ê¸°ì¤€ ì²­í¬ì˜ ì¤‘ìš”ë„ ê³„ì‚°
        
        ì¤‘ìš”ë„ ê¸°ì¤€:
        - resolution_row (í•´ê²°ê¸°ì¤€): 2.0  <- ê°€ì¥ ì¤‘ìš”
        - item_chunk (í’ˆëª©): 1.5
        - warranty/lifespan (ë³´ì¦/ë‚´ìš©ì—°ìˆ˜): 1.3
        - guideline (ê°€ì´ë“œë¼ì¸): 1.0
        """
        print("\nğŸ” ê¸°ì¤€ ì²­í¬ ì¤‘ìš”ë„ ê³„ì‚° ì‹œì‘...")
        
        self.cur.execute("""
            UPDATE chunks
            SET importance_score = CASE
                WHEN chunk_type = 'resolution_row' THEN 2.0
                WHEN chunk_type LIKE '%item%' THEN 1.5
                WHEN chunk_type LIKE '%warranty%' OR chunk_type LIKE '%lifespan%' THEN 1.3
                WHEN chunk_type LIKE '%guideline%' THEN 1.0
                ELSE 1.0
            END
            WHERE doc_id IN (
                SELECT doc_id FROM documents 
                WHERE doc_type LIKE 'criteria%' OR doc_type LIKE 'guideline%'
            )
        """)
        
        updated = self.cur.rowcount
        self.conn.commit()
        
        print(f"âœ… ê¸°ì¤€ ì²­í¬ ì¤‘ìš”ë„ ê³„ì‚° ì™„ë£Œ: {updated}ê±´")
    
    def run(self):
        """ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        try:
            self.connect_db()
            self.extract_metadata_for_criteria_docs()
            self.calculate_chunk_importance()
            
            # í†µê³„ ì¶œë ¥
            self.cur.execute("""
                SELECT 
                    COUNT(*) as total,
                    COUNT(*) FILTER (WHERE keywords IS NOT NULL) as with_keywords
                FROM documents
                WHERE doc_type LIKE 'criteria%' OR doc_type LIKE 'guideline%'
            """)
            stats = self.cur.fetchone()
            
            print("\n" + "="*50)
            print("ğŸ“Š ê¸°ì¤€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
            print("="*50)
            print(f"  ì „ì²´ ê¸°ì¤€ ë¬¸ì„œ: {stats[0]}ê±´")
            print(f"  í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {stats[1]}ê±´")
            print(f"  ì™„ë£Œìœ¨: {stats[1]/stats[0]*100:.1f}%" if stats[0] > 0 else "")
            print("="*50)
            
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            if self.conn:
                self.conn.rollback()
            raise
        finally:
            self.close_db()


if __name__ == '__main__':
    print("="*50)
    print("ê¸°ì¤€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ")
    print("="*50)
    
    extractor = CriteriaMetadataExtractor(DB_CONFIG)
    extractor.run()
