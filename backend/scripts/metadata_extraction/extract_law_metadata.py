#!/usr/bin/env python3
"""
ë²•ë ¹ ë°ì´í„° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ìŠ¤í¬ë¦½íŠ¸

ë²•ë ¹ ë°ì´í„°ì—ì„œ keywords, search_vector ë“±ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ DBì— ì €ì¥
"""

import json
import re
import psycopg2
from psycopg2.extras import execute_batch
from pathlib import Path
import sys
from typing import List, Dict, Set
from collections import Counter

# ë¶€ëª¨ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from dotenv import load_dotenv
import os

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
backend_dir = Path(__file__).parent.parent.parent
env_file = backend_dir / '.env'
if env_file.exists():
    load_dotenv(env_file)

# DB ì—°ê²° ì •ë³´
DB_CONFIG = {
    'dbname': os.getenv('POSTGRES_DB', 'ddoksori'),
    'user': os.getenv('POSTGRES_USER', 'maroco'),
    'password': os.getenv('POSTGRES_PASSWORD', ''),
    'host': os.getenv('POSTGRES_HOST', 'localhost'),
    'port': os.getenv('POSTGRES_PORT', '5432')
}


class LawMetadataExtractor:
    """ë²•ë ¹ ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸°"""
    
    # ë²•ë¥  ìš©ì–´ ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜
    LEGAL_TERMS = {
        'ì œ': 2.0, 'ì¡°': 2.0, 'í•­': 1.5, 'í˜¸': 1.5,
        'ë²•ë¥ ': 2.0, 'ê·œì •': 1.8, 'ì‹œí–‰ë ¹': 1.8, 'ì‹œí–‰ê·œì¹™': 1.8,
        'ì†Œë¹„ì': 1.5, 'ì‚¬ì—…ì': 1.5, 'ê³„ì•½': 1.5, 'ì±…ì„': 1.5,
        'ì†í•´ë°°ìƒ': 2.0, 'í™˜ê¸‰': 1.8, 'êµí™˜': 1.8, 'ë°˜í’ˆ': 1.8,
        'ì·¨ì†Œ': 1.5, 'í•´ì œ': 1.5, 'ì² íšŒ': 1.5,
        'ê¶Œë¦¬': 1.3, 'ì˜ë¬´': 1.3, 'ê¸ˆì§€': 1.3,
    }
    
    # ë¶ˆìš©ì–´ (ì œì™¸í•  ë‹¨ì–´)
    STOPWORDS = {
        'ì˜', 'ê°€', 'ì´', 'ì€', 'ëŠ”', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ',
        'ê³¼', 'ì™€', 'ë°', 'ê·¸', 'ì €', 'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ê²ƒ',
        'ë“±', 'ê¸°íƒ€', 'ê²½ìš°', 'ë•Œ', 'ìˆ˜', 'ë‚´', 'ì¤‘', 'ê°„',
    }
    
    def __init__(self, db_config: Dict):
        self.db_config = db_config
        self.conn = None
        self.cur = None
    
    def connect_db(self):
        """DB ì—°ê²°"""
        self.conn = psycopg2.connect(**self.db_config)
        self.cur = self.conn.cursor()
        print(f"âœ… DB ì—°ê²° ì„±ê³µ: {self.db_config['dbname']}")
    
    def close_db(self):
        """DB ì—°ê²° ì¢…ë£Œ"""
        if self.cur:
            self.cur.close()
        if self.conn:
            self.conn.close()
        print("âœ… DB ì—°ê²° ì¢…ë£Œ")
    
    def extract_keywords(self, text: str, metadata: Dict) -> List[str]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            text: ì²­í¬ í…ìŠ¤íŠ¸
            metadata: ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
        
        Returns:
            ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        keywords = set()
        
        # 1. ì¡°ë¬¸ ì •ë³´ ì¶”ê°€
        if metadata.get('law_name'):
            keywords.add(metadata['law_name'])
        if metadata.get('article_no'):
            keywords.add(metadata['article_no'])
        if metadata.get('path'):
            keywords.add(metadata['path'])
        
        # 2. í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ (ê°„ë‹¨í•œ í† í°í™”)
        # í•œê¸€, ìˆ«ì, ì¼ë¶€ íŠ¹ìˆ˜ë¬¸ìë§Œ ìœ ì§€
        text_clean = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        words = text_clean.split()
        
        # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        word_freq = Counter()
        for word in words:
            word = word.strip()
            if len(word) >= 2 and word not in self.STOPWORDS:
                # ë²•ë¥  ìš©ì–´ì— ê°€ì¤‘ì¹˜ ì ìš©
                weight = self.LEGAL_TERMS.get(word, 1.0)
                word_freq[word] += weight
        
        # 3. ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ (ë¹ˆë„ ê¸°ì¤€)
        top_keywords = [word for word, _ in word_freq.most_common(15)]
        keywords.update(top_keywords)
        
        # 4. ì¡°ë¬¸ ë²ˆí˜¸ íŒ¨í„´ ì¶”ì¶œ (ì œNì¡°, ì œNí•­ ë“±)
        article_patterns = re.findall(r'ì œ\d+ì¡°', text)
        keywords.update(article_patterns[:5])  # ìµœëŒ€ 5ê°œ
        
        return list(keywords)[:20]  # ìµœëŒ€ 20ê°œ í‚¤ì›Œë“œ
    
    def extract_metadata_for_law_docs(self, batch_size: int = 100):
        """
        ë²•ë ¹ ë¬¸ì„œë“¤ì˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° ì—…ë°ì´íŠ¸
        
        Args:
            batch_size: ë°°ì¹˜ í¬ê¸°
        """
        print("\nğŸ” ë²•ë ¹ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œì‘...")
        
        # ë²•ë ¹ ë¬¸ì„œ ì¡°íšŒ
        try:
            self.cur.execute("""
                SELECT d.doc_id, d.title, d.metadata, c.content
                FROM documents d
                JOIN chunks c ON d.doc_id = c.doc_id
                WHERE d.doc_type = 'law'
                    AND d.keywords IS NULL
                    AND c.chunk_index = 0  -- ì²« ë²ˆì§¸ ì²­í¬ë§Œ ì‚¬ìš©
                ORDER BY d.doc_id
            """)
            
            law_docs = self.cur.fetchall()
        except Exception as e:
            print(f"âš ï¸  ì¿¼ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return
        
        total = len(law_docs)
        
        if total == 0:
            print("âš ï¸  ì²˜ë¦¬í•  ë²•ë ¹ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"ğŸ“Š ì´ {total}ê±´ì˜ ë²•ë ¹ ë¬¸ì„œ ë°œê²¬")
        
        updates = []
        for idx, row in enumerate(law_docs, 1):
            try:
                if len(row) != 4:
                    print(f"âš ï¸  ì˜ˆìƒì¹˜ ëª»í•œ row êµ¬ì¡° (ê¸¸ì´: {len(row)})")
                    continue
                
                doc_id, title, metadata, content = row
                
                if idx % 100 == 0:
                    print(f"  ì²˜ë¦¬ ì¤‘: {idx}/{total} ({idx/total*100:.1f}%)")
                
                # í‚¤ì›Œë“œ ì¶”ì¶œ
                keywords = self.extract_keywords(content, metadata or {})
                
                updates.append((keywords, doc_id))
                
                # ë°°ì¹˜ í¬ê¸°ì— ë„ë‹¬í•˜ë©´ ì—…ë°ì´íŠ¸
                if len(updates) >= batch_size:
                    self._update_keywords(updates)
                    updates = []
            except Exception as e:
                print(f"âš ï¸  í–‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (idx={idx}): {e}")
                continue
        
        # ë‚¨ì€ ì—…ë°ì´íŠ¸ ì²˜ë¦¬
        if updates:
            self._update_keywords(updates)
        
        print(f"âœ… ë²•ë ¹ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ: {total}ê±´")
    
    def _update_keywords(self, updates: List[tuple]):
        """í‚¤ì›Œë“œ ë°°ì¹˜ ì—…ë°ì´íŠ¸"""
        execute_batch(self.cur, """
            UPDATE documents
            SET keywords = %s,
                updated_at = NOW()
            WHERE doc_id = %s
        """, updates)
        self.conn.commit()
    
    def calculate_chunk_importance(self):
        """
        ë²•ë ¹ ì²­í¬ì˜ ì¤‘ìš”ë„ ê³„ì‚°
        
        ì¤‘ìš”ë„ ê¸°ì¤€:
        - article (ì¡°): 1.5
        - paragraph (í•­): 1.2
        - item (í˜¸): 1.0
        """
        print("\nğŸ” ë²•ë ¹ ì²­í¬ ì¤‘ìš”ë„ ê³„ì‚° ì‹œì‘...")
        
        self.cur.execute("""
            UPDATE chunks
            SET importance_score = CASE
                WHEN chunk_type = 'article' THEN 1.5
                WHEN chunk_type = 'paragraph' THEN 1.2
                WHEN chunk_type = 'item' THEN 1.0
                ELSE 1.0
            END
            WHERE doc_id IN (
                SELECT doc_id FROM documents WHERE doc_type = 'law'
            )
        """)
        
        updated = self.cur.rowcount
        self.conn.commit()
        
        print(f"âœ… ë²•ë ¹ ì²­í¬ ì¤‘ìš”ë„ ê³„ì‚° ì™„ë£Œ: {updated}ê±´")
    
    def run(self):
        """ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        try:
            self.connect_db()
            self.extract_metadata_for_law_docs()
            self.calculate_chunk_importance()
            
            # í†µê³„ ì¶œë ¥
            self.cur.execute("""
                SELECT 
                    COUNT(*) as total,
                    COUNT(*) FILTER (WHERE keywords IS NOT NULL) as with_keywords
                FROM documents
                WHERE doc_type = 'law'
            """)
            stats = self.cur.fetchone()
            
            print("\n" + "="*50)
            print("ğŸ“Š ë²•ë ¹ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
            print("="*50)
            print(f"  ì „ì²´ ë²•ë ¹ ë¬¸ì„œ: {stats[0]}ê±´")
            print(f"  í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {stats[1]}ê±´")
            print(f"  ì™„ë£Œìœ¨: {stats[1]/stats[0]*100:.1f}%" if stats[0] > 0 else "")
            print("="*50)
            
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            if self.conn:
                self.conn.rollback()
            raise
        finally:
            self.close_db()


if __name__ == '__main__':
    print("="*50)
    print("ë²•ë ¹ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ")
    print("="*50)
    
    extractor = LawMetadataExtractor(DB_CONFIG)
    extractor.run()
