"""
HuggingFace ì ‘ê·¼ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
naver/splade-v3 ëª¨ë¸ ì ‘ê·¼ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
"""

import os
import sys
from dotenv import load_dotenv
from pathlib import Path

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
backend_dir = Path(__file__).parent.parent
env_file = backend_dir / '.env'
if env_file.exists():
    load_dotenv(env_file)
else:
    root_env = Path(__file__).parent.parent.parent / '.env'
    if root_env.exists():
        load_dotenv(root_env)
    else:
        load_dotenv()

# HuggingFace í† í° í™•ì¸
HF_TOKEN = os.getenv('HF_TOKEN') or os.getenv('HUGGINGFACE_TOKEN')


def check_hf_access():
    """HuggingFace ëª¨ë¸ ì ‘ê·¼ í™•ì¸"""
    print("=" * 80)
    print("HuggingFace ì ‘ê·¼ í™•ì¸")
    print("=" * 80)
    
    model_name = "naver/splade-v3"
    print(f"\nğŸ“¦ ëª¨ë¸: {model_name}")
    
    # í† í° í™•ì¸
    if HF_TOKEN:
        print(f"âœ… HF_TOKEN í™˜ê²½ ë³€ìˆ˜ ë°œê²¬ (ê¸¸ì´: {len(HF_TOKEN)})")
        os.environ['HF_TOKEN'] = HF_TOKEN
    else:
        print("âš ï¸  HF_TOKEN í™˜ê²½ ë³€ìˆ˜ ì—†ìŒ")
        print("   í† í° ì—†ì´ ì ‘ê·¼ ì‹œë„ (ê³µê°œ ëª¨ë¸ì¸ ê²½ìš° ê°€ëŠ¥)")
    
    # transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
    try:
        from transformers import AutoModelForMaskedLM, AutoTokenizer
        print("âœ… transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ë¨")
    except ImportError:
        print("âŒ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ì„¤ì¹˜: pip install transformers torch")
        return False
    
    # ëª¨ë¸ ì ‘ê·¼ ì‹œë„
    print(f"\nğŸ” ëª¨ë¸ ì ‘ê·¼ ì‹œë„ ì¤‘...")
    try:
        from transformers import AutoTokenizer
        
        # í† í¬ë‚˜ì´ì €ë§Œ ë¨¼ì € ì‹œë„ (ê°€ë²¼ì›€)
        print("  1. í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹œë„...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            token=HF_TOKEN if HF_TOKEN else None
        )
        print("  âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ!")
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
        test_text = "ë¯¼ë²• ì œ750ì¡° ë¶ˆë²•í–‰ìœ„"
        tokens = tokenizer(test_text, return_tensors="pt")
        print(f"  âœ… í† í°í™” í…ŒìŠ¤íŠ¸ ì„±ê³µ (í† í° ìˆ˜: {tokens['input_ids'].shape[1]})")
        
        # ëª¨ë¸ ë©”íƒ€ë°ì´í„° í™•ì¸
        print("\n  2. ëª¨ë¸ ë©”íƒ€ë°ì´í„° í™•ì¸...")
        try:
            from huggingface_hub import model_info
            info = model_info(model_name, token=HF_TOKEN if HF_TOKEN else None)
            print(f"  âœ… ëª¨ë¸ ì •ë³´:")
            print(f"     - ID: {info.id}")
            print(f"     - ê³µê°œ ì—¬ë¶€: {info.private == False}")
            if hasattr(info, 'tags'):
                print(f"     - íƒœê·¸: {', '.join(info.tags[:5])}")
        except ImportError:
            print("  âš ï¸  huggingface_hub ì—†ìŒ (ë©”íƒ€ë°ì´í„° í™•ì¸ ê±´ë„ˆëœ€)")
        except Exception as e:
            print(f"  âš ï¸  ë©”íƒ€ë°ì´í„° í™•ì¸ ì‹¤íŒ¨: {e}")
        
        # ì „ì²´ ëª¨ë¸ ë¡œë“œ ì‹œë„ (ì„ íƒì )
        print("\n  3. ì „ì²´ ëª¨ë¸ ë¡œë“œ ì‹œë„ (ì„ íƒì )...")
        try:
            import torch
            from transformers import AutoModelForMaskedLM
            
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            print(f"     ë””ë°”ì´ìŠ¤: {device}")
            
            model = AutoModelForMaskedLM.from_pretrained(
                model_name,
                token=HF_TOKEN if HF_TOKEN else None
            )
            model.to(device)
            model.eval()
            print("  âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
            
            # ê°„ë‹¨í•œ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸
            print("  4. ì¸ì½”ë”© í…ŒìŠ¤íŠ¸...")
            with torch.no_grad():
                inputs = tokenizer(test_text, return_tensors="pt").to(device)
                outputs = model(**inputs)
                print(f"  âœ… ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ ì„±ê³µ (ì¶œë ¥ shape: {outputs.logits.shape})")
            
            print("\n" + "=" * 80)
            print("âœ… HuggingFace ëª¨ë¸ ì ‘ê·¼ ì„±ê³µ!")
            print("=" * 80)
            if not HF_TOKEN:
                print("\nğŸ’¡ ì°¸ê³ : í† í° ì—†ì´ë„ ì ‘ê·¼ ê°€ëŠ¥í•©ë‹ˆë‹¤ (ê³µê°œ ëª¨ë¸).")
            return True
            
        except Exception as e:
            print(f"  âš ï¸  ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            if "401" in str(e) or "Unauthorized" in str(e):
                print("\nâŒ ì¸ì¦ ì˜¤ë¥˜: í† í°ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                print("\nğŸ’¡ í•´ê²° ë°©ë²•:")
                print("  1. HuggingFace ê³„ì • ìƒì„±: https://huggingface.co/join")
                print("  2. í† í° ìƒì„±: Settings > Access Tokens > New token")
                print("  3. .env íŒŒì¼ì— ì¶”ê°€: HF_TOKEN=your_token_here")
                return False
            else:
                print(f"\nâš ï¸  ë‹¤ë¥¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
                print("   í† í¬ë‚˜ì´ì €ëŠ” ì •ìƒì´ë¯€ë¡œ ê¸°ë³¸ ê¸°ëŠ¥ì€ ì‚¬ìš© ê°€ëŠ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                return True
        
    except Exception as e:
        error_msg = str(e)
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {error_msg}")
        
        if "401" in error_msg or "Unauthorized" in error_msg:
            print("\nâŒ ì¸ì¦ ì˜¤ë¥˜: í† í°ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            print("\nğŸ’¡ í•´ê²° ë°©ë²•:")
            print("  1. HuggingFace ê³„ì • ìƒì„±: https://huggingface.co/join")
            print("  2. í† í° ìƒì„±: Settings > Access Tokens > New token")
            print("  3. .env íŒŒì¼ì— ì¶”ê°€: HF_TOKEN=your_token_here")
            return False
        elif "404" in error_msg or "not found" in error_msg.lower():
            print("\nâŒ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"   ëª¨ë¸ëª… í™•ì¸: {model_name}")
            return False
        else:
            print(f"\nâš ï¸  ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return False


if __name__ == "__main__":
    success = check_hf_access()
    sys.exit(0 if success else 1)
