"""
ë¡œì»¬ í™˜ê²½ìš© RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
Hybrid Search, BM25, Cosine Similarity 3ê°€ì§€ ê²€ìƒ‰ ë°©ë²•ì„ ë¹„êµí•˜ê³  LLM ë¶„ì„ì„ í¬í•¨í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python backend/scripts/test_rag_local_simple.py
    python backend/scripts/test_rag_local_simple.py --query "ëƒ‰ì¥ê³  í™˜ë¶ˆ ì‚¬ë¡€"
"""

import os
import sys
import argparse
import json
import re
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, List
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
backend_dir = Path(__file__).parent.parent
project_root = backend_dir.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(backend_dir))

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

from app.rag.multi_method_retriever import MultiMethodRetriever
from app.rag.retriever import VectorRetriever
from app.rag.generator import RAGGenerator


class SimpleRAGTester:
    """ë¡œì»¬ í™˜ê²½ìš© ê°„ë‹¨í•œ RAG í…ŒìŠ¤í„°"""
    
    def __init__(self, db_config: dict):
        """
        Args:
            db_config: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
        """
        self.db_config = db_config
        self.retriever = None
        self.vector_retriever = None  # ìˆœì°¨ ê²€ìƒ‰ìš©
        self.generator = None
        # logs í´ë” ê²½ë¡œ ì„¤ì •
        self.logs_dir = backend_dir / 'logs'
        self._ensure_logs_dir()
        
    def _ensure_logs_dir(self):
        """logs í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±"""
        self.logs_dir.mkdir(parents=True, exist_ok=True)
        # .gitkeep íŒŒì¼ ìƒì„± (ë¹ˆ í´ë”ë„ gitì— í¬í•¨ë˜ë„ë¡)
        gitkeep_file = self.logs_dir / '.gitkeep'
        if not gitkeep_file.exists():
            gitkeep_file.touch()
    
    def initialize(self):
        """ê²€ìƒ‰ê¸°ì™€ ìƒì„±ê¸° ì´ˆê¸°í™”"""
        print("ğŸ”§ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        try:
            # MultiMethodRetriever ì´ˆê¸°í™” (SPLADE ì œì™¸)
            self.retriever = MultiMethodRetriever(self.db_config)
            print("âœ… MultiMethodRetriever ì´ˆê¸°í™” ì™„ë£Œ")
            
            # VectorRetriever ì´ˆê¸°í™” (ìˆœì°¨ ê²€ìƒ‰ìš©)
            self.vector_retriever = VectorRetriever(self.db_config)
            print("âœ… VectorRetriever ì´ˆê¸°í™” ì™„ë£Œ")
            
            # RAGGenerator ì´ˆê¸°í™”
            self.generator = RAGGenerator()
            print("âœ… RAGGenerator ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def _search_by_doc_type(
        self,
        query: str,
        doc_type: str,
        top_k: int = 10
    ) -> List[Dict]:
        """
        doc_typeìœ¼ë¡œ í•„í„°ë§ëœ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            doc_type: ë¬¸ì„œ íƒ€ì… (ì˜ˆ: 'mediation_case', 'counsel_case', 'law')
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
        
        Returns:
            ê²€ìƒ‰ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        self.vector_retriever.connect_db()
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.vector_retriever.embed_query(query)
        
        # SQL ì¿¼ë¦¬ êµ¬ì„± (doc_type í•„í„° ì¶”ê°€)
        sql = """
            SELECT 
                c.chunk_id,
                c.doc_id,
                c.chunk_type,
                c.content,
                c.content_length,
                d.title,
                d.metadata->>'decision_date' AS decision_date,
                d.source_org AS agency,
                d.doc_type AS source,
                1 - (c.embedding <=> %s::vector) AS similarity
            FROM chunks c
            JOIN documents d ON c.doc_id = d.doc_id
            WHERE c.drop = FALSE
                AND d.doc_type = %s
        """
        
        params = [query_embedding, doc_type]
        
        sql += """
            ORDER BY c.embedding <=> %s::vector
            LIMIT %s
        """
        params.append(query_embedding)
        params.append(top_k)
        
        # ì¿¼ë¦¬ ì‹¤í–‰
        with self.vector_retriever.conn.cursor() as cur:
            cur.execute(sql, params)
            rows = cur.fetchall()
        
        # ê²°ê³¼ í¬ë§·íŒ…
        results = []
        for row in rows:
            results.append({
                'chunk_id': row[0],
                'doc_id': row[1],
                'chunk_type': row[2],
                'text': row[3],
                'text_len': row[4],
                'case_no': row[5],
                'decision_date': row[6],
                'agency': row[7],
                'source': row[8],
                'similarity': float(row[9])
            })
        
        return results
    
    def _get_document_full_text(self, doc_id: str) -> str:
        """
        ê°™ì€ doc_idì˜ ëª¨ë“  chunksë¥¼ ê°€ì ¸ì™€ì„œ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
        
        Args:
            doc_id: ë¬¸ì„œ ID
        
        Returns:
            ë¬¸ì„œ ì „ì²´ í…ìŠ¤íŠ¸ (ëª¨ë“  chunks í•©ì¹œ ê²ƒ)
        """
        chunks = self.vector_retriever.get_case_chunks(doc_id)
        # chunk_index ìˆœì„œë¡œ ì •ë ¬ (seq í•„ë“œ ì‚¬ìš©)
        chunks.sort(key=lambda x: x.get('seq', 0))
        # ëª¨ë“  í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
        full_text = '\n\n'.join(chunk.get('text', '') for chunk in chunks)
        return full_text
    
    def _enrich_with_full_text(self, results: List[Dict]) -> List[Dict]:
        """
        ê²€ìƒ‰ ê²°ê³¼ì— ë¬¸ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ê°€
        
        Args:
            results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            raw_text í•„ë“œê°€ ì¶”ê°€ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        enriched_results = []
        seen_doc_ids = set()  # ì¤‘ë³µ ì œê±°ìš©
        
        for result in results:
            doc_id = result.get('doc_id') or result.get('case_uid')
            if not doc_id:
                continue
            
            # ì´ë¯¸ ì²˜ë¦¬í•œ doc_idëŠ” ê±´ë„ˆë›°ê¸°
            if doc_id in seen_doc_ids:
                continue
            seen_doc_ids.add(doc_id)
            
            try:
                # ë¬¸ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
                raw_text = self._get_document_full_text(doc_id)
                
                # chunks ê°œìˆ˜ ê°€ì ¸ì˜¤ê¸°
                chunks = self.vector_retriever.get_case_chunks(doc_id)
                chunks_count = len(chunks)
                
                # ê²°ê³¼ì— ì¶”ê°€ ì •ë³´ í¬í•¨
                enriched_result = result.copy()
                enriched_result['raw_text'] = raw_text
                enriched_result['chunks_count'] = chunks_count
                enriched_result['doc_id'] = doc_id
                enriched_result['chunk_id'] = result.get('chunk_id') or result.get('chunk_uid')
                
                enriched_results.append(enriched_result)
            except Exception as e:
                print(f"âš ï¸  ë¬¸ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (doc_id: {doc_id}): {e}")
                # ì‹¤íŒ¨í•´ë„ ì›ë³¸ ê²°ê³¼ëŠ” í¬í•¨
                enriched_result = result.copy()
                enriched_result['raw_text'] = result.get('text', '')
                enriched_result['chunks_count'] = 1
                enriched_results.append(enriched_result)
        
        return enriched_results
    
    def _search_sequential(
        self,
        query: str,
        top_k: int = 10,
        min_threshold: int = 2
    ) -> List[Dict]:
        """
        ìˆœì°¨ ê²€ìƒ‰ ë¡œì§: ë¶„ìŸì¡°ì •ì‚¬ë¡€ â†’ ìƒë‹´ì‚¬ë¡€ â†’ ë²•ë ¹+ê¸°ì¤€
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ê° ë‹¨ê³„ë³„ ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            min_threshold: ìµœì†Œ ê²°ê³¼ ìˆ˜ ì„ê³„ê°’
        
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (raw_text í¬í•¨)
        """
        all_results = []
        
        # 1ìˆœìœ„: ë¶„ìŸì¡°ì •ì‚¬ë¡€
        print(f"[1ìˆœìœ„] ë¶„ìŸì¡°ì •ì‚¬ë¡€ ê²€ìƒ‰ ì¤‘...")
        mediation_results = self._search_by_doc_type(
            query=query,
            doc_type='mediation_case',
            top_k=top_k
        )
        print(f"  - ë¶„ìŸì¡°ì •ì‚¬ë¡€: {len(mediation_results)}ê±´")
        
        if len(mediation_results) >= min_threshold:
            print(f"  âœ… ë¶„ìŸì¡°ì •ì‚¬ë¡€ ì¶©ë¶„ ({len(mediation_results)}ê±´ >= {min_threshold}ê±´), ê²€ìƒ‰ ì¢…ë£Œ")
            return self._enrich_with_full_text(mediation_results)
        
        all_results.extend(mediation_results)
        
        # 2ìˆœìœ„: ìƒë‹´ì‚¬ë¡€
        print(f"[2ìˆœìœ„] ìƒë‹´ì‚¬ë¡€ ê²€ìƒ‰ ì¤‘...")
        counsel_results = self._search_by_doc_type(
            query=query,
            doc_type='counsel_case',
            top_k=top_k
        )
        print(f"  - ìƒë‹´ì‚¬ë¡€: {len(counsel_results)}ê±´")
        
        all_results.extend(counsel_results)
        if len(all_results) >= min_threshold:
            print(f"  âœ… ê²°ê³¼ ì¶©ë¶„ ({len(all_results)}ê±´ >= {min_threshold}ê±´), ê²€ìƒ‰ ì¢…ë£Œ")
            return self._enrich_with_full_text(all_results)
        
        # 3ìˆœìœ„: ë²•ë ¹ + ë¶„ìŸì¡°ì •ê¸°ì¤€
        print(f"[3ìˆœìœ„] ë²•ë ¹ ë° ë¶„ìŸì¡°ì •ê¸°ì¤€ ê²€ìƒ‰ ì¤‘...")
        law_results = self._search_by_doc_type(
            query=query,
            doc_type='law',
            top_k=top_k
        )
        print(f"  - ë²•ë ¹: {len(law_results)}ê±´")
        
        # ê¸°ì¤€ ê²€ìƒ‰ (criteria_* íƒ€ì…ë“¤)
        criteria_types = ['criteria_item', 'criteria_resolution', 'criteria_warranty', 'criteria_lifespan']
        criteria_results = []
        for criteria_type in criteria_types:
            try:
                results = self._search_by_doc_type(
                    query=query,
                    doc_type=criteria_type,
                    top_k=top_k // len(criteria_types)  # ê· ë“± ë¶„ë°°
                )
                criteria_results.extend(results)
            except Exception as e:
                print(f"  âš ï¸  {criteria_type} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        print(f"  - ë¶„ìŸì¡°ì •ê¸°ì¤€: {len(criteria_results)}ê±´")
        
        all_results.extend(law_results)
        all_results.extend(criteria_results)
        
        print(f"  âœ… ì „ì²´ ê²€ìƒ‰ ì™„ë£Œ: ì´ {len(all_results)}ê±´")
        return self._enrich_with_full_text(all_results)
    
    def test_query(
        self,
        query: str,
        top_k: int = 10,
        show_details: bool = False
    ):
        """
        ë‹¨ì¼ ì¿¼ë¦¬ì— ëŒ€í•´ ìˆœì°¨ ê²€ìƒ‰ ë° ë¬¸ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ë°˜í™˜ í…ŒìŠ¤íŠ¸
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ê° ë‹¨ê³„ë³„ ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            show_details: ìƒì„¸ ê²°ê³¼ ì¶œë ¥ ì—¬ë¶€
        """
        print("\n" + "="*80)
        print(f"ğŸ“ ê²€ìƒ‰ ì¿¼ë¦¬: {query}")
        print("="*80)
        
        print(f"\nğŸ” ìˆœì°¨ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
        print(f"ğŸ“Š ê° ë‹¨ê³„ë³„ Top-{top_k} ê²°ê³¼ ê²€ìƒ‰\n")
        
        try:
            # ìˆœì°¨ ê²€ìƒ‰ ì‹¤í–‰
            sequential_results = self._search_sequential(
                query=query,
                top_k=top_k,
                min_threshold=2
            )
            
            print(f"\nâœ… ìˆœì°¨ ê²€ìƒ‰ ì™„ë£Œ: ì´ {len(sequential_results)}ê±´")
            
            # ìƒì„¸ ê²°ê³¼ ì¶œë ¥ (ì„ íƒì‚¬í•­)
            if show_details:
                self._print_sequential_results(sequential_results)
            
            # ê²°ê³¼ë¥¼ MultiMethodRetriever í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (LLM ë¶„ì„ìš©)
            method_results = self._convert_to_method_results(sequential_results, query)
            
            # LLM ë¹„êµ ë¶„ì„
            print("\n" + "="*80)
            print("ğŸ¤– LLM ë¹„êµ ë¶„ì„ ì¤‘...")
            print("="*80 + "\n")
            
            analysis_result = self.generator.generate_comparative_answer(
                query=query,
                method_results=method_results,
                temperature=0.3,
                max_tokens=1500
            )
            
            # LLM ë¶„ì„ ê²°ê³¼ ì¶œë ¥
            print(analysis_result['answer'])
            
            # ë©”íƒ€ë°ì´í„° ì¶œë ¥
            print("\n" + "-"*80)
            print("ğŸ“Š ê²€ìƒ‰ í†µê³„:")
            print(f"  - ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(sequential_results)}ê°œ")
            if sequential_results:
                sources = {}
                for result in sequential_results:
                    source = result.get('source', 'unknown')
                    sources[source] = sources.get(source, 0) + 1
                print(f"  - ë¬¸ì„œ ìœ í˜•ë³„ ë¶„í¬:")
                for source, count in sources.items():
                    print(f"    - {source}: {count}ê°œ")
            print(f"  - ì‚¬ìš©ëœ í† í° ìˆ˜: {analysis_result['usage']['total_tokens']:,}ê°œ")
            print(f"    (í”„ë¡¬í”„íŠ¸: {analysis_result['usage']['prompt_tokens']:,}, ìƒì„±: {analysis_result['usage']['completion_tokens']:,})")
            print("-"*80)
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            saved_path = self._save_sequential_results_to_json(query, sequential_results)
            if saved_path:
                print(f"\nğŸ’¾ ê²€ìƒ‰ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {saved_path}")
            
            return {
                'query': query,
                'sequential_results': sequential_results,
                'method_results': method_results,
                'analysis': analysis_result
            }
            
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _print_search_summary(self, method_results: dict):
        """ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("ğŸ“ˆ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½:")
        print("-"*80)
        
        for method_name, method_data in method_results.get('methods', {}).items():
            if method_data.get('success', False):
                count = method_data.get('count', 0)
                elapsed = method_data.get('elapsed_time', 0)
                print(f"  âœ… {method_name.upper():8s}: {count:3d}ê°œ ê²°ê³¼ ({elapsed*1000:6.1f}ms)")
            else:
                error = method_data.get('error', 'Unknown error')
                print(f"  âŒ {method_name.upper():8s}: ì‹¤íŒ¨ - {error}")
        
        print("-"*80)
    
    def _print_detailed_results(self, method_results: dict):
        """ê° ê²€ìƒ‰ ë°©ë²•ë³„ ìƒì„¸ ê²°ê³¼ ì¶œë ¥"""
        print("\nğŸ“‹ ìƒì„¸ ê²€ìƒ‰ ê²°ê³¼:")
        print("="*80)
        
        for method_name, method_data in method_results.get('methods', {}).items():
            if not method_data.get('success', False):
                continue
            
            results = method_data.get('results', [])
            if not results:
                print(f"\n[{method_name.upper()}] ê²°ê³¼ ì—†ìŒ")
                continue
            
            print(f"\n[{method_name.upper()}] ìƒìœ„ {len(results)}ê°œ ê²°ê³¼:")
            print("-"*80)
            
            for idx, result in enumerate(results[:5], 1):  # ìƒìœ„ 5ê°œë§Œ ì¶œë ¥
                print(f"\n{idx}. ì ìˆ˜: {result.get('score', 0):.4f}")
                print(f"   ì¶œì²˜: {result.get('source', 'N/A')}")
                if result.get('case_no'):
                    print(f"   ì‚¬ê±´ë²ˆí˜¸: {result.get('case_no')}")
                if result.get('agency'):
                    print(f"   ê¸°ê´€: {result.get('agency')}")
                if result.get('decision_date'):
                    print(f"   ê²°ì •ì¼ì: {result.get('decision_date')}")
                
                text = result.get('text', '') or result.get('content', '')
                if text:
                    preview = text[:200] + "..." if len(text) > 200 else text
                    print(f"   ë‚´ìš©: {preview}")
    
    def _print_sequential_results(self, results: List[Dict]):
        """ìˆœì°¨ ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ì¶œë ¥"""
        print("\nğŸ“‹ ìˆœì°¨ ê²€ìƒ‰ ìƒì„¸ ê²°ê³¼:")
        print("="*80)
        
        for idx, result in enumerate(results[:10], 1):  # ìƒìœ„ 10ê°œë§Œ ì¶œë ¥
            print(f"\n{idx}. ìœ ì‚¬ë„: {result.get('similarity', 0):.4f}")
            print(f"   ì¶œì²˜: {result.get('source', 'N/A')}")
            print(f"   ë¬¸ì„œ ID: {result.get('doc_id', 'N/A')}")
            if result.get('case_no'):
                print(f"   ì‚¬ê±´ë²ˆí˜¸: {result.get('case_no')}")
            if result.get('agency'):
                print(f"   ê¸°ê´€: {result.get('agency')}")
            if result.get('decision_date'):
                print(f"   ê²°ì •ì¼ì: {result.get('decision_date')}")
            if result.get('chunks_count'):
                print(f"   ì²­í¬ ìˆ˜: {result.get('chunks_count')}ê°œ")
            
            raw_text = result.get('raw_text', '')
            if raw_text:
                preview = raw_text[:300] + "..." if len(raw_text) > 300 else raw_text
                print(f"   ì „ì²´ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {preview}")
    
    def _convert_to_method_results(self, sequential_results: List[Dict], query: str) -> Dict:
        """
        ìˆœì°¨ ê²€ìƒ‰ ê²°ê³¼ë¥¼ MultiMethodRetriever í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        Args:
            sequential_results: ìˆœì°¨ ê²€ìƒ‰ ê²°ê³¼
            query: ê²€ìƒ‰ ì¿¼ë¦¬
        
        Returns:
            MultiMethodRetriever í˜•ì‹ì˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # cosine ë°©ë²•ìœ¼ë¡œ ë³€í™˜ (ìˆœì°¨ ê²€ìƒ‰ì€ cosine similarity ê¸°ë°˜)
        normalized_results = []
        for result in sequential_results:
            normalized = {
                'chunk_id': result.get('chunk_id'),
                'doc_id': result.get('doc_id'),
                'text': result.get('raw_text', result.get('text', '')),
                'chunk_type': result.get('chunk_type'),
                'source': result.get('source'),
                'agency': result.get('agency'),
                'case_no': result.get('case_no'),
                'decision_date': result.get('decision_date'),
                'method': 'cosine',
                'score': result.get('similarity', 0.0)
            }
            normalized_results.append(normalized)
        
        return {
            'query': query,
            'methods': {
                'cosine': {
                    'method': 'cosine',
                    'results': normalized_results,
                    'count': len(normalized_results),
                    'elapsed_time': 0.0,
                    'success': True
                }
            },
            'total_methods': 1,
            'successful_methods': 1
        }
    
    def _sanitize_filename(self, text: str, max_length: int = 20) -> str:
        """
        íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í…ìŠ¤íŠ¸ ì •ë¦¬
        
        Args:
            text: ì •ë¦¬í•  í…ìŠ¤íŠ¸
            max_length: ìµœëŒ€ ê¸¸ì´
        
        Returns:
            ì •ë¦¬ëœ íŒŒì¼ëª… ë¬¸ìì—´
        """
        # í•œê¸€, ì˜ë¬¸, ìˆ«ìëŠ” ìœ ì§€, ë‚˜ë¨¸ì§€ëŠ” ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜
        sanitized = re.sub(r'[^\wê°€-í£]', '_', text)
        # ì—°ì†ëœ ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ í•˜ë‚˜ë¡œ
        sanitized = re.sub(r'_+', '_', sanitized)
        # ì•ë’¤ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
        sanitized = sanitized.strip('_')
        # ê¸¸ì´ ì œí•œ
        if len(sanitized) > max_length:
            sanitized = sanitized[:max_length]
        return sanitized
    
    def _save_sequential_results_to_json(self, query: str, sequential_results: List[Dict]) -> Optional[Path]:
        """
        ìˆœì°¨ ê²€ìƒ‰ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            sequential_results: ìˆœì°¨ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ (ì‹¤íŒ¨ ì‹œ None)
        """
        try:
            # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
            now = datetime.now()
            timestamp = now.strftime('%Y%m%d_%H%M%S')
            
            # ì¿¼ë¦¬ì—ì„œ íŒŒì¼ëª… ìƒì„± (ìµœëŒ€ 20ì)
            query_part = self._sanitize_filename(query, max_length=20)
            if not query_part:
                query_part = 'query'
            
            # íŒŒì¼ëª… ìƒì„±
            filename = f"rag_test_{timestamp}_{query_part}.json"
            filepath = self.logs_dir / filename
            
            # ì €ì¥í•  ë°ì´í„° êµ¬ì¡°í™”
            save_data = {
                'query': query,
                'timestamp': now.isoformat(),
                'search_type': 'sequential',
                'results': []
            }
            
            # ê° ê²°ê³¼ ì €ì¥
            for result in sequential_results:
                result_data = {
                    'chunk_id': result.get('chunk_id'),
                    'doc_id': result.get('doc_id'),
                    'raw_text': result.get('raw_text', ''),
                    'score': result.get('similarity', 0.0),
                    'source': result.get('source'),
                    'agency': result.get('agency'),
                    'case_no': result.get('case_no'),
                    'decision_date': result.get('decision_date'),
                    'chunk_type': result.get('chunk_type'),
                    'chunks_count': result.get('chunks_count', 0)
                }
                save_data['results'].append(result_data)
            
            # í†µê³„ ì •ë³´ ì¶”ê°€
            save_data['statistics'] = {
                'total_results': len(sequential_results),
                'sources': {}
            }
            
            for result in sequential_results:
                source = result.get('source', 'unknown')
                save_data['statistics']['sources'][source] = save_data['statistics']['sources'].get(source, 0) + 1
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            return filepath
            
        except Exception as e:
            print(f"âš ï¸  JSON ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
    
    def _save_results_to_json(self, query: str, method_results: Dict) -> Optional[Path]:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            method_results: ê²€ìƒ‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ (ì‹¤íŒ¨ ì‹œ None)
        """
        try:
            # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
            now = datetime.now()
            timestamp = now.strftime('%Y%m%d_%H%M%S')
            
            # ì¿¼ë¦¬ì—ì„œ íŒŒì¼ëª… ìƒì„± (ìµœëŒ€ 20ì)
            query_part = self._sanitize_filename(query, max_length=20)
            if not query_part:
                query_part = 'query'
            
            # íŒŒì¼ëª… ìƒì„±
            filename = f"rag_test_{timestamp}_{query_part}.json"
            filepath = self.logs_dir / filename
            
            # ì €ì¥í•  ë°ì´í„° êµ¬ì¡°í™”
            save_data = {
                'query': query,
                'timestamp': now.isoformat(),
                'search_methods': {}
            }
            
            # ê° ê²€ìƒ‰ ë°©ë²•ë³„ ê²°ê³¼ ì €ì¥
            for method_name, method_data in method_results.get('methods', {}).items():
                save_data['search_methods'][method_name] = {
                    'success': method_data.get('success', False),
                    'count': method_data.get('count', 0),
                    'elapsed_time': method_data.get('elapsed_time', 0),
                    'results': []
                }
                
                if method_data.get('success', False):
                    # ê° ê²°ê³¼ ì €ì¥ (í•„ìš”í•œ í•„ë“œë§Œ)
                    for result in method_data.get('results', []):
                        result_data = {
                            'chunk_id': result.get('chunk_id'),
                            'doc_id': result.get('doc_id'),
                            'text': result.get('text') or result.get('content', ''),
                            'score': result.get('score', 0.0),
                            'source': result.get('source'),
                            'agency': result.get('agency'),
                            'case_no': result.get('case_no'),
                            'decision_date': result.get('decision_date'),
                            'chunk_type': result.get('chunk_type')
                        }
                        save_data['search_methods'][method_name]['results'].append(result_data)
                else:
                    save_data['search_methods'][method_name]['error'] = method_data.get('error', 'Unknown error')
            
            # í†µê³„ ì •ë³´ ì¶”ê°€
            save_data['statistics'] = {
                'total_results': sum(
                    m.get('count', 0) 
                    for m in method_results.get('methods', {}).values()
                ),
                'successful_methods': sum(
                    1 for m in method_results.get('methods', {}).values()
                    if m.get('success', False)
                ),
                'total_methods': len(method_results.get('methods', {}))
            }
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            return filepath
            
        except Exception as e:
            print(f"âš ï¸  JSON ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
    
    def interactive_mode(self):
        """ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ"""
        print("\n" + "="*80)
        print("ğŸ¯ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ (ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ)")
        print("="*80)
        print("\nì¢…ë£Œí•˜ë ¤ë©´ 'quit', 'exit', ë˜ëŠ” 'q'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        print("ìƒì„¸ ê²°ê³¼ë¥¼ ë³´ë ¤ë©´ ì¿¼ë¦¬ ì•ì— '--detail' ë˜ëŠ” '-d'ë¥¼ ë¶™ì´ì„¸ìš”.")
        print("ì˜ˆ: --detail ëƒ‰ì¥ê³  í™˜ë¶ˆ ì‚¬ë¡€\n")
        
        while True:
            try:
                user_input = input("\nê²€ìƒ‰ ì¿¼ë¦¬ ì…ë ¥: ").strip()
                
                if user_input.lower() in ('quit', 'exit', 'q'):
                    print("\nğŸ‘‹ í…ŒìŠ¤íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                if not user_input:
                    continue
                
                # ìƒì„¸ ëª¨ë“œ í™•ì¸
                show_details = False
                if user_input.startswith('--detail') or user_input.startswith('-d'):
                    show_details = True
                    query = user_input.replace('--detail', '').replace('-d', '').strip()
                else:
                    query = user_input
                
                if not query:
                    print("âš ï¸  ì¿¼ë¦¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue
                
                # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
                self.test_query(query, top_k=10, show_details=show_details)
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ í…ŒìŠ¤íŠ¸ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.retriever:
            self.retriever.close()
        if self.vector_retriever:
            self.vector_retriever.close()
        print("\nâœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='ë¡œì»¬ í™˜ê²½ìš© RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ
  python backend/scripts/test_rag_local_simple.py
  
  # ë‹¨ì¼ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
  python backend/scripts/test_rag_local_simple.py --query "ëƒ‰ì¥ê³  í™˜ë¶ˆ ì‚¬ë¡€"
  
  # ìƒì„¸ ê²°ê³¼ í¬í•¨
  python backend/scripts/test_rag_local_simple.py --query "ëƒ‰ì¥ê³  í™˜ë¶ˆ ì‚¬ë¡€" --detail
        """
    )
    
    parser.add_argument(
        '--query', '-q',
        type=str,
        help='ê²€ìƒ‰í•  ì¿¼ë¦¬ (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ)'
    )
    
    parser.add_argument(
        '--detail', '-d',
        action='store_true',
        help='ìƒì„¸ ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥'
    )
    
    parser.add_argument(
        '--top-k',
        type=int,
        default=10,
        help='ê° ë°©ë²•ë³„ ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 10)'
    )
    
    args = parser.parse_args()
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
    db_config = {
        'host': os.getenv('DB_HOST', 'localhost'),
        'port': int(os.getenv('DB_PORT', 5432)),
        'database': os.getenv('DB_NAME', 'ddoksori'),
        'user': os.getenv('DB_USER', 'postgres'),
        'password': os.getenv('DB_PASSWORD', 'postgres')
    }
    
    # í…ŒìŠ¤í„° ì´ˆê¸°í™”
    tester = SimpleRAGTester(db_config)
    
    try:
        # ì´ˆê¸°í™”
        tester.initialize()
        
        if args.query:
            # ë‹¨ì¼ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
            tester.test_query(
                query=args.query,
                top_k=args.top_k,
                show_details=args.detail
            )
        else:
            # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ
            tester.interactive_mode()
    
    except KeyboardInterrupt:
        print("\n\ní…ŒìŠ¤íŠ¸ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    finally:
        tester.close()


if __name__ == "__main__":
    main()
