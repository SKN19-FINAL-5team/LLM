#!/usr/bin/env python3
"""
ë¶„ìŸì¡°ì •ê¸°ì¤€ ë°ì´í„° ë¡œë”© ìŠ¤í¬ë¦½íŠ¸ (S1-D3)
- ë³„í‘œ1~4, ì§€ì¹¨ ë°ì´í„°ë¥¼ JSONLì—ì„œ PostgreSQLë¡œ ë¡œë“œ
- Dual storage: criteria_units (êµ¬ì¡°í™”) + documents/chunks (RAG ê²€ìƒ‰)
"""

import os
import json
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime
import psycopg2
from psycopg2.extras import execute_batch

# í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’
DB_CONFIG = {
    'host': os.getenv('DB_HOST', 'localhost'),
    'port': int(os.getenv('DB_PORT', 5432)),
    'database': os.getenv('DB_NAME', 'ddoksori'),
    'user': os.getenv('DB_USER', 'postgres'),
    'password': os.getenv('DB_PASSWORD', 'postgres')
}

# JSONL íŒŒì¼ ë§¤í•‘
CRITERIA_FILES = {
    'table1': 'consumer_dispute_resolution_criteria_table1_items.jsonl',
    'table2': 'consumer_dispute_resolution_criteria_table2_resolutions.jsonl',
    'table3': 'consumer_dispute_resolution_criteria_table3_warranty.jsonl',
    'table4': 'consumer_dispute_resolution_criteria_table4_lifespan.jsonl',
    'ecommerce_guideline': 'ecommerce_guideline.jsonl',
    'content_guideline': 'content_guideline.jsonl',
}


def compute_md5(text: str) -> str:
    """í…ìŠ¤íŠ¸ì˜ MD5 í•´ì‹œ ê³„ì‚°"""
    return hashlib.md5(text.encode('utf-8')).hexdigest()


def parse_jsonl_line(line: str, source_id: str, line_no: int) -> Optional[Dict[str, Any]]:
    """JSONL ë¼ì¸ íŒŒì‹±"""
    try:
        data = json.loads(line.strip())

        # unit_id ìƒì„±: source_id:record_type:line_no
        record_type = data.get('record_type', 'unknown')
        unit_id = f"{source_id}:{record_type}:{line_no:06d}"

        # unit_text ìƒì„± (ëŒ€í‘œ í…ìŠ¤íŠ¸)
        text_data = data.get('text', {})
        unit_text = text_data.get('normalized') or text_data.get('raw', '')

        if not unit_text:
            print(f"  âš ï¸ Warning: Empty unit_text for {unit_id}, skipping")
            return None

        # ê³„ì¸µ ì •ë³´ ì¶”ì¶œ (payloadì—ì„œ)
        payload = data.get('payload', {})
        category = payload.get('category')
        industry = payload.get('industry')
        item_group = payload.get('item_group')
        item = payload.get('item')
        dispute_type = payload.get('dispute_type')

        # search_stage ê²°ì • (table1ì€ stage1, ë‚˜ë¨¸ì§€ëŠ” stage2)
        search_stage = 'stage1' if source_id == 'table1' else 'stage2'

        # path_hint ìƒì„±
        path_parts = []
        if category:
            path_parts.append(category)
        if industry:
            path_parts.append(industry)
        if item_group:
            path_parts.append(item_group)
        path_hint = ' > '.join(path_parts) if path_parts else None

        return {
            'unit_id': unit_id,
            'source_id': source_id,
            'record_type': record_type,
            'unit_type': data.get('unit_type'),
            'path_hint': path_hint,
            'unit_text': unit_text,
            'content_md5': compute_md5(unit_text),
            'doc': json.dumps(data, ensure_ascii=False),  # ì›ë³¸ JSON ì „ì²´ ì €ì¥
            'category': category,
            'industry': industry,
            'item_group': item_group,
            'item': item,
            'dispute_type': dispute_type,
            'search_stage': search_stage,
        }
    except json.JSONDecodeError as e:
        print(f"  âŒ JSON parsing error at line {line_no}: {e}")
        return None
    except Exception as e:
        print(f"  âŒ Unexpected error at line {line_no}: {e}")
        return None


def load_criteria_units(conn, source_id: str, jsonl_path: Path) -> int:
    """criteria_units í…Œì´ë¸”ì— ë°ì´í„° ë¡œë“œ"""
    cursor = conn.cursor()

    # JSONL íŒŒì¼ ì½ê¸°
    units = []
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line_no, line in enumerate(f, start=1):
            if not line.strip():
                continue

            unit_data = parse_jsonl_line(line, source_id, line_no)
            if unit_data:
                units.append(unit_data)

    if not units:
        print(f"  âš ï¸ No valid units found in {jsonl_path.name}")
        return 0

    # ë°ì´í„°ë² ì´ìŠ¤ì— ì‚½ì…
    insert_sql = """
    INSERT INTO criteria_units (
        unit_id, source_id, record_type, unit_type, path_hint,
        unit_text, content_md5, doc, embedding,
        category, industry, item_group, item, dispute_type, search_stage
    ) VALUES (
        %(unit_id)s, %(source_id)s, %(record_type)s, %(unit_type)s, %(path_hint)s,
        %(unit_text)s, %(content_md5)s, %(doc)s::jsonb, NULL,
        %(category)s, %(industry)s, %(item_group)s, %(item)s, %(dispute_type)s, %(search_stage)s
    )
    ON CONFLICT (unit_id) DO UPDATE SET
        unit_text = EXCLUDED.unit_text,
        content_md5 = EXCLUDED.content_md5,
        doc = EXCLUDED.doc,
        category = EXCLUDED.category,
        industry = EXCLUDED.industry,
        item_group = EXCLUDED.item_group,
        item = EXCLUDED.item,
        dispute_type = EXCLUDED.dispute_type,
        updated_at = NOW()
    """

    execute_batch(cursor, insert_sql, units, page_size=100)
    conn.commit()

    print(f"  âœ… Loaded {len(units)} units from {jsonl_path.name}")
    return len(units)


def load_to_documents_chunks(conn, source_id: str, source_label: str, jsonl_path: Path) -> int:
    """
    documents + chunks í…Œì´ë¸”ì—ë„ ë°ì´í„° ë¡œë“œ (RAG í†µí•© ê²€ìƒ‰ìš©)
    - doc_type: 'criteria_{source_id}'
    - chunk_type: record_type ê°’ ì‚¬ìš©
    """
    cursor = conn.cursor()

    # 1. documents í…Œì´ë¸”ì— ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì‚½ì…
    doc_id = f"criteria_{source_id}"
    cursor.execute("""
        INSERT INTO documents (
            doc_id, doc_type, title, source_org, category_path, url, metadata
        ) VALUES (
            %s, %s, %s, %s, %s, %s, %s::jsonb
        )
        ON CONFLICT (doc_id) DO UPDATE SET
            title = EXCLUDED.title,
            metadata = EXCLUDED.metadata,
            updated_at = NOW()
    """, (
        doc_id,
        f'criteria_{source_id}',
        source_label,
        'consumer.go.kr',
        ['ë¶„ìŸì¡°ì •ê¸°ì¤€', source_label],
        None,
        json.dumps({'source_id': source_id, 'source_label': source_label})
    ))

    # 2. chunks í…Œì´ë¸”ì— ì²­í¬ ì‚½ì…
    chunks = []
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line_no, line in enumerate(f, start=1):
            if not line.strip():
                continue

            try:
                data = json.loads(line.strip())
                text_data = data.get('text', {})
                unit_text = text_data.get('normalized') or text_data.get('raw', '')

                if not unit_text:
                    continue

                record_type = data.get('record_type', 'unknown')
                chunk_id = f"{doc_id}:{record_type}:{line_no:06d}"

                chunks.append({
                    'chunk_id': chunk_id,
                    'doc_id': doc_id,
                    'chunk_index': line_no - 1,  # 0-based
                    'chunk_type': record_type,
                    'content': unit_text,
                    'content_length': len(unit_text),
                })
            except Exception as e:
                print(f"  âš ï¸ Error parsing line {line_no} for chunks: {e}")
                continue

    if not chunks:
        print(f"  âš ï¸ No chunks created from {jsonl_path.name}")
        return 0

    # chunk_total ì„¤ì •
    chunk_total = len(chunks)
    for chunk in chunks:
        chunk['chunk_total'] = chunk_total

    # ì‚½ì…
    insert_sql = """
    INSERT INTO chunks (
        chunk_id, doc_id, chunk_index, chunk_total, chunk_type,
        content, content_length, embedding, drop
    ) VALUES (
        %(chunk_id)s, %(doc_id)s, %(chunk_index)s, %(chunk_total)s, %(chunk_type)s,
        %(content)s, %(content_length)s, NULL, FALSE
    )
    ON CONFLICT (chunk_id) DO UPDATE SET
        content = EXCLUDED.content,
        content_length = EXCLUDED.content_length,
        updated_at = NOW()
    """

    execute_batch(cursor, insert_sql, chunks, page_size=100)
    conn.commit()

    print(f"  âœ… Loaded {len(chunks)} chunks to documents/chunks tables")
    return len(chunks)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("S1-D3: ë¶„ìŸì¡°ì •ê¸°ì¤€ ë°ì´í„° ë¡œë”©")
    print("=" * 60)

    # ë°ì´í„° ë””ë ‰í† ë¦¬
    script_dir = Path(__file__).parent
    # backend/scripts/data_loading -> backend/data/criteria/jsonl
    data_dir = script_dir.parent.parent / 'data' / 'criteria' / 'jsonl'

    if not data_dir.exists():
        print(f"âŒ Error: Data directory not found: {data_dir}")
        return

    print(f"\nData directory: {data_dir}")
    print(f"DB: {DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}")

    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        print("âœ… Database connected")
    except Exception as e:
        print(f"âŒ Database connection failed: {e}")
        return

    try:
        cursor = conn.cursor()

        # criteria í…Œì´ë¸” ë°ì´í„° í™•ì¸
        cursor.execute("SELECT source_id, source_label FROM criteria ORDER BY source_id")
        criteria_sources = cursor.fetchall()
        print(f"\nğŸ“Š Available criteria sources ({len(criteria_sources)}):")
        for source_id, source_label in criteria_sources:
            print(f"  - {source_id}: {source_label}")

        # ê° ì†ŒìŠ¤ë³„ JSONL íŒŒì¼ ë¡œë“œ
        total_units = 0
        total_chunks = 0

        for source_id, filename in CRITERIA_FILES.items():
            jsonl_path = data_dir / filename

            print(f"\nğŸ“‚ Processing: {source_id} ({filename})")

            if not jsonl_path.exists():
                print(f"  âš ï¸ File not found: {jsonl_path}, skipping")
                continue

            # criteria í…Œì´ë¸”ì—ì„œ source_label ê°€ì ¸ì˜¤ê¸°
            cursor.execute("SELECT source_label FROM criteria WHERE source_id = %s", (source_id,))
            result = cursor.fetchone()
            source_label = result[0] if result else source_id

            # 1. criteria_units í…Œì´ë¸”ì— ë¡œë“œ
            units_count = load_criteria_units(conn, source_id, jsonl_path)
            total_units += units_count

            # 2. documents + chunks í…Œì´ë¸”ì—ë„ ë¡œë“œ (RAG í†µí•© ê²€ìƒ‰)
            chunks_count = load_to_documents_chunks(conn, source_id, source_label, jsonl_path)
            total_chunks += chunks_count

        # ê²°ê³¼ ìš”ì•½
        print("\n" + "=" * 60)
        print("ğŸ“Š Loading Summary")
        print("=" * 60)

        cursor.execute("SELECT source_id, COUNT(*) FROM criteria_units GROUP BY source_id ORDER BY source_id")
        print("\n[criteria_units table]")
        for source_id, count in cursor.fetchall():
            print(f"  {source_id}: {count} units")

        cursor.execute("""
            SELECT d.doc_type, COUNT(c.*)
            FROM chunks c
            JOIN documents d ON c.doc_id = d.doc_id
            WHERE d.doc_type LIKE 'criteria_%'
            GROUP BY d.doc_type
            ORDER BY d.doc_type
        """)
        print("\n[chunks table]")
        for doc_type, count in cursor.fetchall():
            print(f"  {doc_type}: {count} chunks")

        print(f"\nâœ… Total loaded: {total_units} units, {total_chunks} chunks")
        print("=" * 60)

    except Exception as e:
        print(f"\nâŒ Error during processing: {e}")
        conn.rollback()
    finally:
        conn.close()
        print("\nğŸ”Œ Database connection closed")


if __name__ == '__main__':
    main()
