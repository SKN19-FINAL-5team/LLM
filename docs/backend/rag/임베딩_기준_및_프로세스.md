#    

## 1. 

### 1.1  
  ddoksori     (embedding)  ** **   .   ,  ,          .

### 1.2 ?
        .        ,   (semantic search)  .

### 1.3   
ddoksori RAG(Retrieval-Augmented Generation)  :
-        
- , ,       
-      

---

## 2.    

### 2.1  
****: `nlpai-lab/KURE-v1` (Korean Universal Representation Embedding v1)

** **:
-    
-      
- Sentence-BERT  

### 2.2  
****: 1024  (dense vector)

### 2.3   
```bash
# backend/.env
EMBEDDING_MODEL=nlpai-lab/KURE-v1
EMBEDDING_DIMENSION=1024
EMBED_API_URL=http://localhost:8001/embed  #  API  URL
```

### 2.4   
```python
from sentence_transformers import SentenceTransformer

# GPU     
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = SentenceTransformer('nlpai-lab/KURE-v1', device=device)
```

---

## 3.   

###  : "   ?"

####  
1. ** **:     
2. ** **:  `content`  `text`  ( )
3. ** **: `chunk_type`    
4. ** **:      

### 3.1  
- ****:   (chunk)   
- ** **: (chunk_id, doc_id, chunk_type )     DB 
- ** **:      

### 3.2    

#### A.   (Law)
**  **: `content`

****:
```
[] {}
[] {}

{ }
```

****:
```
[] 
[] 750

             .
```

** **: `backend/data/law/Civil_Law_chunks.jsonl`

#### B.  (Counsel Case)
**  **: `text`

** **:
- `qa_combined`:     
- `question`:  
- `answer`:  

****:
```json
{
  "chunk_id": "ecmc:counsel:case123:qa_combined:0",
  "text": "Q:       ...\nA:  ...",
  "chunk_type": "qa_combined"
}
```

#### C.  (Mediation Case)
**  **: `text`

** **:
- `case_overview`:  
- `claim_consumer`:  
- `claim_business`:  
- `decision`:   
- `order`: 
- `other`: 

****:
```json
{
  "chunk_id": "kca:mediation:ADR2024-001:case_overview:0",
  "text": " 2024 1  ...",
  "chunk_type": "case_overview"
}
```

** **: `backend/data/dispute_resolution/kca_final_rag_chunks_normalized.jsonl`

### 3.3   

1. **  **:  (`chunk_type`)    
2. **  **:  (,  )   ,    
3. ** **:        
4. **  **: `chunk_index` `chunk_total`      

---

## 4.   

### 4.1  

```
[ ] → [ ] → [ ] → [ ] → [DB ]
```

** **:
1. JSONL    
2.  (`content`  `text` ) 
3.  (32  64)  
4.    API   
5. PostgreSQL + pgvector 

### 4.2  

#### A. `embed_pipeline_v2.py` ( )
****: `backend/scripts/embed_pipeline_v2.py`

****:
- , ,    
- PostgreSQL + pgvector  
-   (next/prev)  

** **:
```python
def embed_and_insert_chunks(chunks, batch_size=32):
    for i in range(0, len(chunks), batch_size):
        batch_chunks = chunks[i:i+batch_size]
        texts = [chunk['content'] for chunk in batch_chunks]

        #  API 
        response = requests.post(
            EMBED_API_URL,
            json={"texts": texts},
            timeout=60
        )
        embeddings = response.json()['embeddings']

        # DB 
        for idx, chunk in enumerate(batch_chunks):
            cursor.execute("""
                INSERT INTO chunks (
                    chunk_id, doc_id, chunk_index, chunk_total,
                    chunk_type, content, embedding, embedding_model
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            """, (
                chunk['chunk_id'],
                chunk['doc_id'],
                chunk['chunk_index'],
                chunk['chunk_total'],
                chunk['chunk_type'],
                chunk['content'],
                embeddings[idx],
                'KURE-v1'
            ))
```

#### B. `embed_data.py` ( )
****: `backend/scripts/embedding/embed_data.py` (, embed_data_remote.py )

****:
-  SentenceTransformer   
- GPU     GPU 
-  : 64

** **:
```python
model = SentenceTransformer('nlpai-lab/KURE-v1', device='cuda'|'cpu')
embeddings = model.encode(texts, convert_to_numpy=True)
```

#### C. `embed_data_remote.py` ( GPU)
****: `backend/scripts/embedding/embed_data_remote.py`

****:
- RunPod   GPU   
- REST API   
-  : 64

### 4.3  API 

#### `runpod_embed_server.py`
****: `backend/runpod_embed_server.py`

****: FastAPI   

****:

```python
# POST /embed -      
@app.post("/embed")
async def embed_texts(request: EmbedRequest):
    embeddings = model.encode(request.texts, convert_to_numpy=True)
    return {"embeddings": embeddings.tolist()}

# GET / -    
@app.get("/")
async def health_check():
    return {
        "status": "healthy",
        "model": "nlpai-lab/KURE-v1",
        "dimension": 1024
    }
```

** **:
```bash
curl -X POST http://localhost:8001/embed \
  -H "Content-Type: application/json" \
  -d '{"texts": ["1", "2"]}'
```

### 4.4   

|  |   |  |
|---------|----------|---------|
| `embed_pipeline_v2.py` | 32 | 60 |
| `embed_data.py` | 64 | - |
| `embed_data_remote.py` | 64 | 60 |

**   **:
- GPU  
-   ( API )
-     

---

## 5.   

### 5.1  

****: PostgreSQL 15 + pgvector 
** **: `backend/database/schema_v2_final.sql`

#### chunks 
```sql
CREATE TABLE chunks (
    chunk_id VARCHAR(255) PRIMARY KEY,           --   ID
    doc_id VARCHAR(255) NOT NULL,                --   ID
    chunk_index INTEGER NOT NULL,                --     (0)
    chunk_total INTEGER NOT NULL,                --     
    chunk_type VARCHAR(50),                      --   ( )
    content TEXT NOT NULL,                       --   
    content_length INTEGER,                      --   ( )
    embedding vector(1024),                      -- 1024  
    embedding_model VARCHAR(50) DEFAULT 'KURE-v1', --  
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);
```

#### chunk_id 
```
{source}:{doc_type}:{doc_identifier}:{chunk_type}:{chunk_index}
```

****:
- `statute:law:civil_law:article:0` -   
- `kca:mediation:ADR2024-001:case_overview:0` -    
- `ecmc:counsel:case123:qa_combined:0` -   

### 5.2  

####  
```sql
CREATE INDEX idx_chunks_embedding ON chunks
USING ivfflat(embedding vector_cosine_ops)
WITH (lists = 100);
```

** **: IVFFlat (Inverted File Flat)
-    
-   (Approximate Nearest Neighbor) 
- `lists` :   (   )

** **: `vector_cosine_ops` -  
```
similarity = 1 - cosine_distance
cosine_distance = 1 - (A · B) / (||A|| × ||B||)
```

####  
```sql
CREATE INDEX idx_chunks_doc_id ON chunks(doc_id);      --   
CREATE INDEX idx_chunks_type ON chunks(chunk_type);    --   
```

### 5.3  

#### chunks  
- `chunk_id`:   
- `doc_id`:   ID (    )
- `chunk_index`:     (0 )
- `chunk_total`:     
- `chunk_type`:   (`case_overview`, `decision`, `qa_combined` )
- `embedding_model`:    (   )

#### documents  ()
```sql
CREATE TABLE documents (
    doc_id VARCHAR(255) PRIMARY KEY,
    doc_type VARCHAR(50) NOT NULL,     -- 'law', 'counsel_case', 'mediation_case'
    title TEXT,
    source_org VARCHAR(50),            -- 'KCA', 'ECMC', 'KCDRC', 'statute'
    metadata JSONB,                    --   (JSON )
    created_at TIMESTAMP DEFAULT NOW()
);
```

---

## 6.     

### 6.1  

```bash
# backend/.env

#   
EMBEDDING_MODEL=nlpai-lab/KURE-v1
EMBEDDING_DIMENSION=1024

#  API 
EMBED_API_URL=http://localhost:8001/embed

#  (pgvector)
DATABASE_HOST=localhost
DATABASE_PORT=5432
DATABASE_NAME=ddoksori
DATABASE_USER=postgres
DATABASE_PASSWORD=your_password
```

### 6.2   

####    
```python
# backend/scripts/__.py
class LawChunker:
    MIN_CHUNK_SIZE = 100      #    ( )
    MAX_CHUNK_SIZE = 800      #   
    OPTIMAL_CHUNK_SIZE = 400  #   
```

** **:
- (article)  
- (paragraph), (item)    
-    ,   

### 6.3   

####   vs  API

|  |   |  API |
|-----|----------|---------|
| **** | - GPU  <br>-    | - GPU   <br>-    |
| **** | -  GPU <br>-    | -  <br>- API    |
| ** ** | GPU    |    GPU  |
| **** | `embed_data.py` | `embed_data_remote.py` |

---

## 7.     

### 7.1  

#### `check_embedding_status.py`
****: `backend/scripts/check_embedding_status.py`

** **:
1. ** **:      
2. **   **:  
3. **  **: NULL , 0     
4. **  **:     
5. ** **:   

** **:
```bash
cd backend/scripts
python check_embedding_status.py
```

### 7.2 SQL  

####   
```sql
SELECT
    COUNT(*) as total_chunks,
    COUNT(embedding) as embedded_chunks,
    ROUND(COUNT(embedding)::NUMERIC / COUNT(*) * 100, 2) as completion_rate
FROM chunks;
```

####     
```sql
SELECT
    embedding_model,
    array_length(embedding::real[], 1) as dimension,
    COUNT(*) as count
FROM chunks
WHERE embedding IS NOT NULL
GROUP BY embedding_model, array_length(embedding::real[], 1);
```

####   
```sql
SELECT
    chunk_type,
    COUNT(*) as count,
    AVG(content_length) as avg_length,
    MIN(content_length) as min_length,
    MAX(content_length) as max_length
FROM chunks
GROUP BY chunk_type
ORDER BY count DESC;
```

####    (0  )
```sql
SELECT chunk_id, content_length
FROM chunks
WHERE embedding IS NOT NULL
  AND embedding = ARRAY_FILL(0::real, ARRAY[1024])::vector;
```

---

## 8. RAG  

### 8.1  

****: `backend/app/rag/retriever.py`

****:
```
[ ] → [ ] → [ ] → [ K ] → [LLM ]
```

#### 1:   
```python
def get_query_embedding(query: str) -> List[float]:
    model = SentenceTransformer('nlpai-lab/KURE-v1')
    embedding = model.encode(query, convert_to_numpy=True)
    return embedding.tolist()
```

#### 2:   
```python
def retrieve_similar_chunks(query_embedding: List[float], top_k: int = 5):
    cursor.execute("""
        SELECT
            c.*,
            d.*,
            1 - (c.embedding <=> %s::vector) AS similarity
        FROM chunks c
        JOIN documents d ON c.doc_id = d.doc_id
        WHERE c.drop = FALSE
        ORDER BY c.embedding <=> %s::vector
        LIMIT %s
    """, (query_embedding, query_embedding, top_k))

    return cursor.fetchall()
```

** **:
- `<=>`: pgvector   
- `1 - (embedding <=> query)`:     (0~1 )

#### 3: LLM  
```python
# backend/app/rag/generator.py
def generate_answer(query: str, retrieved_chunks: List[dict]):
    context = "\n\n".join([
        f"[{chunk['doc_type']}] {chunk['content']}"
        for chunk in retrieved_chunks
    ])

    prompt = f"""
        .

    :
    {context}

    : {query}
    """

    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )

    return response.choices[0].message.content
```

### 8.2   

####   ( 5)
```sql
SELECT
    chunk_id,
    content,
    1 - (embedding <=> '[0.123, 0.456, ...]'::vector) AS similarity
FROM chunks
ORDER BY embedding <=> '[0.123, 0.456, ...]'::vector
LIMIT 5;
```

####   
```sql
SELECT
    chunk_id,
    content,
    chunk_type,
    1 - (embedding <=> %s::vector) AS similarity
FROM chunks
WHERE chunk_type IN ('decision', 'case_overview')
ORDER BY embedding <=> %s::vector
LIMIT 5;
```

####   
```sql
SELECT
    c.chunk_id,
    c.content,
    d.doc_type,
    1 - (c.embedding <=> %s::vector) AS similarity
FROM chunks c
JOIN documents d ON c.doc_id = d.doc_id
WHERE d.doc_type = 'mediation_case'
ORDER BY c.embedding <=> %s::vector
LIMIT 5;
```

####   
```sql
SELECT
    chunk_id,
    content,
    1 - (embedding <=> %s::vector) AS similarity
FROM chunks
WHERE 1 - (embedding <=> %s::vector) >= 0.7  --  70% 
ORDER BY embedding <=> %s::vector
LIMIT 10;
```

---

## 9.  

### "   ?"

|  |  |
|-----|------|
| ** ** |      |
| ** ** | KURE-v1 ( , SentenceTransformer) |
| ** ** | 1024   |
| ** ** |  `content`  `text`  ( ) |
| ** ** | `chunk_type`     |
| ** ** | PostgreSQL + pgvector, IVFFlat  |
| ** ** |      |
| ** ** |   (32~64  ) |
| **API ** | FastAPI       |

###   

|   |    |    |  |
|-----------|----------------|--------------|------|
| **** | `content` | `article`, `paragraph` |    |
| **** | `text` | `qa_combined`, `question`, `answer` |  / |
| **** | `text` | `case_overview`, `decision`, `claim_*` |    |

###   

```
  (JSONL)
    ↓
  (content/text )
    ↓
  (32~64 )
    ↓
  (KURE-v1 )
    ↓
PostgreSQL  (pgvector)
    ↓
 (IVFFlat, )
    ↓
RAG  
```

---

## 10.   

###   
- `backend/scripts/embed_pipeline_v2.py` -   
- `backend/scripts/embedding/embed_data_remote.py` -  GPU  ( )
- `backend/scripts/check_embedding_status.py` -   

###  
- `backend/runpod_embed_server.py` - FastAPI  API 

### 
- `backend/database/schema_v2_final.sql` - PostgreSQL + pgvector 

### RAG 
- `backend/app/rag/retriever.py` -   
- `backend/app/rag/generator.py` - LLM  

###  
- `backend/.env` -    API 

###  
- `backend/data/law/Civil_Law_chunks.jsonl` -   
- `backend/data/dispute_resolution/kca_final_rag_chunks_normalized.jsonl` -  

---

##  
- [     .md](./%20%20%20%20%20.md)
- [RAG_SETUP_GUIDE.md](./RAG_SETUP_GUIDE.md)
- [PR4_DATA_SCHEMA.md](./PR4_DATA_SCHEMA.md)
