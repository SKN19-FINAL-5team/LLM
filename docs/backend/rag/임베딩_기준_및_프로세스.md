# 임베딩 기준 및 프로세스

## 1. 개요

### 1.1 문서 목적
이 문서는 ddoksori 프로젝트에서 청킹된 데이터에 대한 임베딩(embedding) 작업이 **어떤 기준**으로 진행되는지 체계적으로 설명합니다. 새로운 팀원의 온보딩, 시스템 유지보수, 그리고 임베딩 관련 이슈 해결 시 참고할 수 있도록 작성되었습니다.

### 1.2 임베딩이란?
임베딩은 텍스트를 고차원 벡터 공간의 수치형 벡터로 변환하는 과정입니다. 의미적으로 유사한 텍스트는 벡터 공간에서 가까운 위치에 배치되어, 의미 기반 검색(semantic search)을 가능하게 합니다.

### 1.3 시스템에서 임베딩의 역할
ddoksori RAG(Retrieval-Augmented Generation) 시스템에서 임베딩은:
- 사용자 쿼리와 의미적으로 유사한 문서 청크를 빠르게 검색
- 법령, 피해구제사례, 분쟁조정사례 등 다양한 법률 문서의 통합 검색
- 코사인 유사도 기반의 정확한 관련성 평가

---

## 2. 임베딩 모델 및 설정

### 2.1 사용 모델
**모델명**: `nlpai-lab/KURE-v1` (Korean Universal Representation Embedding v1)

**선택 이유**:
- 한국어에 특화된 임베딩 모델
- 법률 도메인 텍스트 처리에 적합한 성능
- Sentence-BERT 아키텍처 기반

### 2.2 임베딩 차원
**차원**: 1024차원 밀집 벡터(dense vector)

### 2.3 환경 변수 설정
```bash
# backend/.env
EMBEDDING_MODEL=nlpai-lab/KURE-v1
EMBEDDING_DIMENSION=1024
EMBED_API_URL=http://localhost:8001/embed  # 임베딩 API 서버 URL
```

### 2.4 모델 로드 방식
```python
from sentence_transformers import SentenceTransformer

# GPU 사용 가능 여부 자동 감지
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = SentenceTransformer('nlpai-lab/KURE-v1', device=device)
```

---

## 3. 임베딩 대상 기준

### 핵심 질문: "어떤 기준으로 임베딩을 진행하는가?"

#### 답변 요약
1. **임베딩 단위**: 각 청크의 완전한 텍스트 콘텐츠
2. **임베딩 대상**: 청크의 `content` 또는 `text` 필드 (메타데이터 제외)
3. **의미적 분류**: `chunk_type`을 통한 청크 타입별 구분
4. **처리 방식**: 배치 처리를 통한 효율적인 임베딩 생성

### 3.1 임베딩 단위
- **단위**: 각 개별 청크(chunk)의 전체 텍스트 콘텐츠
- **메타데이터 처리**: 메타데이터(chunk_id, doc_id, chunk_type 등)는 임베딩에 포함하지 않고 별도로 DB에 저장
- **순수 텍스트**: 청크의 텍스트 내용만 임베딩 벡터로 변환

### 3.2 데이터 타입별 임베딩 대상

#### A. 법령 데이터 (Law)
**임베딩 대상 필드**: `content`

**포맷**:
```
[법령] {법령명}
[조문] {조문번호}

{조문 내용}
```

**예시**:
```
[법령] 민법
[조문] 제750조

고의 또는 과실로 인한 위법행위로 타인에게 손해를 가한 자는 그 손해를 배상할 책임이 있다.
```

**데이터 위치**: `backend/data/law/Civil_Law_chunks.jsonl`

#### B. 피해구제사례 (Counsel Case)
**임베딩 대상 필드**: `text`

**청크 타입**:
- `qa_combined`: 질문과 답변이 함께 포함된 청크
- `question`: 질문만 포함
- `answer`: 답변만 포함

**예시**:
```json
{
  "chunk_id": "ecmc:counsel:case123:qa_combined:0",
  "text": "Q: 인터넷 쇼핑몰에서 옷을 구매했는데 배송이 안 옵니다...\nA: 전자상거래법에 따라...",
  "chunk_type": "qa_combined"
}
```

#### C. 분쟁조정사례 (Mediation Case)
**임베딩 대상 필드**: `text`

**청크 타입**:
- `case_overview`: 사건 개요
- `claim_consumer`: 소비자 주장
- `claim_business`: 사업자 주장
- `decision`: 판단 및 주문
- `order`: 주문
- `other`: 기타

**예시**:
```json
{
  "chunk_id": "kca:mediation:ADR2024-001:case_overview:0",
  "text": "소비자는 2024년 1월 온라인 쇼핑몰에서...",
  "chunk_type": "case_overview"
}
```

**데이터 위치**: `backend/data/dispute_resolution/kca_final_rag_chunks_normalized.jsonl`

### 3.3 임베딩 구성 원칙

1. **의미 단위별 분리**: 청크 타입(`chunk_type`)을 통해 의미적 경계 구분
2. **단순 텍스트 기반**: 구조 정보(제목, 조문번호 등)는 텍스트 포맷팅으로 포함하되, 별도 필드로 임베딩하지 않음
3. **컨텍스트 유지**: 각 청크는 독립적으로 이해 가능하도록 필요한 컨텍스트 포함
4. **청크 간 관계**: `chunk_index`와 `chunk_total`을 통해 전체 문서 내 위치 추적

---

## 4. 임베딩 생성 프로세스

### 4.1 파이프라인 개요

```
[데이터 로드] → [청크 추출] → [배치 구성] → [임베딩 생성] → [DB 저장]
```

**처리 흐름**:
1. JSONL 파일에서 청킹된 데이터 로드
2. 청크 텍스트(`content` 또는 `text` 필드) 추출
3. 배치 크기(32 또는 64)로 묶어 처리
4. 임베딩 모델 또는 API를 통해 벡터 생성
5. PostgreSQL + pgvector에 저장

### 4.2 주요 스크립트

#### A. `embed_pipeline_v2.py` (통합 파이프라인)
**위치**: `backend/scripts/embed_pipeline_v2.py`

**기능**:
- 법령, 피해구제사례, 분쟁조정사례 데이터 통합 처리
- PostgreSQL + pgvector에 직접 저장
- 청크 간 관계(next/prev) 자동 생성

**핵심 코드**:
```python
def embed_and_insert_chunks(chunks, batch_size=32):
    for i in range(0, len(chunks), batch_size):
        batch_chunks = chunks[i:i+batch_size]
        texts = [chunk['content'] for chunk in batch_chunks]

        # 임베딩 API 호출
        response = requests.post(
            EMBED_API_URL,
            json={"texts": texts},
            timeout=60
        )
        embeddings = response.json()['embeddings']

        # DB 삽입
        for idx, chunk in enumerate(batch_chunks):
            cursor.execute("""
                INSERT INTO chunks (
                    chunk_id, doc_id, chunk_index, chunk_total,
                    chunk_type, content, embedding, embedding_model
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            """, (
                chunk['chunk_id'],
                chunk['doc_id'],
                chunk['chunk_index'],
                chunk['chunk_total'],
                chunk['chunk_type'],
                chunk['content'],
                embeddings[idx],
                'KURE-v1'
            ))
```

#### B. `embed_data.py` (로컬 모델)
**위치**: `backend/scripts/embedding/embed_data.py` (삭제됨, embed_data_remote.py 사용)

**특징**:
- 로컬에서 SentenceTransformer 모델 직접 로드
- GPU 사용 가능 시 자동으로 GPU 활용
- 배치 크기: 64

**핵심 코드**:
```python
model = SentenceTransformer('nlpai-lab/KURE-v1', device='cuda'|'cpu')
embeddings = model.encode(texts, convert_to_numpy=True)
```

#### C. `embed_data_remote.py` (원격 GPU)
**위치**: `backend/scripts/embedding/embed_data_remote.py`

**특징**:
- RunPod 등 원격 GPU 서버를 통한 임베딩
- REST API 방식으로 임베딩 요청
- 배치 크기: 64

### 4.3 임베딩 API 서버

#### `runpod_embed_server.py`
**위치**: `backend/runpod_embed_server.py`

**기능**: FastAPI 기반 임베딩 서비스

**엔드포인트**:

```python
# POST /embed - 텍스트 배열을 받아 임베딩 벡터 반환
@app.post("/embed")
async def embed_texts(request: EmbedRequest):
    embeddings = model.encode(request.texts, convert_to_numpy=True)
    return {"embeddings": embeddings.tolist()}

# GET / - 헬스체크 및 상태 정보
@app.get("/")
async def health_check():
    return {
        "status": "healthy",
        "model": "nlpai-lab/KURE-v1",
        "dimension": 1024
    }
```

**사용 예시**:
```bash
curl -X POST http://localhost:8001/embed \
  -H "Content-Type: application/json" \
  -d '{"texts": ["텍스트1", "텍스트2"]}'
```

### 4.4 배치 처리 파라미터

| 스크립트 | 배치 크기 | 타임아웃 |
|---------|----------|---------|
| `embed_pipeline_v2.py` | 32 | 60초 |
| `embed_data.py` | 64 | - |
| `embed_data_remote.py` | 64 | 60초 |

**배치 크기 선택 고려사항**:
- GPU 메모리 크기
- 네트워크 대역폭 (원격 API의 경우)
- 처리 속도와 메모리 사용량의 균형

---

## 5. 임베딩 저장 방식

### 5.1 데이터베이스 스키마

**데이터베이스**: PostgreSQL 15 + pgvector 확장
**스키마 파일**: `backend/database/schema_v2_final.sql`

#### chunks 테이블
```sql
CREATE TABLE chunks (
    chunk_id VARCHAR(255) PRIMARY KEY,           -- 청크 고유 ID
    doc_id VARCHAR(255) NOT NULL,                -- 부모 문서 ID
    chunk_index INTEGER NOT NULL,                -- 문서 내 청크 순번 (0부터)
    chunk_total INTEGER NOT NULL,                -- 문서 내 전체 청크 개수
    chunk_type VARCHAR(50),                      -- 청크 타입 (의미적 분류)
    content TEXT NOT NULL,                       -- 원본 텍스트 콘텐츠
    content_length INTEGER,                      -- 콘텐츠 길이 (문자 수)
    embedding vector(1024),                      -- 1024차원 임베딩 벡터
    embedding_model VARCHAR(50) DEFAULT 'KURE-v1', -- 사용 모델명
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);
```

#### chunk_id 형식
```
{source}:{doc_type}:{doc_identifier}:{chunk_type}:{chunk_index}
```

**예시**:
- `statute:law:civil_law:article:0` - 민법 조문 청크
- `kca:mediation:ADR2024-001:case_overview:0` - 분쟁조정 사건 개요 청크
- `ecmc:counsel:case123:qa_combined:0` - 상담사례 질의응답 청크

### 5.2 인덱싱 전략

#### 벡터 인덱스
```sql
CREATE INDEX idx_chunks_embedding ON chunks
USING ivfflat(embedding vector_cosine_ops)
WITH (lists = 100);
```

**인덱스 타입**: IVFFlat (Inverted File Flat)
- 고차원 벡터 검색 최적화
- 근사 최근접 이웃(Approximate Nearest Neighbor) 검색
- `lists` 파라미터: 클러스터 개수 (데이터 크기에 따라 조정)

**거리 함수**: `vector_cosine_ops` - 코사인 유사도
```
similarity = 1 - cosine_distance
cosine_distance = 1 - (A · B) / (||A|| × ||B||)
```

#### 기타 인덱스
```sql
CREATE INDEX idx_chunks_doc_id ON chunks(doc_id);      -- 문서별 청크 조회
CREATE INDEX idx_chunks_type ON chunks(chunk_type);    -- 청크 타입별 조회
```

### 5.3 메타데이터 관리

#### chunks 테이블 메타데이터
- `chunk_id`: 청크 고유 식별자
- `doc_id`: 부모 문서 ID (여러 청크를 하나의 문서로 그룹화)
- `chunk_index`: 문서 내 청크 순번 (0부터 시작)
- `chunk_total`: 문서 내 전체 청크 개수
- `chunk_type`: 의미적 타입 (`case_overview`, `decision`, `qa_combined` 등)
- `embedding_model`: 사용된 임베딩 모델명 (버전 관리 및 추적)

#### documents 테이블 (참조)
```sql
CREATE TABLE documents (
    doc_id VARCHAR(255) PRIMARY KEY,
    doc_type VARCHAR(50) NOT NULL,     -- 'law', 'counsel_case', 'mediation_case'
    title TEXT,
    source_org VARCHAR(50),            -- 'KCA', 'ECMC', 'KCDRC', 'statute'
    metadata JSONB,                    -- 추가 메타데이터 (JSON 형식)
    created_at TIMESTAMP DEFAULT NOW()
);
```

---

## 6. 임베딩 관련 설정 및 파라미터

### 6.1 환경 변수

```bash
# backend/.env

# 임베딩 모델 설정
EMBEDDING_MODEL=nlpai-lab/KURE-v1
EMBEDDING_DIMENSION=1024

# 임베딩 API 서버
EMBED_API_URL=http://localhost:8001/embed

# 데이터베이스 (pgvector)
DATABASE_HOST=localhost
DATABASE_PORT=5432
DATABASE_NAME=ddoksori
DATABASE_USER=postgres
DATABASE_PASSWORD=your_password
```

### 6.2 청크 크기 설정

#### 법령 데이터 청킹 파라미터
```python
# backend/scripts/법령_청킹_스크립트.py
class LawChunker:
    MIN_CHUNK_SIZE = 100      # 최소 청크 크기 (문자 수)
    MAX_CHUNK_SIZE = 800      # 최대 청크 크기
    OPTIMAL_CHUNK_SIZE = 400  # 권장 청크 크기
```

**청킹 기준**:
- 조문(article) 단위 우선
- 항(paragraph), 호(item) 등 하위 구조 고려
- 너무 긴 조문은 분할, 짧은 조문은 병합

### 6.3 처리 방식별 차이

#### 로컬 모델 vs 원격 API

| 구분 | 로컬 모델 | 원격 API |
|-----|----------|---------|
| **장점** | - GPU 활용 최적화<br>- 네트워크 오버헤드 없음 | - GPU 서버 공유 가능<br>- 클라이언트 리소스 절약 |
| **단점** | - 로컬 GPU 필요<br>- 메모리 사용량 큼 | - 네트워크 지연<br>- API 타임아웃 고려 필요 |
| **사용 시나리오** | GPU 서버에서 직접 실행 | 로컬 환경에서 원격 GPU 활용 |
| **스크립트** | `embed_data.py` | `embed_data_remote.py` |

---

## 7. 임베딩 검증 및 품질 관리

### 7.1 검증 스크립트

#### `check_embedding_status.py`
**위치**: `backend/scripts/check_embedding_status.py`

**확인 항목**:
1. **임베딩 완료율**: 전체 청크 중 임베딩된 청크 비율
2. **임베딩 모델 및 차원**: 일관성 확인
3. **임베딩 벡터 유효성**: NULL 값, 0 벡터 등 이상 데이터 검사
4. **청크 길이 분포**: 너무 짧거나 긴 청크 확인
5. **데이터 품질**: 청크 타입별 통계

**실행 예시**:
```bash
cd backend/scripts
python check_embedding_status.py
```

### 7.2 SQL 검증 쿼리

#### 임베딩 완료율 확인
```sql
SELECT
    COUNT(*) as total_chunks,
    COUNT(embedding) as embedded_chunks,
    ROUND(COUNT(embedding)::NUMERIC / COUNT(*) * 100, 2) as completion_rate
FROM chunks;
```

#### 임베딩 모델 및 차원 확인
```sql
SELECT
    embedding_model,
    array_length(embedding::real[], 1) as dimension,
    COUNT(*) as count
FROM chunks
WHERE embedding IS NOT NULL
GROUP BY embedding_model, array_length(embedding::real[], 1);
```

#### 청크 타입별 통계
```sql
SELECT
    chunk_type,
    COUNT(*) as count,
    AVG(content_length) as avg_length,
    MIN(content_length) as min_length,
    MAX(content_length) as max_length
FROM chunks
GROUP BY chunk_type
ORDER BY count DESC;
```

#### 임베딩 품질 검증 (0 벡터 검사)
```sql
SELECT chunk_id, content_length
FROM chunks
WHERE embedding IS NOT NULL
  AND embedding = ARRAY_FILL(0::real, ARRAY[1024])::vector;
```

---

## 8. RAG 시스템에서의 활용

### 8.1 검색 프로세스

**파일**: `backend/app/rag/retriever.py`

**흐름**:
```
[사용자 쿼리] → [쿼리 임베딩] → [벡터 검색] → [상위 K개 청크] → [LLM 컨텍스트]
```

#### 1단계: 쿼리 임베딩 생성
```python
def get_query_embedding(query: str) -> List[float]:
    model = SentenceTransformer('nlpai-lab/KURE-v1')
    embedding = model.encode(query, convert_to_numpy=True)
    return embedding.tolist()
```

#### 2단계: 벡터 유사도 검색
```python
def retrieve_similar_chunks(query_embedding: List[float], top_k: int = 5):
    cursor.execute("""
        SELECT
            c.*,
            d.*,
            1 - (c.embedding <=> %s::vector) AS similarity
        FROM chunks c
        JOIN documents d ON c.doc_id = d.doc_id
        WHERE c.drop = FALSE
        ORDER BY c.embedding <=> %s::vector
        LIMIT %s
    """, (query_embedding, query_embedding, top_k))

    return cursor.fetchall()
```

**연산자 설명**:
- `<=>`: pgvector의 코사인 거리 연산자
- `1 - (embedding <=> query)`: 코사인 거리를 유사도로 변환 (0~1 범위)

#### 3단계: LLM에 컨텍스트 전달
```python
# backend/app/rag/generator.py
def generate_answer(query: str, retrieved_chunks: List[dict]):
    context = "\n\n".join([
        f"[{chunk['doc_type']}] {chunk['content']}"
        for chunk in retrieved_chunks
    ])

    prompt = f"""
    다음 문서들을 참고하여 질문에 답변해주세요.

    문서:
    {context}

    질문: {query}
    """

    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )

    return response.choices[0].message.content
```

### 8.2 검색 쿼리 예제

#### 기본 검색 (상위 5개)
```sql
SELECT
    chunk_id,
    content,
    1 - (embedding <=> '[0.123, 0.456, ...]'::vector) AS similarity
FROM chunks
ORDER BY embedding <=> '[0.123, 0.456, ...]'::vector
LIMIT 5;
```

#### 청크 타입 필터링
```sql
SELECT
    chunk_id,
    content,
    chunk_type,
    1 - (embedding <=> %s::vector) AS similarity
FROM chunks
WHERE chunk_type IN ('decision', 'case_overview')
ORDER BY embedding <=> %s::vector
LIMIT 5;
```

#### 문서 타입별 검색
```sql
SELECT
    c.chunk_id,
    c.content,
    d.doc_type,
    1 - (c.embedding <=> %s::vector) AS similarity
FROM chunks c
JOIN documents d ON c.doc_id = d.doc_id
WHERE d.doc_type = 'mediation_case'
ORDER BY c.embedding <=> %s::vector
LIMIT 5;
```

#### 유사도 임계값 적용
```sql
SELECT
    chunk_id,
    content,
    1 - (embedding <=> %s::vector) AS similarity
FROM chunks
WHERE 1 - (embedding <=> %s::vector) >= 0.7  -- 유사도 70% 이상
ORDER BY embedding <=> %s::vector
LIMIT 10;
```

---

## 9. 핵심 요약

### "어떤 기준으로 임베딩을 진행하는가?"

| 항목 | 기준 |
|-----|------|
| **임베딩 단위** | 각 청크의 완전한 텍스트 콘텐츠 |
| **임베딩 모델** | KURE-v1 (한국어 특화, SentenceTransformer) |
| **임베딩 차원** | 1024차원 밀집 벡터 |
| **임베딩 대상** | 청크의 `content` 또는 `text` 필드 (메타데이터 제외) |
| **의미적 분류** | `chunk_type`을 통한 청크 타입별 구분 |
| **저장 방식** | PostgreSQL + pgvector, IVFFlat 인덱스 |
| **검색 방식** | 코사인 유사도 기반 벡터 검색 |
| **처리 방식** | 배치 처리 (32~64 청크 단위) |
| **API 방식** | FastAPI 기반 임베딩 서버 또는 로컬 모델 |

### 데이터 타입별 요약

| 데이터 타입 | 임베딩 대상 필드 | 청크 타입 예시 | 특징 |
|-----------|----------------|--------------|------|
| **법령** | `content` | `article`, `paragraph` | 조문별 구조화된 텍스트 |
| **피해구제사례** | `text` | `qa_combined`, `question`, `answer` | 질의응답 분리/통합 |
| **분쟁조정사례** | `text` | `case_overview`, `decision`, `claim_*` | 의미 섹션별 분리 |

### 임베딩 프로세스 요약

```
데이터 준비 (JSONL)
    ↓
청크 추출 (content/text 필드)
    ↓
배치 구성 (32~64 청크)
    ↓
임베딩 생성 (KURE-v1 모델)
    ↓
PostgreSQL 저장 (pgvector)
    ↓
인덱싱 (IVFFlat, 코사인)
    ↓
RAG 검색 활용
```

---

## 10. 참고 파일 목록

### 임베딩 관련 스크립트
- `backend/scripts/embed_pipeline_v2.py` - 통합 임베딩 파이프라인
- `backend/scripts/embedding/embed_data_remote.py` - 원격 GPU 임베딩 (현재 사용)
- `backend/scripts/check_embedding_status.py` - 임베딩 상태 검증

### 임베딩 서버
- `backend/runpod_embed_server.py` - FastAPI 임베딩 API 서버

### 데이터베이스
- `backend/database/schema_v2_final.sql` - PostgreSQL + pgvector 스키마

### RAG 시스템
- `backend/app/rag/retriever.py` - 벡터 검색 로직
- `backend/app/rag/generator.py` - LLM 답변 생성

### 환경 설정
- `backend/.env` - 임베딩 모델 및 API 설정

### 데이터 파일
- `backend/data/law/Civil_Law_chunks.jsonl` - 법령 청크 데이터
- `backend/data/dispute_resolution/kca_final_rag_chunks_normalized.jsonl` - 분쟁조정사례 청크

---

## 추가 문서
- [청킹 및 임베딩 결과 확인 가이드.md](./청킹%20및%20임베딩%20결과%20확인%20가이드.md)
- [RAG_SETUP_GUIDE.md](./RAG_SETUP_GUIDE.md)
- [PR4_DATA_SCHEMA.md](./PR4_DATA_SCHEMA.md)
