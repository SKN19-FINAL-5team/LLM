# ì²­í‚¹ ë° ì„ë² ë”© ê²°ê³¼ í™•ì¸ ê°€ì´ë“œ

**ì‘ì„±ì¼**: 2026-01-05  
**í”„ë¡œì íŠ¸**: ë˜‘ì†Œë¦¬ (ddoksori_demo)

---

## ğŸ“‹ ëª©ì°¨

1. [ë°ì´í„°ë² ì´ìŠ¤ ì§ì ‘ ì¡°íšŒ](#1-ë°ì´í„°ë² ì´ìŠ¤-ì§ì ‘-ì¡°íšŒ)
2. [Python ìŠ¤í¬ë¦½íŠ¸ë¡œ í™•ì¸](#2-python-ìŠ¤í¬ë¦½íŠ¸ë¡œ-í™•ì¸)
3. [í†µê³„ ì •ë³´ í™•ì¸](#3-í†µê³„-ì •ë³´-í™•ì¸)
4. [ìƒ˜í”Œ ë°ì´í„° í™•ì¸](#4-ìƒ˜í”Œ-ë°ì´í„°-í™•ì¸)
5. [ì„ë² ë”© í’ˆì§ˆ ê²€ì¦](#5-ì„ë² ë”©-í’ˆì§ˆ-ê²€ì¦)
6. [ì‹œê°í™”](#6-ì‹œê°í™”)

---

## 1. ë°ì´í„°ë² ì´ìŠ¤ ì§ì ‘ ì¡°íšŒ

### ë°©ë²• 1-1: psql ì»¤ë§¨ë“œë¼ì¸

```bash
# PostgreSQL ì ‘ì†
psql -h localhost -p 5432 -U postgres -d ddoksori

# ë˜ëŠ” Docker ì»¨í…Œì´ë„ˆë¥¼ í†µí•´
docker exec -it ddoksori_db psql -U postgres -d ddoksori
```

### ê¸°ë³¸ í†µê³„ í™•ì¸

```sql
-- 1. ì „ì²´ ë¬¸ì„œ ìˆ˜
SELECT COUNT(*) as total_documents FROM documents;

-- 2. ì „ì²´ ì²­í¬ ìˆ˜
SELECT COUNT(*) as total_chunks FROM chunks;

-- 3. ì„ë² ë”©ëœ ì²­í¬ ìˆ˜
SELECT COUNT(*) as embedded_chunks 
FROM chunks 
WHERE embedding IS NOT NULL;

-- 4. ë¬¸ì„œ ìœ í˜•ë³„ í†µê³„
SELECT 
    doc_type,
    COUNT(*) as document_count
FROM documents
GROUP BY doc_type
ORDER BY doc_type;

-- 5. ì²­í¬ ìœ í˜•ë³„ í†µê³„
SELECT 
    chunk_type,
    COUNT(*) as chunk_count,
    AVG(content_length) as avg_length
FROM chunks
GROUP BY chunk_type
ORDER BY chunk_count DESC;

-- 6. ì¶œì²˜ë³„ í†µê³„
SELECT 
    source_org,
    COUNT(DISTINCT d.doc_id) as document_count,
    COUNT(c.chunk_id) as chunk_count
FROM documents d
LEFT JOIN chunks c ON d.doc_id = c.doc_id
GROUP BY source_org
ORDER BY document_count DESC;
```

### ìƒì„¸ ë°ì´í„° í™•ì¸

```sql
-- 7. ìƒ˜í”Œ ë¬¸ì„œ 10ê°œ ì¡°íšŒ
SELECT 
    doc_id,
    doc_type,
    title,
    source_org
FROM documents
LIMIT 10;

-- 8. ìƒ˜í”Œ ì²­í¬ 10ê°œ ì¡°íšŒ (ì„ë² ë”© í¬í•¨)
SELECT 
    chunk_id,
    doc_id,
    chunk_index,
    chunk_total,
    chunk_type,
    LEFT(content, 100) as content_preview,
    content_length,
    CASE WHEN embedding IS NOT NULL THEN 'YES' ELSE 'NO' END as has_embedding
FROM chunks
LIMIT 10;

-- 9. íŠ¹ì • ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ ì¡°íšŒ
SELECT 
    chunk_id,
    chunk_index,
    chunk_type,
    LEFT(content, 50) as content_preview
FROM chunks
WHERE doc_id = 'consumer.go.kr:consumer_counsel_case:53321'
ORDER BY chunk_index;

-- 10. ê°€ì¥ ê¸´ ì²­í¬ Top 10
SELECT 
    chunk_id,
    doc_id,
    content_length,
    LEFT(content, 100) as content_preview
FROM chunks
ORDER BY content_length DESC
LIMIT 10;

-- 11. ê°€ì¥ ì§§ì€ ì²­í¬ Top 10
SELECT 
    chunk_id,
    doc_id,
    content_length,
    content
FROM chunks
ORDER BY content_length ASC
LIMIT 10;
```

### í†µí•© ë·° í™œìš©

```sql
-- 12. í†µí•© ë·°ë¡œ ì¡°íšŒ (ë¬¸ì„œ + ì²­í¬ ì •ë³´)
SELECT 
    chunk_id,
    doc_type,
    doc_title,
    chunk_type,
    LEFT(content, 100) as content_preview
FROM v_chunks_with_documents
LIMIT 10;

-- 13. í†µê³„ ë·° ì¡°íšŒ
SELECT * FROM v_data_statistics;
```

---

## 2. Python ìŠ¤í¬ë¦½íŠ¸ë¡œ í™•ì¸

### ë°©ë²• 2-1: ê°„ë‹¨í•œ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸

```python
# backend/scripts/check_embedding_status.py
import psycopg2
from dotenv import load_dotenv
import os

load_dotenv()

# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
conn = psycopg2.connect(
    host=os.getenv('DB_HOST', 'localhost'),
    port=os.getenv('DB_PORT', '5432'),
    database=os.getenv('DB_NAME', 'ddoksori'),
    user=os.getenv('DB_USER', 'postgres'),
    password=os.getenv('DB_PASSWORD', 'postgres')
)

cur = conn.cursor()

print("=" * 60)
print("ì²­í‚¹ ë° ì„ë² ë”© ê²°ê³¼ í™•ì¸")
print("=" * 60)

# 1. ì „ì²´ í†µê³„
cur.execute("SELECT COUNT(*) FROM documents")
doc_count = cur.fetchone()[0]
print(f"\nì´ ë¬¸ì„œ ìˆ˜: {doc_count:,}ê°œ")

cur.execute("SELECT COUNT(*) FROM chunks")
chunk_count = cur.fetchone()[0]
print(f"ì´ ì²­í¬ ìˆ˜: {chunk_count:,}ê°œ")

cur.execute("SELECT COUNT(*) FROM chunks WHERE embedding IS NOT NULL")
embedded_count = cur.fetchone()[0]
print(f"ì„ë² ë”©ëœ ì²­í¬ ìˆ˜: {embedded_count:,}ê°œ")
print(f"ì„ë² ë”© ì™„ë£Œìœ¨: {embedded_count/chunk_count*100:.2f}%")

# 2. ë¬¸ì„œ ìœ í˜•ë³„ í†µê³„
print("\n" + "-" * 60)
print("ë¬¸ì„œ ìœ í˜•ë³„ í†µê³„")
print("-" * 60)
cur.execute("""
    SELECT 
        doc_type,
        COUNT(*) as count
    FROM documents
    GROUP BY doc_type
    ORDER BY doc_type
""")
for row in cur.fetchall():
    print(f"  {row[0]:<20} {row[1]:>10,}ê°œ")

# 3. ì²­í¬ ìœ í˜•ë³„ í†µê³„
print("\n" + "-" * 60)
print("ì²­í¬ ìœ í˜•ë³„ í†µê³„")
print("-" * 60)
cur.execute("""
    SELECT 
        chunk_type,
        COUNT(*) as count,
        AVG(content_length) as avg_length
    FROM chunks
    GROUP BY chunk_type
    ORDER BY count DESC
    LIMIT 10
""")
for row in cur.fetchall():
    print(f"  {row[0]:<20} {row[1]:>10,}ê°œ  (í‰ê·  {row[2]:.0f}ì)")

# 4. ì²­í¬ ê¸¸ì´ ë¶„í¬
print("\n" + "-" * 60)
print("ì²­í¬ ê¸¸ì´ ë¶„í¬")
print("-" * 60)
cur.execute("""
    SELECT 
        MIN(content_length) as min_length,
        AVG(content_length) as avg_length,
        MAX(content_length) as max_length,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY content_length) as median_length
    FROM chunks
""")
row = cur.fetchone()
print(f"  ìµœì†Œ ê¸¸ì´: {row[0]:,}ì")
print(f"  í‰ê·  ê¸¸ì´: {row[1]:.0f}ì")
print(f"  ì¤‘ì•™ê°’: {row[2]:.0f}ì")
print(f"  ìµœëŒ€ ê¸¸ì´: {row[3]:,}ì")

# 5. ìƒ˜í”Œ ì²­í¬ ì¶œë ¥
print("\n" + "-" * 60)
print("ìƒ˜í”Œ ì²­í¬ (5ê°œ)")
print("-" * 60)
cur.execute("""
    SELECT 
        chunk_id,
        chunk_type,
        LEFT(content, 100) as content_preview
    FROM chunks
    LIMIT 5
""")
for i, row in enumerate(cur.fetchall(), 1):
    print(f"\n[{i}] {row[0]}")
    print(f"    íƒ€ì…: {row[1]}")
    print(f"    ë‚´ìš©: {row[2]}...")

print("\n" + "=" * 60)

cur.close()
conn.close()
```

**ì‹¤í–‰**:
```bash
cd backend
python scripts/check_embedding_status.py
```

---

## 3. í†µê³„ ì •ë³´ í™•ì¸

### ë°©ë²• 3-1: í†µê³„ ë·° ì‚¬ìš©

```sql
-- í†µê³„ ë·° ì¡°íšŒ
SELECT 
    doc_type,
    source_org,
    document_count,
    chunk_count,
    avg_chunk_length,
    embedded_chunk_count,
    ROUND(embedded_chunk_count::numeric / chunk_count * 100, 2) as embedding_rate
FROM v_data_statistics
ORDER BY doc_type, source_org;
```

### ë°©ë²• 3-2: ì„ë² ë”© ì§„í–‰ë¥  í™•ì¸

```sql
-- ì „ì²´ ì„ë² ë”© ì§„í–‰ë¥ 
SELECT 
    COUNT(*) as total_chunks,
    COUNT(CASE WHEN embedding IS NOT NULL THEN 1 END) as embedded_chunks,
    ROUND(COUNT(CASE WHEN embedding IS NOT NULL THEN 1 END)::numeric / COUNT(*) * 100, 2) as progress_percent
FROM chunks;

-- ë¬¸ì„œ ìœ í˜•ë³„ ì„ë² ë”© ì§„í–‰ë¥ 
SELECT 
    d.doc_type,
    COUNT(c.chunk_id) as total_chunks,
    COUNT(CASE WHEN c.embedding IS NOT NULL THEN 1 END) as embedded_chunks,
    ROUND(COUNT(CASE WHEN c.embedding IS NOT NULL THEN 1 END)::numeric / COUNT(c.chunk_id) * 100, 2) as progress_percent
FROM documents d
JOIN chunks c ON d.doc_id = c.doc_id
GROUP BY d.doc_type
ORDER BY d.doc_type;
```

---

## 4. ìƒ˜í”Œ ë°ì´í„° í™•ì¸

### ë°©ë²• 4-1: íŠ¹ì • ë¬¸ì„œì˜ ì²­í‚¹ ê²°ê³¼ í™•ì¸

```sql
-- íŠ¹ì • ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ ì¡°íšŒ
SELECT 
    chunk_index,
    chunk_type,
    content_length,
    LEFT(content, 200) as content_preview,
    CASE WHEN embedding IS NOT NULL THEN 'YES' ELSE 'NO' END as has_embedding
FROM chunks
WHERE doc_id = 'YOUR_DOC_ID'
ORDER BY chunk_index;
```

### ë°©ë²• 4-2: ëœë¤ ìƒ˜í”Œ í™•ì¸

```sql
-- ëœë¤ ì²­í¬ 10ê°œ ì¡°íšŒ
SELECT 
    c.chunk_id,
    d.doc_type,
    d.title,
    c.chunk_type,
    c.content_length,
    LEFT(c.content, 150) as content_preview
FROM chunks c
JOIN documents d ON c.doc_id = d.doc_id
ORDER BY RANDOM()
LIMIT 10;
```

---

## 5. ì„ë² ë”© í’ˆì§ˆ ê²€ì¦

### ë°©ë²• 5-1: ì„ë² ë”© ë²¡í„° í™•ì¸

```sql
-- ì„ë² ë”© ë²¡í„° ì°¨ì› í™•ì¸
SELECT 
    chunk_id,
    array_length(embedding::real[], 1) as embedding_dimension
FROM chunks
WHERE embedding IS NOT NULL
LIMIT 5;

-- ì„ë² ë”© ë²¡í„° ìƒ˜í”Œ í™•ì¸ (ì²˜ìŒ 5ê°œ ì°¨ì›ë§Œ)
SELECT 
    chunk_id,
    (embedding::real[])[1:5] as first_5_dimensions
FROM chunks
WHERE embedding IS NOT NULL
LIMIT 5;
```

### ë°©ë²• 5-2: ìœ ì‚¬ë„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸

```python
# backend/scripts/test_similarity_search.py
import psycopg2
import requests
import json
from dotenv import load_dotenv
import os

load_dotenv()

# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
conn = psycopg2.connect(
    host=os.getenv('DB_HOST', 'localhost'),
    port=os.getenv('DB_PORT', '5432'),
    database=os.getenv('DB_NAME', 'ddoksori'),
    user=os.getenv('DB_USER', 'postgres'),
    password=os.getenv('DB_PASSWORD', 'postgres')
)

# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
test_query = "ì˜¨ë¼ì¸ ì‡¼í•‘ëª°ì—ì„œ êµ¬ë§¤í•œ ì œí’ˆì´ ë¶ˆëŸ‰ì´ì—ìš”. í™˜ë¶ˆ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?"

print(f"í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {test_query}")
print("=" * 60)

# 1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
embed_api_url = os.getenv('EMBED_API_URL', 'http://localhost:8001/embed')
response = requests.post(embed_api_url, json={"text": test_query})
query_embedding = response.json()['embedding']

print(f"ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì™„ë£Œ (ì°¨ì›: {len(query_embedding)})")

# 2. ìœ ì‚¬ë„ ê²€ìƒ‰
cur = conn.cursor()
cur.execute("""
    SELECT 
        chunk_id,
        doc_id,
        chunk_type,
        LEFT(content, 200) as content_preview,
        1 - (embedding <=> %s::vector) as similarity
    FROM chunks
    WHERE embedding IS NOT NULL
    ORDER BY embedding <=> %s::vector
    LIMIT 5
""", (query_embedding, query_embedding))

print("\nìƒìœ„ 5ê°œ ìœ ì‚¬ ì²­í¬:")
print("-" * 60)
for i, row in enumerate(cur.fetchall(), 1):
    print(f"\n[{i}] ìœ ì‚¬ë„: {row[4]:.4f}")
    print(f"    ì²­í¬ ID: {row[0]}")
    print(f"    ë¬¸ì„œ ID: {row[1]}")
    print(f"    ì²­í¬ íƒ€ì…: {row[2]}")
    print(f"    ë‚´ìš©: {row[3]}...")

cur.close()
conn.close()
```

**ì‹¤í–‰**:
```bash
cd backend
python scripts/test_similarity_search.py
```

---

## 6. ì‹œê°í™”

### ë°©ë²• 6-1: ì²­í¬ ê¸¸ì´ ë¶„í¬ ì‹œê°í™”

```python
# backend/scripts/visualize_chunks.py
import psycopg2
import matplotlib.pyplot as plt
import numpy as np
from dotenv import load_dotenv
import os

load_dotenv()

conn = psycopg2.connect(
    host=os.getenv('DB_HOST', 'localhost'),
    port=os.getenv('DB_PORT', '5432'),
    database=os.getenv('DB_NAME', 'ddoksori'),
    user=os.getenv('DB_USER', 'postgres'),
    password=os.getenv('DB_PASSWORD', 'postgres')
)

cur = conn.cursor()

# ì²­í¬ ê¸¸ì´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
cur.execute("SELECT content_length FROM chunks")
lengths = [row[0] for row in cur.fetchall()]

# íˆìŠ¤í† ê·¸ë¨ ê·¸ë¦¬ê¸°
plt.figure(figsize=(12, 6))
plt.hist(lengths, bins=50, edgecolor='black', alpha=0.7)
plt.xlabel('ì²­í¬ ê¸¸ì´ (ë¬¸ì ìˆ˜)')
plt.ylabel('ë¹ˆë„')
plt.title('ì²­í¬ ê¸¸ì´ ë¶„í¬')
plt.axvline(np.mean(lengths), color='r', linestyle='--', label=f'í‰ê· : {np.mean(lengths):.0f}')
plt.axvline(np.median(lengths), color='g', linestyle='--', label=f'ì¤‘ì•™ê°’: {np.median(lengths):.0f}')
plt.legend()
plt.grid(True, alpha=0.3)
plt.savefig('chunk_length_distribution.png', dpi=300, bbox_inches='tight')
print("ì²­í¬ ê¸¸ì´ ë¶„í¬ ê·¸ë˜í”„ ì €ì¥: chunk_length_distribution.png")

# ë¬¸ì„œ ìœ í˜•ë³„ ì²­í¬ ìˆ˜
cur.execute("""
    SELECT d.doc_type, COUNT(c.chunk_id)
    FROM documents d
    LEFT JOIN chunks c ON d.doc_id = c.doc_id
    GROUP BY d.doc_type
    ORDER BY d.doc_type
""")
doc_types = []
chunk_counts = []
for row in cur.fetchall():
    doc_types.append(row[0])
    chunk_counts.append(row[1])

plt.figure(figsize=(10, 6))
plt.bar(doc_types, chunk_counts, edgecolor='black', alpha=0.7)
plt.xlabel('ë¬¸ì„œ ìœ í˜•')
plt.ylabel('ì²­í¬ ìˆ˜')
plt.title('ë¬¸ì„œ ìœ í˜•ë³„ ì²­í¬ ìˆ˜')
plt.xticks(rotation=45, ha='right')
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.savefig('chunks_by_doc_type.png', dpi=300, bbox_inches='tight')
print("ë¬¸ì„œ ìœ í˜•ë³„ ì²­í¬ ìˆ˜ ê·¸ë˜í”„ ì €ì¥: chunks_by_doc_type.png")

cur.close()
conn.close()
```

**ì‹¤í–‰**:
```bash
cd backend
python scripts/visualize_chunks.py
```

---

## ğŸ¯ ë¹ ë¥¸ ì²´í¬ë¦¬ìŠ¤íŠ¸

ì„ë² ë”© ì™„ë£Œ í›„ ë‹¤ìŒ í•­ëª©ë“¤ì„ í™•ì¸í•˜ì„¸ìš”:

### âœ… ê¸°ë³¸ í™•ì¸

```sql
-- 1. ì´ ì²­í¬ ìˆ˜ í™•ì¸
SELECT COUNT(*) FROM chunks;
-- ì˜ˆìƒ: 30,754ê°œ

-- 2. ì„ë² ë”© ì™„ë£Œìœ¨ í™•ì¸
SELECT 
    COUNT(*) as total,
    COUNT(CASE WHEN embedding IS NOT NULL THEN 1 END) as embedded,
    ROUND(COUNT(CASE WHEN embedding IS NOT NULL THEN 1 END)::numeric / COUNT(*) * 100, 2) as rate
FROM chunks;
-- ì˜ˆìƒ: 100%

-- 3. ë¬¸ì„œ ìœ í˜•ë³„ í™•ì¸
SELECT doc_type, COUNT(*) FROM documents GROUP BY doc_type;
-- ì˜ˆìƒ: law(11), counsel_case(13,544), mediation_case(3,173)
```

### âœ… í’ˆì§ˆ í™•ì¸

```sql
-- 4. ë¹„ì •ìƒì ìœ¼ë¡œ ì§§ì€ ì²­í¬ í™•ì¸
SELECT chunk_id, content_length, content
FROM chunks
WHERE content_length < 10
LIMIT 10;

-- 5. ë¹„ì •ìƒì ìœ¼ë¡œ ê¸´ ì²­í¬ í™•ì¸
SELECT chunk_id, content_length, LEFT(content, 100)
FROM chunks
WHERE content_length > 5000
LIMIT 10;

-- 6. ì„ë² ë”© ì°¨ì› í™•ì¸
SELECT DISTINCT array_length(embedding::real[], 1) as dimension
FROM chunks
WHERE embedding IS NOT NULL;
-- ì˜ˆìƒ: 1024 (KURE-v1 ìˆ˜ì • ë²„ì „)
```

### âœ… ê¸°ëŠ¥ í™•ì¸

```sql
-- 7. ìœ ì‚¬ë„ ê²€ìƒ‰ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
-- (ë¨¼ì € í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ì˜ ì„ë² ë”©ì„ ìƒì„±í•´ì•¼ í•¨)

-- 8. ì»¨í…ìŠ¤íŠ¸ í™•ì¥ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
SELECT * FROM get_chunk_with_context('consumer.go.kr:consumer_counsel_case:53321::chunk0', 1);
```

---

## ğŸ“š ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- [PostgreSQL ê³µì‹ ë¬¸ì„œ](https://www.postgresql.org/docs/)
- [pgvector ë¬¸ì„œ](https://github.com/pgvector/pgvector)
- [RAG ì‹œìŠ¤í…œ ê°€ì´ë“œ](backend/RAG_SETUP_GUIDE.md)

---

**ì‘ì„±ì**: Manus AI  
**ìµœì¢… ìˆ˜ì •**: 2026-01-05
