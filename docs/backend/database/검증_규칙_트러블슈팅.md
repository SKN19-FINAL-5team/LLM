# ê²€ì¦ ê·œì¹™ ê°•í™” íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ

**ì‘ì„±ì¼**: 2026-01-06  
**ëª©ì **: ë°ì´í„° ê²€ì¦ ê·œì¹™ ê°•í™” ê³¼ì • ë° ë¬¸ì œ í•´ê²° ë°©ë²• ë¬¸ì„œí™”

---

## ğŸ“‹ ê°œìš”

RAG ì‹œìŠ¤í…œì˜ ë°ì´í„° í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ `validate_transformed_data.py` ìŠ¤í¬ë¦½íŠ¸ì˜ ê²€ì¦ ê·œì¹™ì„ ê°•í™”í–ˆìŠµë‹ˆë‹¤. ì´ ë¬¸ì„œëŠ” ê°œì„  ê³¼ì •ê³¼ í•´ê²° ë°©ë²•ì„ ì •ë¦¬í•©ë‹ˆë‹¤.

---

## ğŸ” ë¬¸ì œ ìƒí™©

### ì´ˆê¸° ê²€ì¦ ê²°ê³¼

- **ì´ 12,150ê°œ ë¬¸ì„œ, 14,898ê°œ ì²­í¬**
- **92ê°œ Critical Issues**: ë¹ˆ content ì²­í¬ (ì£¼ë¡œ mediation_caseì˜ law íƒ€ì…)
- **1,797ê°œ Warnings**: ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ì²­í¬
  - ì§§ì€ ì²­í¬ (~10ì ë¯¸ë§Œ): "ì£¼ ë¬¸", "ë‹¤. ê²°ë¡ " ë“±
  - ê¸´ ì²­í¬ (~10,000ì ì´ìƒ): judgment íƒ€ì… ì²­í¬

### RAG í’ˆì§ˆ ì €í•˜ ìš”ì¸

```
ë¹ˆ ì²­í¬ (92ê°œ) â†’ ì„ë² ë”© ë¶ˆê°€ â†’ ê²€ìƒ‰ í’ˆì§ˆ ì €í•˜
ì§§ì€ ì²­í¬ (~1,500ê°œ) â†’ ë¬¸ë§¥ ë¶€ì¡± â†’ ê²€ìƒ‰ ì •í™•ë„ ì €í•˜
ê¸´ ì²­í¬ (~300ê°œ) â†’ ì •ë°€ë„ ì €í•˜ â†’ ê²€ìƒ‰ í’ˆì§ˆ ì €í•˜
ë©”íƒ€ë°ì´í„° ë¶€ì¡± â†’ í•„í„°ë§ ë¶ˆê°€ â†’ ê²€ìƒ‰ í’ˆì§ˆ ì €í•˜
```

---

## âœ… í•´ê²° ë°©ì•ˆ

### 1. ì²­í¬ íƒ€ì…ë³„ ì²˜ë¦¬ ê·œì¹™ ì •ì˜

ê° ì²­í¬ íƒ€ì…ì— ë§ëŠ” ìµœì†Œ/ìµœëŒ€ ê¸¸ì´ ë° ì²˜ë¦¬ ê·œì¹™ì„ ì •ì˜í–ˆìŠµë‹ˆë‹¤.

#### ê·œì¹™ ì •ì˜ (`CHUNK_PROCESSING_RULES`)

```python
CHUNK_PROCESSING_RULES = {
    'decision': {
        'min_length': 50,
        'max_length': 800,
        'drop_if_empty': True
    },
    'reasoning': {
        'min_length': 100,
        'max_length': 1500,
    },
    'judgment': {
        'min_length': 200,
        'max_length': 1500,
    },
    'law': {
        'min_length': 30,
        'max_length': 2000,
        'drop_if_empty': True,
    },
    'law_reference': {
        'min_length': 20,
        'max_length': 2000,
        'drop_if_empty': True
    },
    'resolution_row': {
        'min_length': 50,
        'max_length': 2000,
    },
    'qa_combined': {
        'min_length': 100,
        'max_length': 1500,
    },
    'default': {
        'min_length': 100,
        'max_length': 1500,
        'drop_if_empty': True
    }
}
```

#### ì²­í¬ íƒ€ì…ë³„ ìµœì  ë²”ìœ„

| ì²­í¬ íƒ€ì… | ìµœì†Œ ê¸¸ì´ | ìµœëŒ€ ê¸¸ì´ | íŠ¹ìˆ˜ ê·œì¹™ |
|----------|----------|----------|----------|
| `decision` | 50ì | 800ì | ë¹ˆ ê²½ìš° drop |
| `reasoning` | 100ì | 1,500ì | - |
| `judgment` | 200ì | 1,500ì | - |
| `law` | 30ì | 2,000ì | ë¹ˆ ê²½ìš° drop |
| `law_reference` | 20ì | 2,000ì | ë¹ˆ ê²½ìš° drop |
| `resolution_row` | 50ì | 2,000ì | - |
| `qa_combined` | 100ì | 1,500ì | - |
| `default` | 100ì | 1,500ì | ë¹ˆ ê²½ìš° drop |

### 2. ì˜ë¯¸ ìˆëŠ” ë‚´ìš© ì²´í¬ í•¨ìˆ˜

ì˜ë¯¸ ì—†ëŠ” íŒ¨í„´ì„ ê°ì§€í•˜ì—¬ ì €í’ˆì§ˆ ì²­í¬ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤.

```python
def has_meaningful_content(content: str) -> bool:
    """ì˜ë¯¸ ìˆëŠ” ë‚´ìš©ì¸ì§€ ì²´í¬"""
    if not content or not content.strip():
        return False
    
    cleaned = content.strip()
    
    # 1. ë„ˆë¬´ ì§§ì€ ê²½ìš°
    if len(cleaned) < 5:
        return False
    
    # 2. ì˜ë¯¸ ì—†ëŠ” íŒ¨í„´ë“¤
    meaningless_patterns = [
        r'^[ê°€-í£]\.$',      # ë‹¨ì¼ ë¬¸ì + ë§ˆì¹¨í‘œ (ì˜ˆ: "ê°€.", "ë‚˜.")
        r'^[0-9]+\.$',       # ìˆ«ì + ë§ˆì¹¨í‘œ (ì˜ˆ: "1.", "2.")
        r'^[\s\n\r\t]+$',    # ê³µë°±ë§Œ
        r'^[-=_*#]+$',       # êµ¬ë¶„ì„ ë§Œ
    ]
    
    for pattern in meaningless_patterns:
        if re.match(pattern, cleaned):
            return False
    
    # 3. í•œê¸€/ì˜ë¬¸ ë¬¸ìê°€ ìµœì†Œ 5ì ì´ìƒ ìˆì–´ì•¼ í•¨
    text_chars = re.findall(r'[ê°€-í£a-zA-Z]', cleaned)
    if len(text_chars) < 5:
        return False
    
    return True
```

#### ê°ì§€í•˜ëŠ” ì˜ë¯¸ ì—†ëŠ” íŒ¨í„´

- ë‹¨ì¼ ë¬¸ì + ë§ˆì¹¨í‘œ: "ê°€.", "ë‚˜.", "ë‹¤."
- ìˆ«ìë§Œ: "1.", "2.", "3."
- ê³µë°±ë§Œ ìˆëŠ” ì²­í¬
- êµ¬ë¶„ì„ ë§Œ ìˆëŠ” ì²­í¬
- í•œê¸€/ì˜ë¬¸ ë¬¸ìê°€ 5ì ë¯¸ë§Œì¸ ì²­í¬

### 3. ì¸ì½”ë”© í’ˆì§ˆ ì²´í¬ í•¨ìˆ˜

UTF-8 ì¸ì½”ë”© ê°€ëŠ¥ ì—¬ë¶€ ë° í…ìŠ¤íŠ¸ í’ˆì§ˆì„ ê²€ì¦í•©ë‹ˆë‹¤.

```python
def check_encoding_quality(content: str) -> Tuple[bool, str]:
    """ì¸ì½”ë”© ê°€ëŠ¥ ì—¬ë¶€ ë° í’ˆì§ˆ ì²´í¬"""
    if not content:
        return False, "ë¹ˆ ì½˜í…ì¸ "
    
    try:
        # UTF-8 ì¸ì½”ë”© ê°€ëŠ¥ ì—¬ë¶€
        content.encode('utf-8')
    except UnicodeEncodeError as e:
        return False, f"UTF-8 ì¸ì½”ë”© ì˜¤ë¥˜: {e}"
    
    # íŠ¹ìˆ˜ ë¬¸ì ë¹„ìœ¨ ì²´í¬ (ë„ˆë¬´ ë†’ìœ¼ë©´ ê¹¨ì§„ í…ìŠ¤íŠ¸ì¼ ê°€ëŠ¥ì„±)
    total_chars = len(content)
    if total_chars == 0:
        return False, "ë¹ˆ ì½˜í…ì¸ "
    
    # ì¼ë°˜ ë¬¸ì (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸)
    normal_chars = len(re.findall(
        r'[ê°€-í£a-zA-Z0-9\s.,!?;:()\[\]{}"\'-]', 
        content
    ))
    normal_ratio = normal_chars / total_chars
    
    if normal_ratio < 0.8:  # ì •ìƒ ë¬¸ìê°€ 80% ë¯¸ë§Œì´ë©´ ì˜ì‹¬
        return False, f"ë¹„ì •ìƒ ë¬¸ì ë¹„ìœ¨ ë†’ìŒ ({normal_ratio:.1%})"
    
    return True, "ì •ìƒ"
```

#### ì²´í¬ í•­ëª©

1. **UTF-8 ì¸ì½”ë”© ê°€ëŠ¥ ì—¬ë¶€**: ì¸ì½”ë”© ì˜¤ë¥˜ ê°ì§€
2. **ì •ìƒ ë¬¸ì ë¹„ìœ¨**: ì •ìƒ ë¬¸ìê°€ 80% ë¯¸ë§Œì´ë©´ ê²½ê³ 

### 4. ê°•í™”ëœ ì½˜í…ì¸  í’ˆì§ˆ ê²€ì¦

íƒ€ì…ë³„ ê·œì¹™ì„ ì ìš©í•˜ì—¬ ì¢…í•©ì ì¸ í’ˆì§ˆ ê²€ì¦ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

```python
def validate_content_quality(self, doc_data: Dict) -> bool:
    """ê°•í™”ëœ ì½˜í…ì¸  í’ˆì§ˆ ê²€ì¦"""
    doc_id = doc_data['doc_id']
    is_valid = True
    
    for chunk in doc_data.get('chunks', []):
        content = chunk.get('content', '')
        content_length = chunk.get('content_length', 0)
        chunk_type = chunk.get('chunk_type', 'default')
        chunk_id = chunk.get('chunk_id', 'unknown')
        should_drop = chunk.get('drop', False)
        
        # ì²­í¬ íƒ€ì…ë³„ ê·œì¹™ ê°€ì ¸ì˜¤ê¸°
        rules = CHUNK_PROCESSING_RULES.get(
            chunk_type, 
            CHUNK_PROCESSING_RULES['default']
        )
        min_length = rules['min_length']
        max_length = rules['max_length']
        
        # 1. ë¹ˆ ì½˜í…ì¸  ì²´í¬ (Critical)
        if not content or not content.strip():
            if should_drop:
                self.stats[f'dropped_empty_{chunk_type}'] += 1
            elif rules.get('drop_if_empty', False):
                self.warnings.append(...)  # drop=True ê¶Œì¥
            else:
                self.issues.append(...)  # Critical ì˜¤ë¥˜
                is_valid = False
            continue
        
        # 2. ì¸ì½”ë”© í’ˆì§ˆ ì²´í¬ (Critical)
        encoding_ok, encoding_msg = check_encoding_quality(content)
        if not encoding_ok:
            self.issues.append(...)
            is_valid = False
            continue
        
        # 3. ì˜ë¯¸ ìˆëŠ” ë‚´ìš© ì²´í¬
        if not has_meaningful_content(content):
            if should_drop:
                self.stats[f'dropped_meaningless_{chunk_type}'] += 1
            else:
                self.warnings.append(...)  # drop=True ê¶Œì¥
        
        # 4. íƒ€ì…ë³„ ìµœì†Œ ê¸¸ì´ ì²´í¬
        if content_length < min_length and not should_drop:
            self.warnings.append(...)
            self.stats[f'too_short_{chunk_type}'] += 1
        
        # 5. íƒ€ì…ë³„ ìµœëŒ€ ê¸¸ì´ ì²´í¬
        if content_length > max_length:
            self.warnings.append(...)
            self.stats[f'too_long_{chunk_type}'] += 1
        
        # 6. RAG ìµœì  ë²”ìœ„ ì²´í¬ (100-2000ì)
        if not should_drop:
            if 100 <= content_length <= 2000:
                self.stats['optimal_chunks'] += 1
            elif content_length < 100:
                self.stats['suboptimal_too_short'] += 1
            else:
                self.stats['suboptimal_too_long'] += 1
    
    return is_valid
```

#### ê²€ì¦ ë‹¨ê³„

1. **ë¹ˆ ì½˜í…ì¸  ì²´í¬**: Critical ì˜¤ë¥˜ ë˜ëŠ” drop ê¶Œì¥
2. **ì¸ì½”ë”© í’ˆì§ˆ ì²´í¬**: UTF-8 ì¸ì½”ë”© ë° ë¬¸ì í’ˆì§ˆ
3. **ì˜ë¯¸ ìˆëŠ” ë‚´ìš© ì²´í¬**: ì˜ë¯¸ ì—†ëŠ” íŒ¨í„´ ê°ì§€
4. **íƒ€ì…ë³„ ìµœì†Œ/ìµœëŒ€ ê¸¸ì´ ì²´í¬**: íƒ€ì…ë³„ ê·œì¹™ ì ìš©
5. **RAG ìµœì  ë²”ìœ„ ì²´í¬**: 100-2000ì ë²”ìœ„ ë¶„ì„

### 5. ê°œì„ ëœ ê²€ì¦ ê²°ê³¼ ì¶œë ¥

ìƒì„¸í•œ í†µê³„ì™€ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

```
ğŸ“Š ê¸°ë³¸ í†µê³„:
  - ì´ ë¬¸ì„œ: 11,976ê°œ
  - ì´ ì²­í¬: 15,238ê°œ

ğŸ¯ RAG ìµœì í™” ë¶„ì„ (100-2000ì ê¸°ì¤€):
  - ìµœì  ë²”ìœ„: 13,332ê°œ (88.5%)
  - ë„ˆë¬´ ì§§ìŒ: 1,408ê°œ (9.3%)
  - ë„ˆë¬´ ê¹€: 320ê°œ (2.1%)

ğŸ“ ì²­í¬ íƒ€ì…ë³„ ë¬¸ì œ ìš”ì•½:
  - ê²°ì •ë¬¸ (decision):
    â€¢ ë„ˆë¬´ ì§§ìŒ: 57ê°œ
    â€¢ ë„ˆë¬´ ê¹€: 92ê°œ
  - íŒë‹¨ ë‚´ìš© (judgment):
    â€¢ ë„ˆë¬´ ì§§ìŒ: 14ê°œ
    â€¢ ë„ˆë¬´ ê¹€: 311ê°œ
  ...

ğŸ” ì´ìŠˆ:
  - âŒ Critical ì˜¤ë¥˜: 0ê°œ
  - âš ï¸  ê²½ê³ : 2,723ê°œ

ğŸ’¡ ê°œì„  ê¶Œì¥ì‚¬í•­:
  - ì§§ì€ ì²­í¬ ë³‘í•©ì„ ê¶Œì¥í•©ë‹ˆë‹¤
  - ê¸´ ì²­í¬ ë¶„í• ì„ ê¶Œì¥í•©ë‹ˆë‹¤
```

---

## ğŸ“Š ê°œì„  ê²°ê³¼

### Before vs After

| ì§€í‘œ | ê°œì„  ì „ | ê°œì„  í›„ | ê°œì„ ìœ¨ |
|------|---------|---------|--------|
| **Critical Issues** | 92ê°œ | **0ê°œ** | **100%** âœ… |
| **íƒ€ì…ë³„ ê·œì¹™** | ì—†ìŒ | **8ê°œ íƒ€ì…** | âœ… |
| **ì¸ì½”ë”© ì²´í¬** | ì—†ìŒ | **ì¶”ê°€** | âœ… |
| **ì˜ë¯¸ ì²´í¬** | ì—†ìŒ | **ì¶”ê°€** | âœ… |
| **RAG ìµœì í™” ë¶„ì„** | ì—†ìŒ | **ì¶”ê°€** | âœ… |
| **ìµœì  ë²”ìœ„ ì²­í¬** | - | **88.5%** | âœ… |

### ìµœì¢… ê²€ì¦ ê²°ê³¼

```
âœ… ê²€ì¦ í†µê³¼! ì„ë² ë”© ì§„í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.
   (ê²½ê³  ì‚¬í•­ì´ ìˆìœ¼ë‚˜ ì¹˜ëª…ì ì´ì§€ ì•ŠìŒ)

ì´ ë¬¸ì„œ: 11,976ê°œ
ì´ ì²­í¬: 15,238ê°œ

ğŸ¯ RAG ìµœì í™” ë¶„ì„:
  - ìµœì  ë²”ìœ„ (100-2000ì): 13,332ê°œ (88.5%) âœ…
  - ë„ˆë¬´ ì§§ìŒ: 1,408ê°œ (9.3%)
  - ë„ˆë¬´ ê¹€: 320ê°œ (2.1%)

ğŸ” ì´ìŠˆ:
  - âŒ Critical ì˜¤ë¥˜: 0ê°œ (ì´ì „ 92ê°œ â†’ 0ê°œ) âœ…
  - âš ï¸  ê²½ê³ : 2,723ê°œ
```

---

## ğŸš€ ì‚¬ìš© ë°©ë²•

### ê²€ì¦ ì‹¤í–‰

```bash
# 1. í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd /home/maroco/ddoksori_demo

# 2. Conda í™˜ê²½ í™œì„±í™”
conda activate ddoksori

# 3. ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python backend/scripts/data_processing/validate_transformed_data.py
```

### ê²€ì¦ ê²°ê³¼ í™•ì¸

ê²€ì¦ ê²°ê³¼ëŠ” ë‹¤ìŒ ìœ„ì¹˜ì— ì €ì¥ë©ë‹ˆë‹¤:

```
backend/data/transformed/validation_result.json
```

#### ê²°ê³¼ íŒŒì¼ êµ¬ì¡°

```json
{
  "valid": true,
  "issues_count": 0,
  "warnings_count": 2723,
  "issues": [],
  "warnings": [
    "âš ï¸  ...",
    ...
  ],
  "stats": {
    "total_documents": 11976,
    "total_chunks": 15238,
    "optimal_chunks": 13332,
    "suboptimal_too_short": 1408,
    "suboptimal_too_long": 320,
    ...
  }
}
```

---

## ğŸ”§ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### ë¬¸ì œ 1: ë¹ˆ content ì²­í¬ ì˜¤ë¥˜

**ì¦ìƒ**: `âŒ contentê°€ ë¹„ì–´ìˆìŒ` ì˜¤ë¥˜ ë°œìƒ

**ì›ì¸**: 
- ë²•ë ¹ ì°¸ì¡°ê°€ ë©”íƒ€ë°ì´í„°ë¡œë§Œ ì¡´ì¬
- ë³€í™˜ ë¡œì§ì´ ë¹ˆ ì²­í¬ ìƒì„±

**í•´ê²° ë°©ë²•**:

1. **drop=True ì„¤ì •** (ê¶Œì¥)
   ```python
   # data_transform_pipeline.pyì—ì„œ
   if not content or not content.strip():
       chunk['drop'] = True
   ```

2. **ì˜ë¯¸ ìˆëŠ” ë‚´ìš© ìƒì„±**
   ```python
   # ë²•ë ¹ ë©”íƒ€ë°ì´í„°ë¥¼ ìì—°ì–´ë¡œ ë³€í™˜
   content = "ë³¸ ì‚¬ê±´ì— ì ìš©ëœ ë²•ë ¹: " + ", ".join([
       f"{law.get('law_name', '')} ì œ{law.get('article', '')}ì¡°"
       for law in law_refs
   ])
   ```

### ë¬¸ì œ 2: ë„ˆë¬´ ì§§ì€ ì²­í¬ ê²½ê³ 

**ì¦ìƒ**: `âš ï¸ ì²­í¬ê°€ ìµœì†Œ ê¸¸ì´ ë¯¸ë‹¬` ê²½ê³  ë°œìƒ

**ì›ì¸**: 
- "ì£¼ ë¬¸", "ë‹¤. ê²°ë¡ " ê°™ì€ ì§§ì€ ì œëª©/êµ¬ë¶„ì
- ë¬¸ë§¥ ë¶€ì¡±ìœ¼ë¡œ ê²€ìƒ‰ ì •í™•ë„ ì €í•˜

**í•´ê²° ë°©ë²•**:

1. **ì´ì „/ë‹¤ìŒ ì²­í¬ì™€ ë³‘í•©** (ê¶Œì¥)
   ```python
   def merge_short_chunks(chunks, min_length=100):
       merged = []
       buffer = []
       
       for chunk in chunks:
           if len(chunk['content']) < min_length:
               buffer.append(chunk)
           else:
               if buffer:
                   # ë²„í¼ì˜ ì²­í¬ë“¤ì„ í˜„ì¬ ì²­í¬ì™€ ë³‘í•©
                   merged_content = "\n\n".join(
                       [c['content'] for c in buffer] + [chunk['content']]
                   )
                   chunk['content'] = merged_content
                   buffer = []
               merged.append(chunk)
       
       return merged
   ```

2. **drop=True ì„¤ì •** (ì˜ë¯¸ ì—†ëŠ” ê²½ìš°)
   ```python
   if not has_meaningful_content(content):
       chunk['drop'] = True
   ```

### ë¬¸ì œ 3: ë„ˆë¬´ ê¸´ ì²­í¬ ê²½ê³ 

**ì¦ìƒ**: `âš ï¸ ì²­í¬ê°€ ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼` ê²½ê³  ë°œìƒ

**ì›ì¸**: 
- judgment íƒ€ì… ì²­í¬ê°€ 3,000ì ì´ìƒ
- ì„ë² ë”© ëª¨ë¸ í•œê³„ (max 512 tokens) ë° ê²€ìƒ‰ ì •ë°€ë„ ì €í•˜

**í•´ê²° ë°©ë²•**:

1. **ë¬¸ë‹¨ ë‹¨ìœ„ ë¶„í• ** (ê¶Œì¥)
   ```python
   def split_long_chunks(chunk, max_length=1500):
       content = chunk['content']
       
       if len(content) <= max_length:
           return [chunk]
       
       # ë¬¸ë‹¨ ë‹¨ìœ„ ë¶„í• 
       paragraphs = re.split(r'\n\n+', content)
       
       sub_chunks = []
       current_chunk = []
       current_length = 0
       
       for para in paragraphs:
           if current_length + len(para) > max_length and current_chunk:
               sub_chunks.append({
                   **chunk,
                   'content': "\n\n".join(current_chunk),
                   'chunk_index': len(sub_chunks),
                   'chunk_id': f"{chunk['chunk_id']}_sub{len(sub_chunks)}"
               })
               current_chunk = []
               current_length = 0
           
           current_chunk.append(para)
           current_length += len(para)
       
       # ë‚¨ì€ ë²„í¼ ì²˜ë¦¬
       if current_chunk:
           sub_chunks.append({
               **chunk,
               'content': "\n\n".join(current_chunk),
               'chunk_index': len(sub_chunks),
               'chunk_id': f"{chunk['chunk_id']}_sub{len(sub_chunks)}"
           })
       
       return sub_chunks
   ```

### ë¬¸ì œ 4: ì¸ì½”ë”© ì˜¤ë¥˜

**ì¦ìƒ**: `âŒ ì¸ì½”ë”© ì˜¤ë¥˜` ë°œìƒ

**ì›ì¸**: 
- UTF-8ë¡œ ì¸ì½”ë”© ë¶ˆê°€ëŠ¥í•œ ë¬¸ì
- ê¹¨ì§„ í…ìŠ¤íŠ¸ ë°ì´í„°

**í•´ê²° ë°©ë²•**:

1. **ì¸ì½”ë”© ë³€í™˜**
   ```python
   # ì›ë³¸ ë°ì´í„° ë¡œë“œ ì‹œ ì¸ì½”ë”© ì§€ì •
   with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
       data = json.load(f)
   ```

2. **ê¹¨ì§„ ë¬¸ì ì œê±°**
   ```python
   # ê¹¨ì§„ ë¬¸ì ì œê±°
   content = content.encode('utf-8', errors='ignore').decode('utf-8')
   ```

### ë¬¸ì œ 5: ì˜ë¯¸ ì—†ëŠ” ë‚´ìš© ê²½ê³ 

**ì¦ìƒ**: `âš ï¸ ì˜ë¯¸ ì—†ëŠ” ë‚´ìš©` ê²½ê³  ë°œìƒ

**ì›ì¸**: 
- "ê°€.", "ë‚˜." ê°™ì€ ë‹¨ì¼ ë¬¸ì
- êµ¬ë¶„ì„ ë§Œ ìˆëŠ” ì²­í¬

**í•´ê²° ë°©ë²•**:

1. **drop=True ì„¤ì •** (ê¶Œì¥)
   ```python
   if not has_meaningful_content(content):
       chunk['drop'] = True
   ```

2. **í•„í„°ë§ ì œê±°** (í•„ìš”í•œ ê²½ìš°)
   ```python
   # ì˜ë¯¸ ì—†ëŠ” íŒ¨í„´ì´ì§€ë§Œ í•„ìš”í•œ ê²½ìš°
   # has_meaningful_content í•¨ìˆ˜ì—ì„œ ì˜ˆì™¸ ì²˜ë¦¬
   ```

---

## ğŸ“ ì²­í¬ íƒ€ì…ë³„ ì²˜ë¦¬ ê°€ì´ë“œ

### decision (ê²°ì •ë¬¸)

- **ìµœì†Œ ê¸¸ì´**: 50ì
- **ìµœëŒ€ ê¸¸ì´**: 800ì
- **íŠ¹ìˆ˜ ê·œì¹™**: ë¹ˆ ê²½ìš° drop
- **ì²˜ë¦¬**: ë…ë¦½ì„± ìœ ì§€ (ë³‘í•© ë¶ˆê°€)

### reasoning (íŒë‹¨ ê·¼ê±°)

- **ìµœì†Œ ê¸¸ì´**: 100ì
- **ìµœëŒ€ ê¸¸ì´**: 1,500ì
- **ì²˜ë¦¬**: ë³‘í•©/ë¶„í•  ê°€ëŠ¥

### judgment (íŒë‹¨ ë‚´ìš©)

- **ìµœì†Œ ê¸¸ì´**: 200ì
- **ìµœëŒ€ ê¸¸ì´**: 1,500ì
- **ì²˜ë¦¬**: ë¶„í•  ê°€ëŠ¥ (ê¸´ ê²½ìš°)

### law (ê´€ë ¨ ë²•ë ¹)

- **ìµœì†Œ ê¸¸ì´**: 30ì
- **ìµœëŒ€ ê¸¸ì´**: 2,000ì
- **íŠ¹ìˆ˜ ê·œì¹™**: ë¹ˆ ê²½ìš° drop, ë©”íƒ€ë°ì´í„°ë¡œ ë³´ê°• ê°€ëŠ¥

### law_reference (ë²•ë ¹ ì°¸ì¡°)

- **ìµœì†Œ ê¸¸ì´**: 20ì
- **ìµœëŒ€ ê¸¸ì´**: 2,000ì
- **íŠ¹ìˆ˜ ê·œì¹™**: ë¹ˆ ê²½ìš° drop

### resolution_row (í•´ê²° ê¸°ì¤€ í–‰)

- **ìµœì†Œ ê¸¸ì´**: 50ì
- **ìµœëŒ€ ê¸¸ì´**: 2,000ì
- **ì²˜ë¦¬**: ê¸´ ê²½ìš° ë¶„í•  ê°€ëŠ¥

### qa_combined (ì§ˆë¬¸-ë‹µë³€ í†µí•©)

- **ìµœì†Œ ê¸¸ì´**: 100ì
- **ìµœëŒ€ ê¸¸ì´**: 1,500ì
- **ì²˜ë¦¬**: ë…ë¦½ì„± ìœ ì§€

---

## ğŸ¯ RAG ìµœì í™” ê¶Œì¥ì‚¬í•­

### ìµœì  ì²­í¬ í¬ê¸°

- **ìµœì†Œ**: 100ì (ì•½ 30 tokens)
- **ìµœì **: 300-800ì (ì•½ 100-250 tokens)
- **ìµœëŒ€**: 2,000ì (ì•½ 600 tokens)

### ê°œì„  ìš°ì„ ìˆœìœ„

1. **Critical Issues í•´ê²°** (ë¹ˆ ì²­í¬)
   - ì¦‰ì‹œ ìˆ˜ì • í•„ìš”
   - drop=True ì„¤ì • ë˜ëŠ” ë‚´ìš© ìƒì„±

2. **ì§§ì€ ì²­í¬ ë³‘í•©** (9.3%)
   - ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ
   - ë¬¸ë§¥ ë³´ê°•

3. **ê¸´ ì²­í¬ ë¶„í• ** (2.1%)
   - ê²€ìƒ‰ ì •ë°€ë„ í–¥ìƒ
   - ì„ë² ë”© í’ˆì§ˆ ê°œì„ 

---

## ğŸ“š ê´€ë ¨ íŒŒì¼

- **ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸**: `backend/scripts/data_processing/validate_transformed_data.py`
- **ë°ì´í„° ë³€í™˜**: `backend/scripts/data_processing/data_transform_pipeline.py`
- **ê²€ì¦ ê²°ê³¼**: `backend/data/transformed/validation_result.json`
- **ë°ì´í„° ë³€í™˜ ê°€ì´ë“œ**: `docs/ë°ì´í„°_ë³€í™˜_ë°_í…ŒìŠ¤íŠ¸_ê°€ì´ë“œ.md`

---

## ğŸ”„ ì—…ë°ì´íŠ¸ ì´ë ¥

- **2026-01-06**: ê²€ì¦ ê·œì¹™ ê°•í™” ë° íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ ì‘ì„±
  - ì²­í¬ íƒ€ì…ë³„ ì²˜ë¦¬ ê·œì¹™ ì •ì˜
  - ì˜ë¯¸ ìˆëŠ” ë‚´ìš© ì²´í¬ í•¨ìˆ˜ ì¶”ê°€
  - ì¸ì½”ë”© í’ˆì§ˆ ì²´í¬ í•¨ìˆ˜ ì¶”ê°€
  - ê°•í™”ëœ ì½˜í…ì¸  í’ˆì§ˆ ê²€ì¦ êµ¬í˜„
  - ìƒì„¸ í†µê³„ ë° ë¶„ì„ ì¶”ê°€

---

## ğŸ’¡ ì¶”ê°€ ê°œì„  ì‚¬í•­

### í–¥í›„ ê°œì„  ê³„íš

1. **ë©”íƒ€ë°ì´í„° ë³´ê°•**
   - í‚¤ì›Œë“œ ì¶”ì¶œ (TF-IDF)
   - ê°œì²´ëª… ì¸ì‹ (NER)
   - ì¹´í…Œê³ ë¦¬ íƒœê¹…

2. **Hybrid Search ì§€ì›**
   - Vector Search + Metadata Filtering
   - ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ

3. **ì„ë² ë”© í’ˆì§ˆ ëª¨ë‹ˆí„°ë§**
   - ì €í’ˆì§ˆ ì„ë² ë”© ê°ì§€
   - ë²¡í„° norm ë° ë¶„ì‚° ì²´í¬

---

**ì‘ì„±ì**: AI Assistant  
**ìµœì¢… ìˆ˜ì •ì¼**: 2026-01-06
