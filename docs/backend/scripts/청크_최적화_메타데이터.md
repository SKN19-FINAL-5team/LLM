# ì²­í¬ ìµœì í™” ë° ë©”íƒ€ë°ì´í„° ë³´ê°•

**ì‘ì„±ì¼**: 2026-01-06  
**ëª©ì **: RAG ë°ì´í„° í’ˆì§ˆ ê°œì„ ì„ ìœ„í•œ ì²­í¬ í¬ê¸° ìµœì í™” ë° ë©”íƒ€ë°ì´í„° ë³´ê°• ê¸°ëŠ¥ ë¬¸ì„œí™”

---

## ğŸ“‹ ê°œìš”

RAG ì‹œìŠ¤í…œì˜ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒì„ ìœ„í•´ ë‹¤ìŒ ë‘ ê°€ì§€ ì£¼ìš” ê¸°ëŠ¥ì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤:

1. **ì²­í¬ í¬ê¸° ìµœì í™”**: ì§§ì€ ì²­í¬ ë³‘í•© ë° ê¸´ ì²­í¬ ë¶„í• 
2. **ë©”íƒ€ë°ì´í„° ë³´ê°•**: í‚¤ì›Œë“œ, ì—”í‹°í‹°, ë²•ë¥  ìš©ì–´ ë“± ìë™ ì¶”ì¶œ

---

## ğŸ¯ ì²­í¬ í¬ê¸° ìµœì í™”

### ë°°ê²½

ê²€ì¦ ê²°ê³¼ ë¶„ì„ì— ë”°ë¥´ë©´:
- **92ê°œ Critical Issues**: ë¹ˆ content ì²­í¬
- **~1,500ê°œ ì§§ì€ ì²­í¬** (~10ì ë¯¸ë§Œ): "ì£¼ ë¬¸", "ë‹¤. ê²°ë¡ " ë“±
- **~300ê°œ ê¸´ ì²­í¬** (~10,000ì ì´ìƒ): judgment íƒ€ì… ì²­í¬

ì´ëŸ¬í•œ ë¬¸ì œë“¤ì´ RAG ê²€ìƒ‰ í’ˆì§ˆ ì €í•˜ì˜ ì£¼ìš” ì›ì¸ì´ì—ˆìŠµë‹ˆë‹¤.

### êµ¬í˜„ ë‚´ìš©

#### 1. ì²­í¬ íƒ€ì…ë³„ ì²˜ë¦¬ ê·œì¹™ ì •ì˜

```python
CHUNK_PROCESSING_RULES = {
    'decision': {
        'min_length': 50,
        'max_length': 800,
        'merge_allowed': False,  # ê²°ì •ë¬¸ì€ ë…ë¦½ì„± ìœ ì§€
        'split_allowed': False,
        'drop_if_empty': True
    },
    'reasoning': {
        'min_length': 100,
        'max_length': 1500,
        'merge_allowed': True,
        'split_allowed': True
    },
    'judgment': {
        'min_length': 200,
        'max_length': 1500,
        'merge_allowed': True,
        'split_allowed': True,  # ê¸´ íŒë‹¨ ë‚´ìš© ë¶„í• 
    },
    'law': {
        'min_length': 30,
        'max_length': 2000,
        'merge_allowed': False,
        'split_allowed': False,
        'drop_if_empty': True,
        'enrich_with_metadata': True
    },
    # ... ê¸°íƒ€ íƒ€ì…
}
```

#### 2. ì§§ì€ ì²­í¬ ë³‘í•© ë¡œì§

**í•¨ìˆ˜**: `_merge_short_chunks(chunks: List[Dict]) -> List[Dict]`

**ë™ì‘ ë°©ì‹**:
- ê° ì²­í¬ì˜ íƒ€ì…ë³„ ìµœì†Œ ê¸¸ì´ í™•ì¸
- ìµœì†Œ ê¸¸ì´ ë¯¸ë§Œì´ê³  `merge_allowed=True`ì¸ ì²­í¬ë¥¼ ë²„í¼ì— ìˆ˜ì§‘
- ë‹¤ìŒ ì²­í¬ì™€ ë³‘í•©í•˜ê±°ë‚˜, ë§ˆì§€ë§‰ ì²­í¬ì™€ ë³‘í•©
- `drop=True`ì¸ ì²­í¬ëŠ” ë³‘í•© ëŒ€ìƒì—ì„œ ì œì™¸

**ì˜ˆì‹œ**:
```python
# ë³‘í•© ì „
[
    {'content': 'ì£¼ ë¬¸', 'chunk_type': 'reasoning'},  # 3ì (ë³‘í•© ëŒ€ìƒ)
    {'content': 'í”¼ì‹ ì²­ì¸ì€ ì‹ ì²­ì¸ì—ê²Œ...', 'chunk_type': 'reasoning'}  # 100ì ì´ìƒ
]

# ë³‘í•© í›„
[
    {'content': 'ì£¼ ë¬¸\n\ní”¼ì‹ ì²­ì¸ì€ ì‹ ì²­ì¸ì—ê²Œ...', 'chunk_type': 'reasoning'}
]
```

#### 3. ê¸´ ì²­í¬ ë¶„í•  ë¡œì§

**í•¨ìˆ˜**: `_split_long_chunks(chunks: List[Dict]) -> List[Dict]`

**ë™ì‘ ë°©ì‹**:
- ê° ì²­í¬ì˜ íƒ€ì…ë³„ ìµœëŒ€ ê¸¸ì´ í™•ì¸
- ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼ì´ê³  `split_allowed=True`ì¸ ì²­í¬ë¥¼ ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í• 
- ë¬¸ë‹¨(`\n\n+`)ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ì—¬ëŸ¬ ì„œë¸Œ ì²­í¬ ìƒì„±
- ì„œë¸Œ ì²­í¬ IDëŠ” `{ì›ë³¸_chunk_id}_sub{ë²ˆí˜¸}` í˜•ì‹

**ì˜ˆì‹œ**:
```python
# ë¶„í•  ì „
{
    'chunk_id': 'kca:mediation:001:judgment:0000',
    'content': 'ê¸´ íŒë‹¨ ë‚´ìš©...\n\në‹¤ìŒ ë¬¸ë‹¨...\n\në§ˆì§€ë§‰ ë¬¸ë‹¨...',  # 3000ì
    'chunk_type': 'judgment'
}

# ë¶„í•  í›„
[
    {
        'chunk_id': 'kca:mediation:001:judgment:0000',
        'content': 'ê¸´ íŒë‹¨ ë‚´ìš©...',  # ~1500ì
        'chunk_type': 'judgment'
    },
    {
        'chunk_id': 'kca:mediation:001:judgment:0000_sub1',
        'content': 'ë‹¤ìŒ ë¬¸ë‹¨...\n\në§ˆì§€ë§‰ ë¬¸ë‹¨...',  # ~1500ì
        'chunk_type': 'judgment'
    }
]
```

#### 4. í†µí•© ìµœì í™” í•¨ìˆ˜

**í•¨ìˆ˜**: `_optimize_chunks(chunks: List[Dict]) -> List[Dict]`

**ì²˜ë¦¬ ìˆœì„œ**:
1. ì§§ì€ ì²­í¬ ë³‘í•© (`_merge_short_chunks`)
2. ê¸´ ì²­í¬ ë¶„í•  (`_split_long_chunks`)
3. ë¹ˆ ì²­í¬ ì²˜ë¦¬ (`drop_if_empty=True`ì¸ ê²½ìš° `drop=True` ì„¤ì •)

**ì ìš© ìœ„ì¹˜**:
- `transform_law_data()`: ë²•ë ¹ ë°ì´í„° ë³€í™˜ í›„
- `transform_criteria_table2()`: ê¸°ì¤€ ë°ì´í„° ë³€í™˜ í›„
- `transform_mediation_kca()`: KCA ë¶„ìŸì¡°ì •ì‚¬ë¡€ ë³€í™˜ í›„
- `transform_mediation_ecmc()`: ECMC ë¶„ìŸì¡°ì •ì‚¬ë¡€ ë³€í™˜ í›„
- `transform_counsel_case()`: í”¼í•´êµ¬ì œì‚¬ë¡€ ë³€í™˜ í›„

### ê°œì„  ê²°ê³¼

| ì§€í‘œ | ê°œì„  ì „ | ê°œì„  í›„ | ê°œì„ ìœ¨ |
|------|---------|---------|--------|
| Critical Issues (ë¹ˆ ì²­í¬) | 92ê°œ | 0ê°œ | âœ… 100% |
| ì§§ì€ ì²­í¬ (< 100ì) | ~1,500ê°œ | 310ê°œ | ğŸ“‰ 80% ê°ì†Œ |
| ìµœì  ë²”ìœ„ ì²­í¬ (100-2000ì) | - | 13,530ê°œ (95.6%) | ğŸ“ˆ ìš°ìˆ˜ |
| ì´ ì²­í¬ ìˆ˜ | 14,898ê°œ | 14,337ê°œ | ğŸ“‰ 561ê°œ ìµœì í™” |

---

## ğŸ” ë©”íƒ€ë°ì´í„° ë³´ê°•

### ë°°ê²½

Hybrid Search(ë²¡í„° ê²€ìƒ‰ + ë©”íƒ€ë°ì´í„° í•„í„°ë§)ë¥¼ ì§€ì›í•˜ê¸° ìœ„í•´ ê° ì²­í¬ì— ë‹¤ìŒ ë©”íƒ€ë°ì´í„°ë¥¼ ìë™ ì¶”ì¶œí•©ë‹ˆë‹¤:

- **í‚¤ì›Œë“œ**: TF-IDF ê¸°ë°˜ ìƒìœ„ í‚¤ì›Œë“œ
- **ì—”í‹°í‹°**: íšŒì‚¬ëª…, ì œí’ˆëª… ë“±
- **ë²•ë¥  ìš©ì–´**: ë²•ë¥  ê´€ë ¨ ì „ë¬¸ ìš©ì–´
- **ì¹´í…Œê³ ë¦¬ íƒœê·¸**: í’ˆëª©, ë¶„ìŸ ìœ í˜• ë“±
- **ë²•ë ¹ ì°¸ì¡°**: ë²•ë ¹ ì¡°ë¬¸ ì°¸ì¡°
- **ë‚ ì§œ**: ë¬¸ì„œ ë‚´ ë‚ ì§œ ì •ë³´

### êµ¬í˜„ ë‚´ìš©

#### 1. MetadataEnricher í´ë˜ìŠ¤

**íŒŒì¼**: `backend/scripts/data_processing/metadata_enricher.py`

**ì£¼ìš” ë©”ì„œë“œ**:

```python
class MetadataEnricher:
    def extract_keywords(self, content: str, top_k: int = 10) -> List[str]:
        """TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    
    def extract_entities(self, content: str) -> Dict[str, List[str]]:
        """íšŒì‚¬ëª…, ì œí’ˆëª… ë“± ì—”í‹°í‹° ì¶”ì¶œ"""
    
    def extract_legal_terms(self, content: str) -> List[str]:
        """ë²•ë¥  ìš©ì–´ ì¶”ì¶œ"""
    
    def infer_category(self, content: str) -> List[str]:
        """ì¹´í…Œê³ ë¦¬ íƒœê·¸ ì¶”ë¡ """
    
    def extract_law_references(self, content: str) -> List[str]:
        """ë²•ë ¹ ì°¸ì¡° ì¶”ì¶œ"""
    
    def extract_dates(self, content: str) -> List[str]:
        """ë‚ ì§œ ì •ë³´ ì¶”ì¶œ"""
    
    def enrich_chunk_metadata(self, chunk: Dict, extract_all: bool = True) -> Dict:
        """ì²­í¬ ë©”íƒ€ë°ì´í„° ë³´ê°•"""
```

#### 2. ë°ì´í„° ë³€í™˜ íŒŒì´í”„ë¼ì¸ í†µí•©

**íŒŒì¼**: `backend/scripts/data_processing/data_transform_pipeline.py`

**ë³€ê²½ ì‚¬í•­**:

```python
class DataTransformer:
    def __init__(self, output_dir: Path = None, use_db: bool = False, 
                 enrich_metadata: bool = True):
        """
        Args:
            enrich_metadata: ë©”íƒ€ë°ì´í„° ë³´ê°• ì—¬ë¶€
        """
        self.enrich_metadata = enrich_metadata
        if enrich_metadata:
            self.metadata_enricher = MetadataEnricher()
            print("âœ… ë©”íƒ€ë°ì´í„° ë³´ê°• í™œì„±í™”")
        else:
            self.metadata_enricher = None
    
    def _enrich_document(self, doc_data: Dict) -> Dict:
        """ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ì— ë©”íƒ€ë°ì´í„° ë³´ê°• ì ìš©"""
        if not self.enrich_metadata or not self.metadata_enricher:
            return doc_data
        
        enriched_count = 0
        for chunk in doc_data.get('chunks', []):
            # drop=Trueì¸ ì²­í¬ëŠ” ìŠ¤í‚µ
            if chunk.get('drop', False):
                continue
            
            # ë©”íƒ€ë°ì´í„° ë³´ê°•
            self.metadata_enricher.enrich_chunk_metadata(chunk, extract_all=True)
            enriched_count += 1
        
        self.stats['enriched_chunks'] += enriched_count
        return doc_data
```

**ì ìš© ìœ„ì¹˜**:
- ëª¨ë“  ë³€í™˜ í•¨ìˆ˜ì—ì„œ ë¬¸ì„œ ë³€í™˜ í›„ `_enrich_document()` í˜¸ì¶œ
- `transform_law_data()`, `transform_criteria_table2()`, `transform_mediation_kca()`, `transform_mediation_ecmc()`, `transform_counsel_case()`

#### 3. ë©”íƒ€ë°ì´í„° êµ¬ì¡°

ë³´ê°•ëœ ì²­í¬ì˜ ë©”íƒ€ë°ì´í„° êµ¬ì¡°:

```json
{
  "chunk_id": "kca:mediation:001:judgment:0000",
  "chunk_type": "judgment",
  "content": "íŒë‹¨ ë‚´ìš©...",
  "metadata": {
    "keywords": ["ì†Œë¹„ì", "í™˜ë¶ˆ", "í•˜ì", "ì†í•´ë°°ìƒ"],
    "entities": {
      "companies": ["ì£¼ì‹íšŒì‚¬ ì‚¼ì„±ì „ì"],
      "products": ["ê°¤ëŸ­ì‹œ S24"]
    },
    "legal_terms": ["ë¯¼ë²•", "ì†Œë¹„ìê¸°ë³¸ë²•", "ê³„ì•½", "í•´ì œ"],
    "category_tags": ["ì „ììƒê±°ë˜", "ìŠ¤ë§ˆíŠ¸í°", "í•˜ì"],
    "law_references": ["ë¯¼ë²• ì œ750ì¡°", "ì†Œë¹„ìê¸°ë³¸ë²• ì œ16ì¡°"],
    "dates": ["2024-01-15", "2024.01.20"]
  }
}
```

### ì‚¬ìš© ë°©ë²•

#### ë©”íƒ€ë°ì´í„° ë³´ê°• í™œì„±í™” (ê¸°ë³¸ê°’)

```python
# ê¸°ë³¸ì ìœ¼ë¡œ ë©”íƒ€ë°ì´í„° ë³´ê°•ì´ í™œì„±í™”ë¨
transformer = DataTransformer(enrich_metadata=True)
transformer.run_transformation()
```

#### ë©”íƒ€ë°ì´í„° ë³´ê°• ë¹„í™œì„±í™”

```python
# ë©”íƒ€ë°ì´í„° ë³´ê°•ì„ ë¹„í™œì„±í™”í•˜ë ¤ë©´
transformer = DataTransformer(enrich_metadata=False)
transformer.run_transformation()
```

### í†µê³„ í™•ì¸

ë³€í™˜ ì™„ë£Œ í›„ í†µê³„ì— ë©”íƒ€ë°ì´í„° ë³´ê°• ì •ë³´ê°€ í¬í•¨ë©ë‹ˆë‹¤:

```
================================================================================
ë³€í™˜ ì™„ë£Œ í†µê³„
================================================================================
  - ì´ ë¬¸ì„œ: 11,976ê°œ
  - ì´ ì²­í¬: 14,337ê°œ
  - ìŠ¤í‚µ: 0ê°œ
  - ë©”íƒ€ë°ì´í„° ë³´ê°•: 14,027ê°œ ì²­í¬
  - ì˜¤ë¥˜: 0ê°œ
```

---

## ğŸ”„ ì „ì²´ í”„ë¡œì„¸ìŠ¤

### ë°ì´í„° ë³€í™˜ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ìˆœì„œ

```
1. ì›ë³¸ ë°ì´í„° ë¡œë“œ (JSONL)
   â†“
2. ì²­í¬ ìƒì„± ë° ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì„¤ì •
   â†“
3. ì²­í¬ ìµœì í™”
   â”œâ”€ ì§§ì€ ì²­í¬ ë³‘í•©
   â”œâ”€ ê¸´ ì²­í¬ ë¶„í• 
   â””â”€ ë¹ˆ ì²­í¬ ì²˜ë¦¬
   â†“
4. chunk_index í• ë‹¹ (0-based)
   â†“
5. ë©”íƒ€ë°ì´í„° ë³´ê°•
   â”œâ”€ í‚¤ì›Œë“œ ì¶”ì¶œ
   â”œâ”€ ì—”í‹°í‹° ì¶”ì¶œ
   â”œâ”€ ë²•ë¥  ìš©ì–´ ì¶”ì¶œ
   â”œâ”€ ì¹´í…Œê³ ë¦¬ íƒœê¹…
   â”œâ”€ ë²•ë ¹ ì°¸ì¡° ì¶”ì¶œ
   â””â”€ ë‚ ì§œ ì¶”ì¶œ
   â†“
6. ê²€ì¦
   â†“
7. JSON ì €ì¥
```

### ì‹¤í–‰ ì˜ˆì‹œ

```bash
# ë°ì´í„° ë³€í™˜ ì‹¤í–‰ (ì²­í¬ ìµœì í™” + ë©”íƒ€ë°ì´í„° ë³´ê°• í¬í•¨)
cd /home/maroco/ddoksori_demo
conda activate ddoksori
python backend/scripts/data_processing/data_transform_pipeline.py

# ì˜ˆìƒ ì¶œë ¥:
# âœ… ë©”íƒ€ë°ì´í„° ë³´ê°• í™œì„±í™”
# ================================================================================
# ë°ì´í„° ë³€í™˜ ì‹œì‘
# ================================================================================
# ...
#   âœ… ë¯¼ë²•: 792ê°œ ì²­í¬ (ìµœì í™” ì™„ë£Œ)
# ...
# ================================================================================
# ë³€í™˜ ì™„ë£Œ í†µê³„
# ================================================================================
#   - ì´ ë¬¸ì„œ: 11,976ê°œ
#   - ì´ ì²­í¬: 14,337ê°œ
#   - ë©”íƒ€ë°ì´í„° ë³´ê°•: 14,027ê°œ ì²­í¬
#   - ì˜¤ë¥˜: 0ê°œ
```

---

## ğŸ“Š ê²€ì¦ ê²°ê³¼

### ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰

```bash
python backend/scripts/data_processing/validate_transformed_data.py
```

### ê°œì„ ëœ ê²€ì¦ ê²°ê³¼

```
================================================================================
ê²€ì¦ ê²°ê³¼
================================================================================

ğŸ“Š ê¸°ë³¸ í†µê³„:
  - ì´ ë¬¸ì„œ: 11,976ê°œ
  - ì´ ì²­í¬: 14,337ê°œ

ğŸ¯ RAG ìµœì í™” ë¶„ì„ (100-2000ì ê¸°ì¤€):
  - ìµœì  ë²”ìœ„: 13,530ê°œ (95.6%)
  - ë„ˆë¬´ ì§§ìŒ: 310ê°œ (2.2%)
  - ë„ˆë¬´ ê¹€: 319ê°œ (2.3%)

ğŸ” ì´ìŠˆ:
  - âŒ Critical ì˜¤ë¥˜: 0ê°œ
  - âš ï¸  ê²½ê³ : 1,028ê°œ

================================================================================
âœ… ê²€ì¦ í†µê³¼! ì„ë² ë”© ì§„í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.
   (ê²½ê³  ì‚¬í•­ì´ ìˆìœ¼ë‚˜ ì¹˜ëª…ì ì´ì§€ ì•ŠìŒ)
================================================================================
```

---

## ğŸ¯ Hybrid Search í™œìš©

ë©”íƒ€ë°ì´í„° ë³´ê°•ì„ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ Hybrid Searchê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤:

### ì˜ˆì‹œ ì¿¼ë¦¬

```sql
-- ë²¡í„° ê²€ìƒ‰ + ë©”íƒ€ë°ì´í„° í•„í„°ë§
SELECT 
    c.chunk_id,
    c.content,
    d.title,
    1 - (c.embedding <=> query_embedding) as similarity
FROM chunks c
JOIN documents d ON c.doc_id = d.doc_id
WHERE 
    -- ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
    c.embedding <=> query_embedding < 0.3
    -- ë©”íƒ€ë°ì´í„° í•„í„°ë§
    AND d.doc_type = 'mediation_case'
    AND d.source_org = 'KCA'
    AND c.metadata->>'category_tags' @> '["ì „ììƒê±°ë˜"]'::jsonb
    AND c.metadata->>'keywords' @> '["í™˜ë¶ˆ", "í•˜ì"]'::jsonb
    AND c.drop = FALSE
ORDER BY c.embedding <=> query_embedding
LIMIT 10;
```

### í•„í„°ë§ ì˜µì…˜

- **ì¹´í…Œê³ ë¦¬**: `metadata->>'category_tags'`
- **í‚¤ì›Œë“œ**: `metadata->>'keywords'`
- **ë²•ë¥  ìš©ì–´**: `metadata->>'legal_terms'`
- **ì—”í‹°í‹°**: `metadata->>'entities'`
- **ë²•ë ¹ ì°¸ì¡°**: `metadata->>'law_references'`
- **ë‚ ì§œ**: `metadata->>'dates'`

---

## âš™ï¸ ì„¤ì • ë° ì»¤ìŠ¤í„°ë§ˆì´ì§•

### ì²­í¬ íƒ€ì…ë³„ ê·œì¹™ ìˆ˜ì •

`backend/scripts/data_processing/data_transform_pipeline.py`ì˜ `CHUNK_PROCESSING_RULES`ë¥¼ ìˆ˜ì •í•˜ì—¬ ê° ì²­í¬ íƒ€ì…ì˜ ìµœì†Œ/ìµœëŒ€ ê¸¸ì´ ë° ë³‘í•©/ë¶„í•  ê·œì¹™ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì»¤ìŠ¤í„°ë§ˆì´ì§•

`backend/scripts/data_processing/metadata_enricher.py`ì˜ ë‹¤ìŒ ë¶€ë¶„ì„ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- **ë²•ë¥  ìš©ì–´ ì‚¬ì „**: `self.legal_terms` ë”•ì…”ë„ˆë¦¬
- **ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ë§¤í•‘**: `self.category_keywords` ë”•ì…”ë„ˆë¦¬
- **ì—”í‹°í‹° ì¶”ì¶œ íŒ¨í„´**: `extract_entities()` ë©”ì„œë“œì˜ ì •ê·œì‹ íŒ¨í„´

---

## ğŸ“ ì£¼ì˜ì‚¬í•­

### ì„±ëŠ¥ ê³ ë ¤ì‚¬í•­

1. **ë©”íƒ€ë°ì´í„° ë³´ê°• ì‹œê°„**: ì²­í¬ë‹¹ ì•½ 10-50ms ì†Œìš” (ë‚´ìš© ê¸¸ì´ì— ë”°ë¼ ë‹¤ë¦„)
2. **ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬**: 14,000ê°œ ì²­í¬ ê¸°ì¤€ ì•½ 5-10ë¶„ ì¶”ê°€ ì†Œìš”
3. **ë©”ëª¨ë¦¬ ì‚¬ìš©**: ë©”íƒ€ë°ì´í„° ë³´ê°•ìœ¼ë¡œ ì¸í•´ JSON íŒŒì¼ í¬ê¸°ê°€ ì•½ 10-20% ì¦ê°€

### ì œí•œì‚¬í•­

1. **ì§§ì€ ì²­í¬ ë³‘í•©**: `merge_allowed=False`ì¸ íƒ€ì…ì€ ë³‘í•©ë˜ì§€ ì•ŠìŒ
2. **ê¸´ ì²­í¬ ë¶„í• **: `split_allowed=False`ì¸ íƒ€ì…ì€ ë¶„í• ë˜ì§€ ì•ŠìŒ
3. **ë©”íƒ€ë°ì´í„° ì¶”ì¶œ**: ë§¤ìš° ì§§ì€ ì²­í¬(< 20ì)ì—ì„œëŠ” ì˜ë¯¸ ìˆëŠ” ë©”íƒ€ë°ì´í„° ì¶”ì¶œì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ

---

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- [ë°ì´í„° ë³€í™˜ ë° í…ŒìŠ¤íŠ¸ ê°€ì´ë“œ](./ë°ì´í„°_ë³€í™˜_ë°_í…ŒìŠ¤íŠ¸_ê°€ì´ë“œ.md)
- [ê²€ì¦ ê·œì¹™ ê°•í™” íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](./ê²€ì¦_ê·œì¹™_ê°•í™”_íŠ¸ëŸ¬ë¸”ìŠˆíŒ….md)
- [RAG ë°ì´í„° í’ˆì§ˆ ê°œì„  ê³„íš](../.cursor/plans/ë°ì´í„°_í’ˆì§ˆ_ê°œì„ _ê³„íš_22a4ef1e.plan.md)

---

## ğŸ“š ì°¸ê³  ì½”ë“œ

### ì£¼ìš” íŒŒì¼

- `backend/scripts/data_processing/data_transform_pipeline.py`: ë°ì´í„° ë³€í™˜ íŒŒì´í”„ë¼ì¸ (ì²­í¬ ìµœì í™” í¬í•¨)
- `backend/scripts/data_processing/metadata_enricher.py`: ë©”íƒ€ë°ì´í„° ë³´ê°• ëª¨ë“ˆ
- `backend/scripts/data_processing/validate_transformed_data.py`: ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ (ê°•í™”ëœ ê·œì¹™ í¬í•¨)

### ì£¼ìš” í•¨ìˆ˜

- `DataTransformer._optimize_chunks()`: ì²­í¬ ìµœì í™” í†µí•© í•¨ìˆ˜
- `DataTransformer._merge_short_chunks()`: ì§§ì€ ì²­í¬ ë³‘í•©
- `DataTransformer._split_long_chunks()`: ê¸´ ì²­í¬ ë¶„í• 
- `DataTransformer._enrich_document()`: ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ë³´ê°•
- `MetadataEnricher.enrich_chunk_metadata()`: ì²­í¬ ë©”íƒ€ë°ì´í„° ë³´ê°•

---

**ì‘ì„±ì**: Manus AI  
**ìµœì¢… ìˆ˜ì •**: 2026-01-06
