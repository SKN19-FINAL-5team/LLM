"""
ë©€í‹° ìŠ¤í…Œì´ì§€ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

4ê°€ì§€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ ë©€í‹° ìŠ¤í…Œì´ì§€ ê²€ìƒ‰ì˜ íš¨ê³¼ë¥¼ ê²€ì¦:
1. ì „ìì œí’ˆ í™˜ë¶ˆ (ë…¸íŠ¸ë¶ ë¶ˆëŸ‰)
2. ì˜¨ë¼ì¸ ê±°ë˜ ë¶„ìŸ (ë°°ì†¡ ì§€ì—°)
3. ì„œë¹„ìŠ¤ í™˜ë¶ˆ (í•™ì› ìˆ˜ê°•ë£Œ)
4. ì½˜í…ì¸  ë¶„ìŸ (ìŒì› ì €ì‘ê¶Œ)
"""

import os
import sys
from dotenv import load_dotenv
from datetime import datetime
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from app.rag import MultiStageRetriever


# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


# DB ì„¤ì •
DB_CONFIG = {
    'host': os.getenv('DB_HOST', 'localhost'),
    'port': int(os.getenv('DB_PORT', 5432)),
    'database': os.getenv('DB_NAME', 'ddoksori'),
    'user': os.getenv('DB_USER', 'postgres'),
    'password': os.getenv('DB_PASSWORD', 'postgres')
}


# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ì˜
TEST_CASES = [
    {
        'id': 1,
        'name': 'ì „ìì œí’ˆ í™˜ë¶ˆ (ë…¸íŠ¸ë¶ ë¶ˆëŸ‰)',
        'query': 'ì˜¨ë¼ì¸ì—ì„œ ë…¸íŠ¸ë¶ì„ êµ¬ë§¤í–ˆëŠ”ë° 3ì¼ ë§Œì— í™”ë©´ì´ ì•ˆ ì¼œì§‘ë‹ˆë‹¤. í™˜ë¶ˆ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?',
        'expected_agency': 'ecmc',  # ì˜¨ë¼ì¸ êµ¬ë§¤ -> ì „ìê±°ë˜ë¶„ìŸì¡°ì •ìœ„ì›íšŒ
        'product_category': 'ì „ìì œí’ˆ',
        'purchase_method': 'ì˜¨ë¼ì¸'
    },
    {
        'id': 2,
        'name': 'ì˜¨ë¼ì¸ ê±°ë˜ ë¶„ìŸ (ë°°ì†¡ ì§€ì—°)',
        'query': 'ì¿ íŒ¡ì—ì„œ ì˜·ì„ ì£¼ë¬¸í–ˆëŠ”ë° 2ì£¼ê°€ ì§€ë‚˜ë„ ë°°ì†¡ì´ ì•ˆ ë©ë‹ˆë‹¤. í™˜ë¶ˆ ìš”ì²­í–ˆëŠ”ë° ê±°ë¶€ë‹¹í–ˆì–´ìš”.',
        'expected_agency': 'ecmc',  # ì¿ íŒ¡ -> ì „ìê±°ë˜ë¶„ìŸì¡°ì •ìœ„ì›íšŒ
        'product_category': 'ì˜ë¥˜',
        'purchase_method': 'ì˜¨ë¼ì¸'
    },
    {
        'id': 3,
        'name': 'ì„œë¹„ìŠ¤ í™˜ë¶ˆ (í•™ì› ìˆ˜ê°•ë£Œ)',
        'query': 'ì˜ì–´ í•™ì›ì„ ë“±ë¡í–ˆëŠ”ë° ê°•ì‚¬ê°€ ê³„ì† ë°”ë€Œê³  ìˆ˜ì—… ì§ˆì´ ë„ˆë¬´ ë–¨ì–´ì§‘ë‹ˆë‹¤. í™˜ë¶ˆ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?',
        'expected_agency': 'kca',  # ì¼ë°˜ ì„œë¹„ìŠ¤ -> í•œêµ­ì†Œë¹„ìì›
        'product_category': 'ì„œë¹„ìŠ¤',
        'purchase_method': 'ì˜¤í”„ë¼ì¸'
    },
    {
        'id': 4,
        'name': 'ì½˜í…ì¸  ë¶„ìŸ (ìŒì› ì €ì‘ê¶Œ)',
        'query': 'ë©œë¡ ì—ì„œ êµ¬ë§¤í•œ ìŒì›ì„ ë‹¤ë¥¸ ê¸°ê¸°ë¡œ ì˜®ê¸°ë ¤ê³  í•˜ëŠ”ë° ì•ˆ ë©ë‹ˆë‹¤. ì œê°€ ì‚° ìŒì›ì¸ë° ì™œ ëª» ì“°ë‚˜ìš”?',
        'expected_agency': 'kcdrc',  # ìŒì› ì €ì‘ê¶Œ -> í•œêµ­ì €ì‘ê¶Œìœ„ì›íšŒ
        'product_category': 'ì½˜í…ì¸ ',
        'purchase_method': 'ì˜¨ë¼ì¸'
    }
]


def print_separator(title: str = None):
    """êµ¬ë¶„ì„  ì¶œë ¥"""
    if title:
        print(f"\n{'='*80}")
        print(f"  {title}")
        print(f"{'='*80}\n")
    else:
        print(f"{'='*80}\n")


def print_test_case_header(test_case: dict):
    """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ í—¤ë” ì¶œë ¥"""
    print_separator(f"í…ŒìŠ¤íŠ¸ {test_case['id']}: {test_case['name']}")
    print(f"**ì§ˆë¬¸:** {test_case['query']}")
    print(f"**ì˜ˆìƒ ê¸°ê´€:** {test_case['expected_agency']}")
    print(f"**ì œí’ˆ ì¹´í…Œê³ ë¦¬:** {test_case['product_category']}")
    print(f"**êµ¬ë§¤ ë°©ë²•:** {test_case['purchase_method']}")
    print()


def print_stage_results(stage_name: str, chunks: list):
    """ê²€ìƒ‰ ë‹¨ê³„ë³„ ê²°ê³¼ ì¶œë ¥"""
    print(f"\n[{stage_name}] {len(chunks)}ê°œ ì²­í¬ ê²€ìƒ‰")
    if chunks:
        for idx, chunk in enumerate(chunks[:3], 1):  # ìƒìœ„ 3ê°œë§Œ ì¶œë ¥
            print(f"  {idx}. [{chunk.get('chunk_type', 'N/A')}] "
                  f"ìœ ì‚¬ë„: {chunk.get('similarity', 0):.3f} - "
                  f"{chunk.get('text', '')[:100]}...")


def evaluate_results(test_case: dict, results: dict) -> dict:
    """
    ê²€ìƒ‰ ê²°ê³¼ í‰ê°€
    
    Returns:
        í‰ê°€ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
    """
    evaluation = {
        'test_id': test_case['id'],
        'test_name': test_case['name'],
        'timestamp': datetime.now().isoformat()
    }
    
    # 1. ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
    stats = results.get('stats', {})
    evaluation['total_chunks'] = stats.get('total_chunks', 0)
    evaluation['law_chunks'] = stats.get('law_chunks', 0)
    evaluation['criteria_chunks'] = stats.get('criteria_chunks', 0)
    evaluation['mediation_chunks'] = stats.get('mediation_chunks', 0)
    evaluation['counsel_chunks'] = stats.get('counsel_chunks', 0)
    evaluation['used_fallback'] = stats.get('used_fallback', False)
    
    # 2. ê¸°ê´€ ì¶”ì²œ ì •í™•ë„
    agency_rec = results.get('agency_recommendation')
    if agency_rec and agency_rec.get('top_agency'):
        recommended_agency = agency_rec['top_agency'][0]  # (agency_code, score, info)
        evaluation['recommended_agency'] = recommended_agency
        evaluation['agency_correct'] = (recommended_agency == test_case['expected_agency'])
        evaluation['agency_score'] = agency_rec['top_agency'][1]
    else:
        evaluation['recommended_agency'] = None
        evaluation['agency_correct'] = False
        evaluation['agency_score'] = 0.0
    
    # 3. ìœ ì‚¬ë„ í‰ê°€
    all_chunks = results.get('all_chunks', [])
    if all_chunks:
        similarities = [chunk.get('similarity', 0) for chunk in all_chunks]
        evaluation['avg_similarity'] = sum(similarities) / len(similarities)
        evaluation['max_similarity'] = max(similarities)
        evaluation['min_similarity'] = min(similarities)
    else:
        evaluation['avg_similarity'] = 0.0
        evaluation['max_similarity'] = 0.0
        evaluation['min_similarity'] = 0.0
    
    # 4. Fallback ì‚¬ìš© ì—¬ë¶€
    evaluation['fallback_triggered'] = results.get('used_fallback', False)
    
    return evaluation


def print_evaluation(evaluation: dict):
    """í‰ê°€ ê²°ê³¼ ì¶œë ¥"""
    print(f"\n{'â”€'*80}")
    print("ğŸ“Š í‰ê°€ ê²°ê³¼")
    print(f"{'â”€'*80}")
    
    print(f"\nâœ… ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½:")
    print(f"  - ì´ ì²­í¬ ìˆ˜: {evaluation['total_chunks']}ê°œ")
    print(f"  - ë²•ë ¹: {evaluation['law_chunks']}ê°œ")
    print(f"  - ê¸°ì¤€: {evaluation['criteria_chunks']}ê°œ")
    print(f"  - ë¶„ìŸì¡°ì •ì‚¬ë¡€: {evaluation['mediation_chunks']}ê°œ")
    print(f"  - í”¼í•´êµ¬ì œì‚¬ë¡€: {evaluation['counsel_chunks']}ê°œ")
    print(f"  - Fallback ì‚¬ìš©: {'ì˜ˆ' if evaluation['fallback_triggered'] else 'ì•„ë‹ˆì˜¤'}")
    
    print(f"\nâœ… ìœ ì‚¬ë„ ë¶„ì„:")
    print(f"  - í‰ê·  ìœ ì‚¬ë„: {evaluation['avg_similarity']:.3f}")
    print(f"  - ìµœëŒ€ ìœ ì‚¬ë„: {evaluation['max_similarity']:.3f}")
    print(f"  - ìµœì†Œ ìœ ì‚¬ë„: {evaluation['min_similarity']:.3f}")
    
    print(f"\nâœ… ê¸°ê´€ ì¶”ì²œ:")
    if evaluation['recommended_agency']:
        status = "âœ“ ì •í™•" if evaluation['agency_correct'] else "âœ— ë¶€ì •í™•"
        print(f"  - ì¶”ì²œ ê¸°ê´€: {evaluation['recommended_agency']} ({status})")
        print(f"  - ì¶”ì²œ ì ìˆ˜: {evaluation['agency_score']:.3f}")
    else:
        print(f"  - ì¶”ì²œ ê¸°ê´€ ì—†ìŒ")
    
    print()


def run_test(retriever: MultiStageRetriever, test_case: dict) -> dict:
    """
    ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰
    
    Args:
        retriever: ë©€í‹° ìŠ¤í…Œì´ì§€ ê²€ìƒ‰ê¸°
        test_case: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        
    Returns:
        í‰ê°€ ê²°ê³¼
    """
    print_test_case_header(test_case)
    
    # ë©€í‹° ìŠ¤í…Œì´ì§€ ê²€ìƒ‰ ì‹¤í–‰
    start_time = datetime.now()
    
    results = retriever.search_multi_stage(
        query=test_case['query'],
        law_top_k=3,
        criteria_top_k=3,
        mediation_top_k=5,
        counsel_top_k=3,
        mediation_threshold=2,
        enable_agency_recommendation=True
    )
    
    end_time = datetime.now()
    elapsed_time = (end_time - start_time).total_seconds()
    
    print(f"\nâ±ï¸ ê²€ìƒ‰ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
    
    # Stageë³„ ê²°ê³¼ ì¶œë ¥
    stage1 = results.get('stage1', {})
    print_stage_results("Stage 1: ë²•ë ¹", stage1.get('law', []))
    print_stage_results("Stage 1: ê¸°ì¤€", stage1.get('criteria', []))
    
    stage2 = results.get('stage2', [])
    print_stage_results("Stage 2: ë¶„ìŸì¡°ì •ì‚¬ë¡€", stage2)
    
    if results.get('used_fallback'):
        stage3 = results.get('stage3', [])
        print_stage_results("Stage 3: í”¼í•´êµ¬ì œì‚¬ë¡€ (Fallback)", stage3)
    
    # ê¸°ê´€ ì¶”ì²œ ê²°ê³¼ ì¶œë ¥
    agency_rec = results.get('agency_recommendation')
    if agency_rec:
        print(f"\nğŸ“‹ ê¸°ê´€ ì¶”ì²œ ê²°ê³¼:")
        print(agency_rec['formatted'])
    
    # í‰ê°€
    evaluation = evaluate_results(test_case, results)
    evaluation['elapsed_time'] = elapsed_time
    print_evaluation(evaluation)
    
    return evaluation


def print_summary(evaluations: list):
    """ì „ì²´ í…ŒìŠ¤íŠ¸ ìš”ì•½ ì¶œë ¥"""
    print_separator("ğŸ“ˆ ì „ì²´ í…ŒìŠ¤íŠ¸ ìš”ì•½")
    
    total_tests = len(evaluations)
    total_chunks = sum(e['total_chunks'] for e in evaluations)
    avg_chunks = total_chunks / total_tests if total_tests > 0 else 0
    
    fallback_count = sum(1 for e in evaluations if e['fallback_triggered'])
    fallback_rate = fallback_count / total_tests * 100 if total_tests > 0 else 0
    
    agency_correct = sum(1 for e in evaluations if e['agency_correct'])
    agency_accuracy = agency_correct / total_tests * 100 if total_tests > 0 else 0
    
    avg_similarity = sum(e['avg_similarity'] for e in evaluations) / total_tests if total_tests > 0 else 0
    avg_time = sum(e['elapsed_time'] for e in evaluations) / total_tests if total_tests > 0 else 0
    
    print(f"âœ… ì´ í…ŒìŠ¤íŠ¸: {total_tests}ê±´")
    print(f"âœ… í‰ê·  ê²€ìƒ‰ ì²­í¬ ìˆ˜: {avg_chunks:.1f}ê°œ")
    print(f"âœ… Fallback ì‚¬ìš©ë¥ : {fallback_rate:.1f}% ({fallback_count}/{total_tests})")
    print(f"âœ… ê¸°ê´€ ì¶”ì²œ ì •í™•ë„: {agency_accuracy:.1f}% ({agency_correct}/{total_tests})")
    print(f"âœ… í‰ê·  ìœ ì‚¬ë„: {avg_similarity:.3f}")
    print(f"âœ… í‰ê·  ê²€ìƒ‰ ì‹œê°„: {avg_time:.2f}ì´ˆ")
    
    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ë³„ ìƒì„¸:")
    for e in evaluations:
        status = "âœ“" if e['agency_correct'] else "âœ—"
        print(f"  {status} í…ŒìŠ¤íŠ¸ {e['test_id']}: {e['test_name']}")
        print(f"     - ì²­í¬: {e['total_chunks']}ê°œ, ìœ ì‚¬ë„: {e['avg_similarity']:.3f}, "
              f"ê¸°ê´€: {e['recommended_agency']}, ì‹œê°„: {e['elapsed_time']:.2f}ì´ˆ")


def save_results(evaluations: list, output_file: str = "test_results.json"):
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(evaluations, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print_separator("ğŸš€ ë©€í‹° ìŠ¤í…Œì´ì§€ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    print("ğŸ“Œ í…ŒìŠ¤íŠ¸ ì„¤ì •:")
    print(f"  - DB Host: {DB_CONFIG['host']}")
    print(f"  - DB Name: {DB_CONFIG['database']}")
    print(f"  - í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìˆ˜: {len(TEST_CASES)}ê°œ")
    
    # ë©€í‹° ìŠ¤í…Œì´ì§€ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
    try:
        retriever = MultiStageRetriever(DB_CONFIG)
        print("âœ… ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰
    evaluations = []
    
    for test_case in TEST_CASES:
        try:
            evaluation = run_test(retriever, test_case)
            evaluations.append(evaluation)
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ {test_case['id']} ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    # ê²€ìƒ‰ê¸° ì¢…ë£Œ
    retriever.close()
    
    # ì „ì²´ ìš”ì•½
    if evaluations:
        print_summary(evaluations)
        save_results(evaluations)
    else:
        print("\nâŒ ì‹¤í–‰ëœ í…ŒìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    print_separator("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")


if __name__ == "__main__":
    main()
